{
  "version": 3,
  "sources": [],
  "sections": [
    {"offset": {"line": 7, "column": 0}, "map": {"version":3,"sources":[],"names":[],"mappings":"","debugId":null}},
    {"offset": {"line": 54, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/prompt.ts"],"sourcesContent":["type PlaceholderKey = string;\r\n\r\ntype InferPromptVars<T extends PlaceholderKey[]> = T extends []\r\n  ? Record<string, never>\r\n  : { [K in T[number]]: string | number | boolean };\r\n\r\ntype CompileFunction<T> =\r\n  T extends Record<string, never> ? () => string : (variables: T) => string;\r\n\r\ntype PromptResult<T> = {\r\n  compile: CompileFunction<T>;\r\n};\r\n\r\nexport const prmpt = <K extends PlaceholderKey[]>(\r\n  strings: TemplateStringsArray,\r\n  ...keys: K\r\n): PromptResult<InferPromptVars<K>> => {\r\n  if (keys.length === 0) {\r\n    return {\r\n      compile: (() => strings[0]?.trim()) as CompileFunction<\r\n        InferPromptVars<K>\r\n      >,\r\n    };\r\n  }\r\n\r\n  return {\r\n    compile: ((variables: InferPromptVars<K>) => {\r\n      let result = strings[0];\r\n\r\n      for (let i = 0; i < keys.length; i++) {\r\n        const key = keys[i];\r\n        if (key !== undefined) {\r\n          result += variables[key] + (strings[i + 1] ?? \"\");\r\n        }\r\n      }\r\n\r\n      return result?.trim();\r\n    }) as CompileFunction<InferPromptVars<K>>,\r\n  };\r\n};\r\n"],"names":[],"mappings":";;;AAaO,MAAM,QAAQ,CACnB,SACA,GAAG;IAEH,IAAI,KAAK,MAAM,KAAK,GAAG;QACrB,OAAO;YACL,SAAU,IAAM,OAAO,CAAC,EAAE,EAAE;QAG9B;IACF;IAEA,OAAO;QACL,SAAU,CAAC;YACT,IAAI,SAAS,OAAO,CAAC,EAAE;YAEvB,IAAK,IAAI,IAAI,GAAG,IAAI,KAAK,MAAM,EAAE,IAAK;gBACpC,MAAM,MAAM,IAAI,CAAC,EAAE;gBACnB,IAAI,QAAQ,WAAW;oBACrB,UAAU,SAAS,CAAC,IAAI,GAAG,CAAC,OAAO,CAAC,IAAI,EAAE,IAAI,EAAE;gBAClD;YACF;YAEA,OAAO,QAAQ;QACjB;IACF;AACF","debugId":null}},
    {"offset": {"line": 82, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/prompts.ts"],"sourcesContent":["import { prmpt } from \"@/lib/prompt\";\r\n\r\nexport const DEFAULT_SYSTEM_PROMPT = prmpt`\r\nYou are an AI assistant powered by Agentset. Your primary task is to provide accurate, factual responses based STRICTLY on the provided search results. You must ONLY answer questions using information explicitly found in the search results - do not make assumptions or add information from outside knowledge.\r\n\r\nFollow these STRICT guidelines:\r\n1. If the search results do not contain information to fully answer the query, state clearly: \"I cannot fully answer this question based on the available information.\" Then explain what specific aspects cannot be answered.\r\n2. Only use information directly stated in the search results - do not infer, assume, or add external knowledge.\r\n3. Your response must match the language of the user's query.\r\n4. Citations are MANDATORY for every factual statement. Format citations by placing the chunk number in brackets immediately after the relevant statement with no space, like this: \"The temperature is 20 degrees[3]\"\r\n5. When possible, include relevant direct quotes from the search results with proper citations.\r\n6. Do not preface responses with phrases like \"based on the search results\" - simply provide the cited answer.\r\n7. Maintain a clear, professional tone focused on accuracy and fidelity to the source material.\r\n\r\nIf the search results are completely irrelevant or insufficient to address any part of the query, respond: \"I cannot answer this question as the search results do not contain relevant information about [specific topic].\"\r\n`;\r\n\r\nexport const NEW_MESSAGE_PROMPT = prmpt`\r\nMost relevant search results:\r\n${\"chunks\"}\r\n\r\nUser's query:\r\n${\"query\"}\r\n`;\r\n\r\nexport const CONDENSE_SYSTEM_PROMPT = prmpt`\r\nGiven a conversation history between Human and Assistant and a follow-up question from Human, rewrite the question into a standalone query that:\r\n\r\n1. Incorporates all relevant context from the prior conversation\r\n2. Preserves specific details, names, and technical terms mentioned earlier\r\n3. Maintains the original language and tone of the user's question\r\n4. Focuses on searchable keywords and concepts to optimize vector database retrieval\r\n5. Removes conversational elements like \"as mentioned before\" or \"following up on\"\r\n6. Expands pronouns and references to their full form (e.g. \"it\" â†’ \"the database schema\")\r\n\r\nYour task is to create a clear, context-rich query that will yield the most relevant search results from the vector database.\r\n\r\n\r\nQuestion: ${\"question\"}\r\n\r\nHistory:\r\n${\"chatHistory\"}\r\n`;\r\n\r\nexport const CONDENSE_USER_PROMPT = prmpt`\r\nChat History:\r\n${\"chatHistory\"}\r\n\r\nFollow Up Message:\r\n${\"query\"}\r\n`;\r\n"],"names":[],"mappings":";;;;;;AAAA;;AAEO,MAAM,wBAAwB,qIAAA,CAAA,QAAK,CAAC;;;;;;;;;;;;;AAa3C,CAAC;AAEM,MAAM,qBAAqB,qIAAA,CAAA,QAAK,CAAC;;AAExC,EAAE,SAAS;;;AAGX,EAAE,QAAQ;AACV,CAAC;AAEM,MAAM,yBAAyB,qIAAA,CAAA,QAAK,CAAC;;;;;;;;;;;;;UAalC,EAAE,WAAW;;;AAGvB,EAAE,cAAc;AAChB,CAAC;AAEM,MAAM,uBAAuB,qIAAA,CAAA,QAAK,CAAC;;AAE1C,EAAE,cAAc;;;AAGhB,EAAE,QAAQ;AACV,CAAC","debugId":null}},
    {"offset": {"line": 142, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/keyword-store/index.ts"],"sourcesContent":["import { env } from \"@/env\";\nimport { metadataDictToNode } from \"@llamaindex/core/vector-store\";\nimport { TextNode } from \"llamaindex\";\n\nimport { formatResults } from \"../vector-store/parse\";\n\nexport type KeywordSearchChunk = {\n  id: string;\n  text: string;\n  namespaceId: string;\n  tenantId?: string | null;\n  documentId: string;\n  metadata: string;\n};\n\nconst topLevelMetadataKeys = [\n  \"namespaceId\",\n  \"documentId\",\n  \"tenantId\",\n] satisfies (keyof KeywordSearchChunk)[];\n\nconst safeParse = (json: string) => {\n  try {\n    return JSON.parse(json);\n  } catch (error) {\n    return null;\n  }\n};\n\nexport class KeywordStore {\n  constructor(\n    private readonly namespaceId: string,\n    private readonly tenantId?: string,\n  ) {}\n\n  private encodeId(id: string) {\n    return id.replaceAll(\"#\", \"_\");\n  }\n\n  private decodeId(id: string) {\n    return id.replaceAll(\"_\", \"#\");\n  }\n\n  async search(\n    query: string,\n    {\n      documentId,\n      page = 1,\n      limit = 10,\n      includeMetadata,\n      includeRelationships,\n    }: {\n      documentId?: string;\n      page?: number;\n      limit?: number;\n      includeMetadata?: boolean;\n      includeRelationships?: boolean;\n    } = {},\n  ) {\n    // Temporary implementation - returns empty results\n    // TODO: Implement proper semantic search with Pinecone\n    console.log(\"KeywordStore.search called - returning empty results (Azure Search disabled)\");\n    \n    return {\n      total: 0,\n      totalPages: 1,\n      perPage: limit,\n      currentPage: page,\n      hasNextPage: false,\n      hasPreviousPage: false,\n      results: [],\n    };\n  }\n\n  async listIds({\n    page = 1,\n    limit = 1000,\n    documentId,\n  }: {\n    page?: number;\n    limit?: number;\n    documentId?: string;\n  } = {}) {\n    // Temporary implementation - returns empty array\n    console.log(\"KeywordStore.listIds called - returning empty array (Azure Search disabled)\");\n    \n    return {\n      total: 0,\n      totalPages: 1,\n      perPage: limit,\n      currentPage: page,\n      hasNextPage: false,\n      hasPreviousPage: false,\n      ids: [],\n    };\n  }\n\n  async deleteByIds(ids: string[]) {\n    // No-op implementation\n    console.log(\"KeywordStore.deleteByIds called - no-op (Azure Search disabled):\", ids.length, \"ids\");\n  }\n\n  async upsert(chunks: KeywordSearchChunk[]) {\n    // No-op implementation\n    console.log(\"KeywordStore.upsert called - no-op (Azure Search disabled):\", chunks.length, \"chunks\");\n  }\n}\n"],"names":[],"mappings":";;;AAeA,MAAM,uBAAuB;IAC3B;IACA;IACA;CACD;AAED,MAAM,YAAY,CAAC;IACjB,IAAI;QACF,OAAO,KAAK,KAAK,CAAC;IACpB,EAAE,OAAO,OAAO;QACd,OAAO;IACT;AACF;AAEO,MAAM;;;IACX,YACE,AAAiB,WAAmB,EACpC,AAAiB,QAAiB,CAClC;aAFiB,cAAA;aACA,WAAA;IAChB;IAEK,SAAS,EAAU,EAAE;QAC3B,OAAO,GAAG,UAAU,CAAC,KAAK;IAC5B;IAEQ,SAAS,EAAU,EAAE;QAC3B,OAAO,GAAG,UAAU,CAAC,KAAK;IAC5B;IAEA,MAAM,OACJ,KAAa,EACb,EACE,UAAU,EACV,OAAO,CAAC,EACR,QAAQ,EAAE,EACV,eAAe,EACf,oBAAoB,EAOrB,GAAG,CAAC,CAAC,EACN;QACA,mDAAmD;QACnD,uDAAuD;QACvD,QAAQ,GAAG,CAAC;QAEZ,OAAO;YACL,OAAO;YACP,YAAY;YACZ,SAAS;YACT,aAAa;YACb,aAAa;YACb,iBAAiB;YACjB,SAAS,EAAE;QACb;IACF;IAEA,MAAM,QAAQ,EACZ,OAAO,CAAC,EACR,QAAQ,IAAI,EACZ,UAAU,EAKX,GAAG,CAAC,CAAC,EAAE;QACN,iDAAiD;QACjD,QAAQ,GAAG,CAAC;QAEZ,OAAO;YACL,OAAO;YACP,YAAY;YACZ,SAAS;YACT,aAAa;YACb,aAAa;YACb,iBAAiB;YACjB,KAAK,EAAE;QACT;IACF;IAEA,MAAM,YAAY,GAAa,EAAE;QAC/B,uBAAuB;QACvB,QAAQ,GAAG,CAAC,oEAAoE,IAAI,MAAM,EAAE;IAC9F;IAEA,MAAM,OAAO,MAA4B,EAAE;QACzC,uBAAuB;QACvB,QAAQ,GAAG,CAAC,+DAA+D,OAAO,MAAM,EAAE;IAC5F;AACF","debugId":null}},
    {"offset": {"line": 312, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/env.ts"],"sourcesContent":["import { createEnv } from \"@t3-oss/env-nextjs\";\nimport { z } from \"zod\";\n\nexport const env = createEnv({\n  shared: {\n    NODE_ENV: z\n      .enum([\"development\", \"test\", \"production\"])\n      .default(\"development\"),\n    NEXT_PUBLIC_APP_NAME: z.string().optional().default(\"Agentset\"),\n    NEXT_PUBLIC_APP_SHORT_DOMAIN: z.string().optional().default(\"agentset.ai\"),\n\n    NEXT_PUBLIC_VERCEL_ENV: z\n      .enum([\"development\", \"preview\", \"production\"])\n      .optional()\n      .default(\"development\"),\n  },\n  server: {\n    DATABASE_URL: z.string().url(),\n    SUPABASE_URL: z.string().url().optional(),\n    SUPABASE_ANON_KEY: z.string().optional(),\n\n    RESEND_API_KEY: z.string().optional().default(\"not_configured\"),\n\n    BETTER_AUTH_SECRET: z.string(),\n    BETTER_AUTH_URL: z.string().url(),\n\n    QSTASH_URL: z.string().url().optional().default(\"https://qstash.upstash.io\"),\n    QSTASH_TOKEN: z.string().optional().default(\"not_configured\"),\n    QSTASH_CURRENT_SIGNING_KEY: z.string().optional().default(\"not_configured\"),\n    QSTASH_NEXT_SIGNING_KEY: z.string().optional().default(\"not_configured\"),\n\n    DEFAULT_OPENAI_API_KEY: z.string().optional().default(\"not_configured\"),\n    DEFAULT_OPENAI_MODEL: z.string().optional().default(\"gpt-4\"),\n    DEFAULT_OPENAI_EMBEDDING_MODEL: z.string().optional().default(\"text-embedding-3-large\"),\n\n    DEFAULT_PINECONE_API_KEY: z.string().optional().default(\"not_configured\"),\n    DEFAULT_PINECONE_HOST: z.string().optional().default(\"not_configured\"),\n\n    SECONDARY_PINECONE_API_KEY: z.string().optional(),\n    SECONDARY_PINECONE_HOST: z.string().url().optional(),\n\n    GITHUB_CLIENT_ID: z.string().optional().default(\"not_configured\"),\n    GITHUB_CLIENT_SECRET: z.string().optional().default(\"not_configured\"),\n\n    GOOGLE_CLIENT_ID: z.string().optional().default(\"not_configured\"),\n    GOOGLE_CLIENT_SECRET: z.string().optional().default(\"not_configured\"),\n\n    PARTITION_API_KEY: z.string().optional().default(\"not_configured\"),\n    PARTITION_API_URL: z.string().url().optional().default(\"https://api.unstructuredapp.io\"),\n\n    DEFAULT_COHERE_API_KEY: z.string().optional().default(\"not_configured\"),\n\n    UPLOADTHING_TOKEN: z.string().optional().default(\"not_configured\"),\n    UPLOADTHING_APP_ID: z.string().optional().default(\"not_configured\"),\n\n    ASSETS_UPLOADTHING_TOKEN: z.string().optional().default(\"not_configured\"),\n    ASSETS_UPLOADTHING_APP_ID: z.string().optional().default(\"not_configured\"),\n    ASSETS_UPLOADTHING_URL: z.string().url().optional().default(\"https://utfs.io/f/\"),\n\n    REDIS_URL: z.string().optional().default(\"not_configured\"),\n    REDIS_TOKEN: z.string().optional().default(\"not_configured\"),\n\n    STRIPE_API_KEY: z.string().optional().default(\"not_configured\"),\n    STRIPE_WEBHOOK_SECRET: z.string().optional().default(\"not_configured\"),\n\n    DISCORD_HOOK_ALERTS: z.string().url().optional(),\n    DISCORD_HOOK_CRON: z.string().url().optional(),\n    DISCORD_HOOK_SUBSCRIBERS: z.string().url().optional(),\n    DISCORD_HOOK_ERRORS: z.string().url().optional(),\n\n    VERCEL_PROJECT_ID: z.string().optional().default(\"not_configured\"),\n    VERCEL_TEAM_ID: z.string().optional().default(\"not_configured\"),\n    VERCEL_API_TOKEN: z.string().optional().default(\"not_configured\"),\n  },\n  client: {\n    NEXT_PUBLIC_STRIPE_PUBLIC_KEY: z.string().optional().default(\"not_configured\"),\n  },\n  runtimeEnv: {\n    NEXT_PUBLIC_APP_NAME: process.env.NEXT_PUBLIC_APP_NAME,\n    NEXT_PUBLIC_APP_SHORT_DOMAIN: process.env.NEXT_PUBLIC_APP_SHORT_DOMAIN,\n    NEXT_PUBLIC_VERCEL_ENV: process.env.NEXT_PUBLIC_VERCEL_ENV,\n\n    DATABASE_URL: process.env.DATABASE_URL,\n    SUPABASE_URL: process.env.SUPABASE_URL,\n    SUPABASE_ANON_KEY: process.env.SUPABASE_ANON_KEY,\n\n    NODE_ENV: process.env.NODE_ENV,\n    RESEND_API_KEY: process.env.RESEND_API_KEY,\n\n    BETTER_AUTH_SECRET: process.env.BETTER_AUTH_SECRET,\n    BETTER_AUTH_URL: process.env.BETTER_AUTH_URL,\n\n    QSTASH_URL: process.env.QSTASH_URL,\n    QSTASH_TOKEN: process.env.QSTASH_TOKEN,\n    QSTASH_CURRENT_SIGNING_KEY: process.env.QSTASH_CURRENT_SIGNING_KEY,\n    QSTASH_NEXT_SIGNING_KEY: process.env.QSTASH_NEXT_SIGNING_KEY,\n\n    DEFAULT_OPENAI_API_KEY: process.env.DEFAULT_OPENAI_API_KEY,\n    DEFAULT_OPENAI_MODEL: process.env.DEFAULT_OPENAI_MODEL,\n    DEFAULT_OPENAI_EMBEDDING_MODEL: process.env.DEFAULT_OPENAI_EMBEDDING_MODEL,\n\n    DEFAULT_PINECONE_API_KEY: process.env.DEFAULT_PINECONE_API_KEY,\n    DEFAULT_PINECONE_HOST: process.env.DEFAULT_PINECONE_HOST,\n\n    SECONDARY_PINECONE_API_KEY: process.env.SECONDARY_PINECONE_API_KEY,\n    SECONDARY_PINECONE_HOST: process.env.SECONDARY_PINECONE_HOST,\n\n    GITHUB_CLIENT_ID: process.env.GITHUB_CLIENT_ID,\n    GITHUB_CLIENT_SECRET: process.env.GITHUB_CLIENT_SECRET,\n\n    GOOGLE_CLIENT_ID: process.env.GOOGLE_CLIENT_ID,\n    GOOGLE_CLIENT_SECRET: process.env.GOOGLE_CLIENT_SECRET,\n\n    PARTITION_API_KEY: process.env.PARTITION_API_KEY,\n    PARTITION_API_URL: process.env.PARTITION_API_URL,\n\n    DEFAULT_COHERE_API_KEY: process.env.DEFAULT_COHERE_API_KEY,\n\n    UPLOADTHING_TOKEN: process.env.UPLOADTHING_TOKEN,\n    UPLOADTHING_APP_ID: process.env.UPLOADTHING_APP_ID,\n\n    ASSETS_UPLOADTHING_TOKEN: process.env.ASSETS_UPLOADTHING_TOKEN,\n    ASSETS_UPLOADTHING_APP_ID: process.env.ASSETS_UPLOADTHING_APP_ID,\n    ASSETS_UPLOADTHING_URL: process.env.ASSETS_UPLOADTHING_URL,\n\n    REDIS_URL: process.env.REDIS_URL,\n    REDIS_TOKEN: process.env.REDIS_TOKEN,\n\n    STRIPE_API_KEY: process.env.STRIPE_API_KEY,\n    STRIPE_WEBHOOK_SECRET: process.env.STRIPE_WEBHOOK_SECRET,\n    NEXT_PUBLIC_STRIPE_PUBLIC_KEY: process.env.NEXT_PUBLIC_STRIPE_PUBLIC_KEY,\n\n    DISCORD_HOOK_ALERTS: process.env.DISCORD_HOOK_ALERTS,\n    DISCORD_HOOK_CRON: process.env.DISCORD_HOOK_CRON,\n    DISCORD_HOOK_SUBSCRIBERS: process.env.DISCORD_HOOK_SUBSCRIBERS,\n    DISCORD_HOOK_ERRORS: process.env.DISCORD_HOOK_ERRORS,\n\n    VERCEL_PROJECT_ID: process.env.VERCEL_PROJECT_ID,\n    VERCEL_TEAM_ID: process.env.VERCEL_TEAM_ID,\n    VERCEL_API_TOKEN: process.env.VERCEL_API_TOKEN,\n  },\n  skipValidation: !!process.env.SKIP_ENV_VALIDATION,\n  emptyStringAsUndefined: true,\n});\n"],"names":[],"mappings":";;;AAAA;AACA;;;AAEO,MAAM,MAAM,CAAA,GAAA,+JAAA,CAAA,YAAS,AAAD,EAAE;IAC3B,QAAQ;QACN,UAAU,sIAAA,CAAA,IAAC,CACR,IAAI,CAAC;YAAC;YAAe;YAAQ;SAAa,EAC1C,OAAO,CAAC;QACX,sBAAsB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACpD,8BAA8B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAE5D,wBAAwB,sIAAA,CAAA,IAAC,CACtB,IAAI,CAAC;YAAC;YAAe;YAAW;SAAa,EAC7C,QAAQ,GACR,OAAO,CAAC;IACb;IACA,QAAQ;QACN,cAAc,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG;QAC5B,cAAc,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ;QACvC,mBAAmB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ;QAEtC,gBAAgB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAE9C,oBAAoB,sIAAA,CAAA,IAAC,CAAC,MAAM;QAC5B,iBAAiB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG;QAE/B,YAAY,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ,GAAG,OAAO,CAAC;QAChD,cAAc,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAC5C,4BAA4B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAC1D,yBAAyB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAEvD,wBAAwB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACtD,sBAAsB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACpD,gCAAgC,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAE9D,0BAA0B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACxD,uBAAuB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAErD,4BAA4B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ;QAC/C,yBAAyB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ;QAElD,kBAAkB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAChD,sBAAsB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAEpD,kBAAkB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAChD,sBAAsB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAEpD,mBAAmB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACjD,mBAAmB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ,GAAG,OAAO,CAAC;QAEvD,wBAAwB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAEtD,mBAAmB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACjD,oBAAoB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAElD,0BAA0B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACxD,2BAA2B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACzD,wBAAwB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ,GAAG,OAAO,CAAC;QAE5D,WAAW,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACzC,aAAa,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAE3C,gBAAgB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAC9C,uBAAuB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAErD,qBAAqB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ;QAC9C,mBAAmB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ;QAC5C,0BAA0B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ;QACnD,qBAAqB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,GAAG,GAAG,QAAQ;QAE9C,mBAAmB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QACjD,gBAAgB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;QAC9C,kBAAkB,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;IAClD;IACA,QAAQ;QACN,+BAA+B,sIAAA,CAAA,IAAC,CAAC,MAAM,GAAG,QAAQ,GAAG,OAAO,CAAC;IAC/D;IACA,YAAY;QACV,sBAAsB,QAAQ,GAAG,CAAC,oBAAoB;QACtD,8BAA8B,QAAQ,GAAG,CAAC,4BAA4B;QACtE,wBAAwB,QAAQ,GAAG,CAAC,sBAAsB;QAE1D,cAAc,QAAQ,GAAG,CAAC,YAAY;QACtC,cAAc,QAAQ,GAAG,CAAC,YAAY;QACtC,mBAAmB,QAAQ,GAAG,CAAC,iBAAiB;QAEhD,QAAQ;QACR,gBAAgB,QAAQ,GAAG,CAAC,cAAc;QAE1C,oBAAoB,QAAQ,GAAG,CAAC,kBAAkB;QAClD,iBAAiB,QAAQ,GAAG,CAAC,eAAe;QAE5C,YAAY,QAAQ,GAAG,CAAC,UAAU;QAClC,cAAc,QAAQ,GAAG,CAAC,YAAY;QACtC,4BAA4B,QAAQ,GAAG,CAAC,0BAA0B;QAClE,yBAAyB,QAAQ,GAAG,CAAC,uBAAuB;QAE5D,wBAAwB,QAAQ,GAAG,CAAC,sBAAsB;QAC1D,sBAAsB,QAAQ,GAAG,CAAC,oBAAoB;QACtD,gCAAgC,QAAQ,GAAG,CAAC,8BAA8B;QAE1E,0BAA0B,QAAQ,GAAG,CAAC,wBAAwB;QAC9D,uBAAuB,QAAQ,GAAG,CAAC,qBAAqB;QAExD,4BAA4B,QAAQ,GAAG,CAAC,0BAA0B;QAClE,yBAAyB,QAAQ,GAAG,CAAC,uBAAuB;QAE5D,kBAAkB,QAAQ,GAAG,CAAC,gBAAgB;QAC9C,sBAAsB,QAAQ,GAAG,CAAC,oBAAoB;QAEtD,kBAAkB,QAAQ,GAAG,CAAC,gBAAgB;QAC9C,sBAAsB,QAAQ,GAAG,CAAC,oBAAoB;QAEtD,mBAAmB,QAAQ,GAAG,CAAC,iBAAiB;QAChD,mBAAmB,QAAQ,GAAG,CAAC,iBAAiB;QAEhD,wBAAwB,QAAQ,GAAG,CAAC,sBAAsB;QAE1D,mBAAmB,QAAQ,GAAG,CAAC,iBAAiB;QAChD,oBAAoB,QAAQ,GAAG,CAAC,kBAAkB;QAElD,0BAA0B,QAAQ,GAAG,CAAC,wBAAwB;QAC9D,2BAA2B,QAAQ,GAAG,CAAC,yBAAyB;QAChE,wBAAwB,QAAQ,GAAG,CAAC,sBAAsB;QAE1D,WAAW,QAAQ,GAAG,CAAC,SAAS;QAChC,aAAa,QAAQ,GAAG,CAAC,WAAW;QAEpC,gBAAgB,QAAQ,GAAG,CAAC,cAAc;QAC1C,uBAAuB,QAAQ,GAAG,CAAC,qBAAqB;QACxD,6BAA6B;QAE7B,qBAAqB,QAAQ,GAAG,CAAC,mBAAmB;QACpD,mBAAmB,QAAQ,GAAG,CAAC,iBAAiB;QAChD,0BAA0B,QAAQ,GAAG,CAAC,wBAAwB;QAC9D,qBAAqB,QAAQ,GAAG,CAAC,mBAAmB;QAEpD,mBAAmB,QAAQ,GAAG,CAAC,iBAAiB;QAChD,gBAAgB,QAAQ,GAAG,CAAC,cAAc;QAC1C,kBAAkB,QAAQ,GAAG,CAAC,gBAAgB;IAChD;IACA,gBAAgB,CAAC,CAAC,QAAQ,GAAG,CAAC,mBAAmB;IACjD,wBAAwB;AAC1B","debugId":null}},
    {"offset": {"line": 435, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/vector-store/index.ts"],"sourcesContent":["import { env } from \"@/env\";\r\n\r\nimport type { Namespace } from \"@agentset/db\";\r\n\r\nexport const DIGNA_NAMESPACE_ID = \"cm7zzvk4w0001ri45hfl7lkyo\";\r\n\r\nexport const getNamespaceVectorStore = async (\r\n  namespace: Pick<Namespace, \"vectorStoreConfig\" | \"id\" | \"createdAt\">,\r\n  tenant?: string,\r\n) => {\r\n  const config = namespace.vectorStoreConfig;\r\n\r\n  const tenantId = tenant\r\n    ? `agentset:${namespace.id}:${tenant}`\r\n    : `agentset:${namespace.id === DIGNA_NAMESPACE_ID ? \"digna\" : namespace.id}`;\r\n\r\n  // TODO: handle different embedding models\r\n  if (!config) {\r\n    const { Pinecone } = await import(\"./pinecone\");\r\n    const shouldUseSecondary =\r\n      namespace.createdAt &&\r\n      (typeof namespace.createdAt === \"string\"\r\n        ? new Date(namespace.createdAt)\r\n        : namespace.createdAt\r\n      ).getTime() > 1747418241190 &&\r\n      !!env.SECONDARY_PINECONE_API_KEY &&\r\n      !!env.SECONDARY_PINECONE_HOST;\r\n\r\n    return new Pinecone({\r\n      apiKey: shouldUseSecondary\r\n        ? env.SECONDARY_PINECONE_API_KEY!\r\n        : env.DEFAULT_PINECONE_API_KEY,\r\n      indexHost: shouldUseSecondary\r\n        ? env.SECONDARY_PINECONE_HOST!\r\n        : env.DEFAULT_PINECONE_HOST,\r\n      namespace: tenantId,\r\n    });\r\n  }\r\n\r\n  switch (config.provider) {\r\n    case \"PINECONE\": {\r\n      const { Pinecone } = await import(\"./pinecone\");\r\n      const { apiKey, indexHost } = config;\r\n      return new Pinecone({ apiKey, indexHost, namespace: tenantId });\r\n    }\r\n\r\n    default: {\r\n      // This exhaustive check ensures TypeScript will error if a new provider\r\n      // is added without handling it in the switch statement\r\n      const _exhaustiveCheck: never = config.provider;\r\n      throw new Error(`Unknown vector store provider: ${_exhaustiveCheck}`);\r\n    }\r\n  }\r\n};\r\n\r\nexport { queryVectorStore } from \"./parse\";\r\n"],"names":[],"mappings":";;;;AAAA;AAuDA;;AAnDO,MAAM,qBAAqB;AAE3B,MAAM,0BAA0B,OACrC,WACA;IAEA,MAAM,SAAS,UAAU,iBAAiB;IAE1C,MAAM,WAAW,SACb,CAAC,SAAS,EAAE,UAAU,EAAE,CAAC,CAAC,EAAE,QAAQ,GACpC,CAAC,SAAS,EAAE,UAAU,EAAE,KAAK,qBAAqB,UAAU,UAAU,EAAE,EAAE;IAE9E,0CAA0C;IAC1C,IAAI,CAAC,QAAQ;QACX,MAAM,EAAE,QAAQ,EAAE,GAAG;QACrB,MAAM,qBACJ,UAAU,SAAS,IACnB,CAAC,OAAO,UAAU,SAAS,KAAK,WAC5B,IAAI,KAAK,UAAU,SAAS,IAC5B,UAAU,SAAS,AACvB,EAAE,OAAO,KAAK,iBACd,CAAC,CAAC,2HAAA,CAAA,MAAG,CAAC,0BAA0B,IAChC,CAAC,CAAC,2HAAA,CAAA,MAAG,CAAC,uBAAuB;QAE/B,OAAO,IAAI,SAAS;YAClB,QAAQ,qBACJ,2HAAA,CAAA,MAAG,CAAC,0BAA0B,GAC9B,2HAAA,CAAA,MAAG,CAAC,wBAAwB;YAChC,WAAW,qBACP,2HAAA,CAAA,MAAG,CAAC,uBAAuB,GAC3B,2HAAA,CAAA,MAAG,CAAC,qBAAqB;YAC7B,WAAW;QACb;IACF;IAEA,OAAQ,OAAO,QAAQ;QACrB,KAAK;YAAY;gBACf,MAAM,EAAE,QAAQ,EAAE,GAAG;gBACrB,MAAM,EAAE,MAAM,EAAE,SAAS,EAAE,GAAG;gBAC9B,OAAO,IAAI,SAAS;oBAAE;oBAAQ;oBAAW,WAAW;gBAAS;YAC/D;QAEA;YAAS;gBACP,wEAAwE;gBACxE,uDAAuD;gBACvD,MAAM,mBAA0B,OAAO,QAAQ;gBAC/C,MAAM,IAAI,MAAM,CAAC,+BAA+B,EAAE,kBAAkB;YACtE;IACF;AACF","debugId":null}},
    {"offset": {"line": 493, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/embedding.ts"],"sourcesContent":["import { env } from \"@/env\";\n\nimport type { Namespace } from \"@agentset/db\";\n\nexport const getNamespaceEmbeddingModel = async (\n  namespace: Pick<Namespace, \"embeddingConfig\">,\n  type?: \"document\" | \"query\",\n) => {\n  const config = namespace.embeddingConfig;\n\n  if (!config) {\n    const { createOpenAI } = await import(\"@ai-sdk/openai\");\n\n    const defaultOpenAI = createOpenAI({\n      apiKey: env.DEFAULT_OPENAI_API_KEY,\n    });\n\n    return defaultOpenAI.textEmbeddingModel(\n      env.DEFAULT_OPENAI_EMBEDDING_MODEL || \"text-embedding-3-large\",\n    );\n  }\n\n  switch (config.provider) {\n    case \"AZURE_OPENAI\": {\n      const { createAzure } = await import(\"@ai-sdk/azure\");\n\n      const { apiKey, baseUrl, deployment, apiVersion } = config;\n      const azure = createAzure({\n        apiKey,\n        apiVersion,\n        baseURL: baseUrl,\n      });\n      return azure.textEmbeddingModel(deployment);\n    }\n\n    case \"OPENAI\": {\n      const { createOpenAI } = await import(\"@ai-sdk/openai\");\n\n      const { apiKey, model } = config;\n      const openai = createOpenAI({ apiKey });\n      return openai.textEmbeddingModel(model);\n    }\n\n    case \"VOYAGE\": {\n      const { createVoyage } = await import(\"voyage-ai-provider\");\n\n      const { apiKey, model } = config;\n      const voyage = createVoyage({ apiKey });\n      return voyage.textEmbeddingModel(model, {\n        inputType: type === \"document\" ? \"document\" : \"query\",\n      });\n    }\n\n    case \"GOOGLE\": {\n      const { createGoogleGenerativeAI } = await import(\"@ai-sdk/google\");\n\n      const { apiKey, model } = config;\n      const google = createGoogleGenerativeAI({ apiKey });\n      return google.textEmbeddingModel(model);\n    }\n\n    default: {\n      // This exhaustive check ensures TypeScript will error if a new provider\n      // is added without handling it in the switch statement\n      const _exhaustiveCheck: never = config;\n      throw new Error(`Unknown vector store provider: ${_exhaustiveCheck}`);\n    }\n  }\n};\n"],"names":[],"mappings":";;;AAAA;;AAIO,MAAM,6BAA6B,OACxC,WACA;IAEA,MAAM,SAAS,UAAU,eAAe;IAExC,IAAI,CAAC,QAAQ;QACX,MAAM,EAAE,YAAY,EAAE,GAAG;QAEzB,MAAM,gBAAgB,aAAa;YACjC,QAAQ,2HAAA,CAAA,MAAG,CAAC,sBAAsB;QACpC;QAEA,OAAO,cAAc,kBAAkB,CACrC,2HAAA,CAAA,MAAG,CAAC,8BAA8B,IAAI;IAE1C;IAEA,OAAQ,OAAO,QAAQ;QACrB,KAAK;YAAgB;gBACnB,MAAM,EAAE,WAAW,EAAE,GAAG;gBAExB,MAAM,EAAE,MAAM,EAAE,OAAO,EAAE,UAAU,EAAE,UAAU,EAAE,GAAG;gBACpD,MAAM,QAAQ,YAAY;oBACxB;oBACA;oBACA,SAAS;gBACX;gBACA,OAAO,MAAM,kBAAkB,CAAC;YAClC;QAEA,KAAK;YAAU;gBACb,MAAM,EAAE,YAAY,EAAE,GAAG;gBAEzB,MAAM,EAAE,MAAM,EAAE,KAAK,EAAE,GAAG;gBAC1B,MAAM,SAAS,aAAa;oBAAE;gBAAO;gBACrC,OAAO,OAAO,kBAAkB,CAAC;YACnC;QAEA,KAAK;YAAU;gBACb,MAAM,EAAE,YAAY,EAAE,GAAG;gBAEzB,MAAM,EAAE,MAAM,EAAE,KAAK,EAAE,GAAG;gBAC1B,MAAM,SAAS,aAAa;oBAAE;gBAAO;gBACrC,OAAO,OAAO,kBAAkB,CAAC,OAAO;oBACtC,WAAW,SAAS,aAAa,aAAa;gBAChD;YACF;QAEA,KAAK;YAAU;gBACb,MAAM,EAAE,wBAAwB,EAAE,GAAG;gBAErC,MAAM,EAAE,MAAM,EAAE,KAAK,EAAE,GAAG;gBAC1B,MAAM,SAAS,yBAAyB;oBAAE;gBAAO;gBACjD,OAAO,OAAO,kBAAkB,CAAC;YACnC;QAEA;YAAS;gBACP,wEAAwE;gBACxE,uDAAuD;gBACvD,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAC,+BAA+B,EAAE,kBAAkB;YACtE;IACF;AACF","debugId":null}},
    {"offset": {"line": 563, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/functions.ts"],"sourcesContent":["// chunk an array into smaller arrays of a given size\r\nexport const chunkArray = <T>(array: T[], size: number) => {\r\n  return Array.from({ length: Math.ceil(array.length / size) }, (_, i) =>\r\n    array.slice(i * size, i * size + size),\r\n  );\r\n};\r\n\r\nexport const filterFalsy = <T>(arr: T[]): NonNullable<T>[] =>\r\n  arr.filter(Boolean) as NonNullable<T>[];\r\n"],"names":[],"mappings":"AAAA,qDAAqD;;;;;AAC9C,MAAM,aAAa,CAAI,OAAY;IACxC,OAAO,MAAM,IAAI,CAAC;QAAE,QAAQ,KAAK,IAAI,CAAC,MAAM,MAAM,GAAG;IAAM,GAAG,CAAC,GAAG,IAChE,MAAM,KAAK,CAAC,IAAI,MAAM,IAAI,OAAO;AAErC;AAEO,MAAM,cAAc,CAAI,MAC7B,IAAI,MAAM,CAAC","debugId":null}},
    {"offset": {"line": 708, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/error.ts"],"sourcesContent":["type Success<T> = {\r\n  data: T;\r\n  error: null;\r\n};\r\n\r\ntype Failure<E> = {\r\n  data: null;\r\n  error: E;\r\n};\r\n\r\ntype Result<T, E = Error> = Success<T> | Failure<E>;\r\n\r\ntype MaybePromise<T> = T | Promise<T>;\r\n\r\nexport function tryCatch<T, E = Error>(\r\n  arg: Promise<T> | (() => MaybePromise<T>),\r\n): Result<T, E> | Promise<Result<T, E>> {\r\n  if (typeof arg === \"function\") {\r\n    try {\r\n      const result = arg();\r\n\r\n      if (result instanceof Promise) {\r\n        return tryCatch(result);\r\n      }\r\n\r\n      return { data: result, error: null };\r\n    } catch (error) {\r\n      return { data: null, error: error as E };\r\n    }\r\n  }\r\n\r\n  return arg\r\n    .then((data) => ({ data, error: null }))\r\n    .catch((error) => ({\r\n      data: null,\r\n      error: error as E,\r\n    }));\r\n}\r\n"],"names":[],"mappings":";;;AAcO,SAAS,SACd,GAAyC;IAEzC,IAAI,OAAO,QAAQ,YAAY;QAC7B,IAAI;YACF,MAAM,SAAS;YAEf,IAAI,kBAAkB,SAAS;gBAC7B,OAAO,SAAS;YAClB;YAEA,OAAO;gBAAE,MAAM;gBAAQ,OAAO;YAAK;QACrC,EAAE,OAAO,OAAO;YACd,OAAO;gBAAE,MAAM;gBAAM,OAAO;YAAW;QACzC;IACF;IAEA,OAAO,IACJ,IAAI,CAAC,CAAC,OAAS,CAAC;YAAE;YAAM,OAAO;QAAK,CAAC,GACrC,KAAK,CAAC,CAAC,QAAU,CAAC;YACjB,MAAM;YACN,OAAO;QACT,CAAC;AACL","debugId":null}},
    {"offset": {"line": 743, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/rerank/cohere.ts"],"sourcesContent":["import type { BaseNode, Metadata } from \"llamaindex\";\r\nimport { env } from \"@/env\";\r\nimport { CohereClient } from \"cohere-ai\";\r\nimport { MetadataMode } from \"llamaindex\";\r\n\r\nimport { tryCatch } from \"../error\";\r\n\r\ninterface BaseRerankDocument {\r\n  node: BaseNode<Metadata>;\r\n}\r\n\r\ninterface RerankOptions {\r\n  limit: number;\r\n  query: string;\r\n  cohereApiKey?: string;\r\n}\r\n\r\nexport type RerankResult<T extends BaseRerankDocument> = T & {\r\n  rerankScore?: number;\r\n};\r\n\r\nexport async function rerankResults<T extends BaseRerankDocument>(\r\n  results: T[],\r\n  options: RerankOptions,\r\n): Promise<RerankResult<T>[]> {\r\n  if (!results.length) return results;\r\n\r\n  const client = new CohereClient({\r\n    token: options.cohereApiKey || env.DEFAULT_COHERE_API_KEY,\r\n  });\r\n\r\n  const { data: rerankResults, error } = await tryCatch(\r\n    client.v2.rerank({\r\n      documents: results.map((doc) => doc.node.getContent(MetadataMode.NONE)),\r\n      query: options.query,\r\n      topN: options.limit,\r\n      model: \"rerank-v3.5\",\r\n      returnDocuments: false,\r\n    }),\r\n  );\r\n\r\n  if (error) {\r\n    console.error(\"Cohere rerank failed:\", error);\r\n    return results;\r\n  }\r\n\r\n  // TODO: track usage with rerankResults.meta\r\n  return rerankResults.results\r\n    .map((result) => {\r\n      // Use the index from the result to find the original document\r\n      const originalIndex = result.index;\r\n      const originalDoc = results[originalIndex];\r\n\r\n      if (!originalDoc) {\r\n        return null;\r\n      }\r\n\r\n      return {\r\n        ...originalDoc,\r\n        rerankScore: result.relevanceScore,\r\n      };\r\n    })\r\n    .filter(Boolean) as RerankResult<T>[];\r\n}\r\n"],"names":[],"mappings":";;;AACA;AACA;AACA;AAAA;AAEA;;;;;AAgBO,eAAe,cACpB,OAAY,EACZ,OAAsB;IAEtB,IAAI,CAAC,QAAQ,MAAM,EAAE,OAAO;IAE5B,MAAM,SAAS,IAAI,uIAAA,CAAA,eAAY,CAAC;QAC9B,OAAO,QAAQ,YAAY,IAAI,2HAAA,CAAA,MAAG,CAAC,sBAAsB;IAC3D;IAEA,MAAM,EAAE,MAAM,aAAa,EAAE,KAAK,EAAE,GAAG,MAAM,CAAA,GAAA,oIAAA,CAAA,WAAQ,AAAD,EAClD,OAAO,EAAE,CAAC,MAAM,CAAC;QACf,WAAW,QAAQ,GAAG,CAAC,CAAC,MAAQ,IAAI,IAAI,CAAC,UAAU,CAAC,iKAAA,CAAA,eAAY,CAAC,IAAI;QACrE,OAAO,QAAQ,KAAK;QACpB,MAAM,QAAQ,KAAK;QACnB,OAAO;QACP,iBAAiB;IACnB;IAGF,IAAI,OAAO;QACT,QAAQ,KAAK,CAAC,yBAAyB;QACvC,OAAO;IACT;IAEA,4CAA4C;IAC5C,OAAO,cAAc,OAAO,CACzB,GAAG,CAAC,CAAC;QACJ,8DAA8D;QAC9D,MAAM,gBAAgB,OAAO,KAAK;QAClC,MAAM,cAAc,OAAO,CAAC,cAAc;QAE1C,IAAI,CAAC,aAAa;YAChB,OAAO;QACT;QAEA,OAAO;YACL,GAAG,WAAW;YACd,aAAa,OAAO,cAAc;QACpC;IACF,GACC,MAAM,CAAC;AACZ","debugId":null}},
    {"offset": {"line": 791, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/vector-store/parse.ts"],"sourcesContent":["import type { BaseNode, Metadata } from \"llamaindex\";\r\nimport { metadataDictToNode } from \"@llamaindex/core/vector-store\";\r\nimport { embed } from \"ai\";\r\nimport { MetadataMode } from \"llamaindex\";\r\n\r\nimport type { Namespace } from \"@agentset/db\";\r\n\r\nimport { getNamespaceVectorStore } from \".\";\r\nimport { getNamespaceEmbeddingModel } from \"../embedding\";\r\nimport { filterFalsy } from \"../functions\";\r\nimport { rerankResults } from \"../rerank/cohere\";\r\n\r\ntype Result = {\r\n  id: string;\r\n  node: BaseNode<Metadata>;\r\n  score?: number;\r\n  rerankScore?: number;\r\n};\r\n\r\nexport const formatResults = (\r\n  results: Result[],\r\n  {\r\n    includeMetadata,\r\n    includeRelationships,\r\n  }: { includeMetadata?: boolean; includeRelationships?: boolean },\r\n) => {\r\n  return results.map((result) => {\r\n    return {\r\n      id: result.id,\r\n      text: result.node.getContent(MetadataMode.NONE),\r\n      metadata: includeMetadata ? result.node.metadata : undefined,\r\n      relationships: includeRelationships\r\n        ? result.node.relationships\r\n        : undefined,\r\n      score: result.score,\r\n      rerankScore: result.rerankScore,\r\n    };\r\n  });\r\n};\r\n\r\nexport type QueryVectorStoreOptions = {\r\n  query: string;\r\n  topK: number;\r\n  tenantId?: string;\r\n  minScore?: number;\r\n  filter?: Record<string, string>;\r\n  includeMetadata?: boolean;\r\n  includeRelationships?: boolean;\r\n  rerankLimit?: number;\r\n  rerank?: boolean;\r\n};\r\n\r\nexport const queryVectorStore = async (\r\n  namespace: Pick<\r\n    Namespace,\r\n    \"id\" | \"vectorStoreConfig\" | \"embeddingConfig\" | \"createdAt\"\r\n  >,\r\n  options: QueryVectorStoreOptions,\r\n) => {\r\n  // TODO: if the embedding model is managed, track the usage\r\n  const [embeddingModel, vectorStore] = await Promise.all([\r\n    getNamespaceEmbeddingModel(namespace, \"query\"),\r\n    getNamespaceVectorStore(namespace, options.tenantId),\r\n  ]);\r\n\r\n  const embedding = await embed({\r\n    model: embeddingModel,\r\n    value: options.query,\r\n  });\r\n\r\n  // TODO: track usage\r\n  let { matches } = await vectorStore.query({\r\n    vector: embedding.embedding,\r\n    topK: options.topK,\r\n    filter: options.filter,\r\n    includeMetadata: true,\r\n  });\r\n\r\n  if (options.minScore !== undefined) {\r\n    matches = matches.filter(\r\n      (match) => match.score && match.score >= options.minScore!,\r\n    );\r\n  }\r\n\r\n  const parsedResults = filterFalsy(\r\n    matches.map((match) => {\r\n      const nodeContent = match.metadata?._node_content;\r\n      if (!nodeContent) return null;\r\n\r\n      try {\r\n        return {\r\n          id: match.id,\r\n          score: match.score,\r\n          node: metadataDictToNode(match.metadata!),\r\n        };\r\n      } catch (e) {\r\n        console.error(e);\r\n        return null;\r\n      }\r\n    }),\r\n  );\r\n\r\n  if (matches.length > 0 && parsedResults.length === 0) {\r\n    return null;\r\n  }\r\n\r\n  // If re-ranking is enabled and we have a query, perform reranking\r\n  let rerankedResults: typeof parsedResults | null = null;\r\n  if (options.rerank) {\r\n    rerankedResults = await rerankResults(parsedResults, {\r\n      limit: options.rerankLimit || options.topK,\r\n      query: options.query,\r\n    });\r\n  }\r\n\r\n  return {\r\n    query: options.query,\r\n    unorderedIds: rerankedResults\r\n      ? parsedResults.map((result) => result.id)\r\n      : null,\r\n    results: formatResults(rerankedResults ?? parsedResults, {\r\n      includeMetadata: options.includeMetadata,\r\n      includeRelationships: options.includeRelationships,\r\n    }),\r\n  };\r\n};\r\n\r\nexport type QueryVectorStoreResult = NonNullable<\r\n  Awaited<ReturnType<typeof queryVectorStore>>\r\n>;\r\n"],"names":[],"mappings":";;;;AACA;AACA;AACA;AAAA;AAIA;AAAA;AACA;AACA;AACA;;;;;;;;AASO,MAAM,gBAAgB,CAC3B,SACA,EACE,eAAe,EACf,oBAAoB,EAC0C;IAEhE,OAAO,QAAQ,GAAG,CAAC,CAAC;QAClB,OAAO;YACL,IAAI,OAAO,EAAE;YACb,MAAM,OAAO,IAAI,CAAC,UAAU,CAAC,iKAAA,CAAA,eAAY,CAAC,IAAI;YAC9C,UAAU,kBAAkB,OAAO,IAAI,CAAC,QAAQ,GAAG;YACnD,eAAe,uBACX,OAAO,IAAI,CAAC,aAAa,GACzB;YACJ,OAAO,OAAO,KAAK;YACnB,aAAa,OAAO,WAAW;QACjC;IACF;AACF;AAcO,MAAM,mBAAmB,OAC9B,WAIA;IAEA,2DAA2D;IAC3D,MAAM,CAAC,gBAAgB,YAAY,GAAG,MAAM,QAAQ,GAAG,CAAC;QACtD,CAAA,GAAA,wIAAA,CAAA,6BAA0B,AAAD,EAAE,WAAW;QACtC,CAAA,GAAA,uKAAA,CAAA,0BAAuB,AAAD,EAAE,WAAW,QAAQ,QAAQ;KACpD;IAED,MAAM,YAAY,MAAM,CAAA,GAAA,sJAAA,CAAA,QAAK,AAAD,EAAE;QAC5B,OAAO;QACP,OAAO,QAAQ,KAAK;IACtB;IAEA,oBAAoB;IACpB,IAAI,EAAE,OAAO,EAAE,GAAG,MAAM,YAAY,KAAK,CAAC;QACxC,QAAQ,UAAU,SAAS;QAC3B,MAAM,QAAQ,IAAI;QAClB,QAAQ,QAAQ,MAAM;QACtB,iBAAiB;IACnB;IAEA,IAAI,QAAQ,QAAQ,KAAK,WAAW;QAClC,UAAU,QAAQ,MAAM,CACtB,CAAC,QAAU,MAAM,KAAK,IAAI,MAAM,KAAK,IAAI,QAAQ,QAAQ;IAE7D;IAEA,MAAM,gBAAgB,CAAA,GAAA,wIAAA,CAAA,cAAW,AAAD,EAC9B,QAAQ,GAAG,CAAC,CAAC;QACX,MAAM,cAAc,MAAM,QAAQ,EAAE;QACpC,IAAI,CAAC,aAAa,OAAO;QAEzB,IAAI;YACF,OAAO;gBACL,IAAI,MAAM,EAAE;gBACZ,OAAO,MAAM,KAAK;gBAClB,MAAM,CAAA,GAAA,0KAAA,CAAA,qBAAkB,AAAD,EAAE,MAAM,QAAQ;YACzC;QACF,EAAE,OAAO,GAAG;YACV,QAAQ,KAAK,CAAC;YACd,OAAO;QACT;IACF;IAGF,IAAI,QAAQ,MAAM,GAAG,KAAK,cAAc,MAAM,KAAK,GAAG;QACpD,OAAO;IACT;IAEA,kEAAkE;IAClE,IAAI,kBAA+C;IACnD,IAAI,QAAQ,MAAM,EAAE;QAClB,kBAAkB,MAAM,CAAA,GAAA,+IAAA,CAAA,gBAAa,AAAD,EAAE,eAAe;YACnD,OAAO,QAAQ,WAAW,IAAI,QAAQ,IAAI;YAC1C,OAAO,QAAQ,KAAK;QACtB;IACF;IAEA,OAAO;QACL,OAAO,QAAQ,KAAK;QACpB,cAAc,kBACV,cAAc,GAAG,CAAC,CAAC,SAAW,OAAO,EAAE,IACvC;QACJ,SAAS,cAAc,mBAAmB,eAAe;YACvD,iBAAiB,QAAQ,eAAe;YACxC,sBAAsB,QAAQ,oBAAoB;QACpD;IACF;AACF","debugId":null}},
    {"offset": {"line": 883, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/agentic/prompts.ts"],"sourcesContent":["export const GENERATE_QUERIES_PROMPT = `\r\nGiven a user question (or a chat history), list the appropriate search queries to find answers. \r\n\r\nThere are two apis to use: keyword search and semantic search. You should return a maximum of 10 queries.\r\n\r\nA good keyword search query contains one (or max two) words that are key to finding the result.\r\n\r\nThe results should be returned in the format: \r\n{\"queries\": [{\"type\": \"keyword\", \"query\": \"...\"}, ...]}\r\n`;\r\n\r\nexport const EVALUATE_QUERIES_PROMPT = `\r\nYou are a research assistant, you will be provided with a chat history, and a list of sources, and you will need to evaluate if the sources are able to answer the user's question.\r\n\r\nThe result should be returned in the format:\r\n{ \"canAnswer\": true | false }\r\n`;\r\n"],"names":[],"mappings":";;;;AAAO,MAAM,0BAA0B,CAAC;;;;;;;;;AASxC,CAAC;AAEM,MAAM,0BAA0B,CAAC;;;;;AAKxC,CAAC","debugId":null}},
    {"offset": {"line": 909, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/agentic/utils.ts"],"sourcesContent":["import type { CoreMessage, LanguageModelV1 } from \"ai\";\r\nimport { generateText } from \"ai\";\r\nimport { z } from \"zod\";\r\n\r\nimport type { QueryVectorStoreResult } from \"../vector-store/parse\";\r\nimport { EVALUATE_QUERIES_PROMPT, GENERATE_QUERIES_PROMPT } from \"./prompts\";\r\n\r\nexport const formatChatHistory = (messages: CoreMessage[]) => {\r\n  return messages.map((m) => `${m.role}: ${m.content as string}`).join(\"\\n\\n\");\r\n};\r\n\r\nexport const formatSources = (sources: QueryVectorStoreResult[\"results\"]) => {\r\n  return sources\r\n    .map((s, idx) => `<source_${idx + 1}>\\n${s.text}\\n</source_${idx + 1}>`)\r\n    .join(\"\\n\\n\");\r\n};\r\n\r\nconst schema = z.object({\r\n  queries: z.array(\r\n    z.object({\r\n      type: z.enum([\"keyword\", \"semantic\"]),\r\n      query: z.string(),\r\n    }),\r\n  ),\r\n});\r\n\r\nexport type Queries = z.infer<typeof schema>[\"queries\"];\r\n\r\nexport const generateQueries = async (\r\n  model: LanguageModelV1,\r\n  messages: CoreMessage[],\r\n  oldQueries: Queries,\r\n) => {\r\n  const queriesResult = await generateText({\r\n    model,\r\n    temperature: 0,\r\n    system: GENERATE_QUERIES_PROMPT,\r\n    prompt: `\r\n${\r\n  oldQueries.length > 0\r\n    ? \"The queries you return should be different from these ones that were tried so far:\\n\" +\r\n      oldQueries.map((q) => `- ${q.query}`).join(\"\\n\")\r\n    : \"\"\r\n}\r\n\r\nChat history:\r\n${formatChatHistory(messages)}\r\n`.trim(),\r\n  });\r\n\r\n  return {\r\n    queries: schema.parse(JSON.parse(queriesResult.text)).queries,\r\n    totalTokens: queriesResult.usage.totalTokens || 0,\r\n  };\r\n};\r\n\r\nconst evalSchema = z.object({\r\n  canAnswer: z.boolean(),\r\n});\r\n\r\nexport const evaluateQueries = async (\r\n  model: LanguageModelV1,\r\n  messages: CoreMessage[],\r\n  sources: QueryVectorStoreResult[\"results\"],\r\n) => {\r\n  const evaluateQueriesResult = await generateText({\r\n    model,\r\n    temperature: 0,\r\n    system: EVALUATE_QUERIES_PROMPT,\r\n    prompt: `\r\nChat history:\r\n${formatChatHistory(messages)}\r\n\r\nRetrieved sources:\r\n${formatSources(sources)}\r\n `,\r\n  });\r\n\r\n  return {\r\n    canAnswer: evalSchema.parse(JSON.parse(evaluateQueriesResult.text))\r\n      .canAnswer,\r\n    totalTokens: evaluateQueriesResult.usage.totalTokens || 0,\r\n  };\r\n};\r\n"],"names":[],"mappings":";;;;;;AACA;AACA;AAGA;;;;AAEO,MAAM,oBAAoB,CAAC;IAChC,OAAO,SAAS,GAAG,CAAC,CAAC,IAAM,GAAG,EAAE,IAAI,CAAC,EAAE,EAAE,EAAE,OAAO,EAAY,EAAE,IAAI,CAAC;AACvE;AAEO,MAAM,gBAAgB,CAAC;IAC5B,OAAO,QACJ,GAAG,CAAC,CAAC,GAAG,MAAQ,CAAC,QAAQ,EAAE,MAAM,EAAE,GAAG,EAAE,EAAE,IAAI,CAAC,WAAW,EAAE,MAAM,EAAE,CAAC,CAAC,EACtE,IAAI,CAAC;AACV;AAEA,MAAM,SAAS,sIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IACtB,SAAS,sIAAA,CAAA,IAAC,CAAC,KAAK,CACd,sIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;QACP,MAAM,sIAAA,CAAA,IAAC,CAAC,IAAI,CAAC;YAAC;YAAW;SAAW;QACpC,OAAO,sIAAA,CAAA,IAAC,CAAC,MAAM;IACjB;AAEJ;AAIO,MAAM,kBAAkB,OAC7B,OACA,UACA;IAEA,MAAM,gBAAgB,MAAM,CAAA,GAAA,sJAAA,CAAA,eAAY,AAAD,EAAE;QACvC;QACA,aAAa;QACb,QAAQ,iJAAA,CAAA,0BAAuB;QAC/B,QAAQ,CAAC;AACb,EACE,WAAW,MAAM,GAAG,IAChB,yFACA,WAAW,GAAG,CAAC,CAAC,IAAM,CAAC,EAAE,EAAE,EAAE,KAAK,EAAE,EAAE,IAAI,CAAC,QAC3C,GACL;;;AAGD,EAAE,kBAAkB,UAAU;AAC9B,CAAC,CAAC,IAAI;IACJ;IAEA,OAAO;QACL,SAAS,OAAO,KAAK,CAAC,KAAK,KAAK,CAAC,cAAc,IAAI,GAAG,OAAO;QAC7D,aAAa,cAAc,KAAK,CAAC,WAAW,IAAI;IAClD;AACF;AAEA,MAAM,aAAa,sIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;IAC1B,WAAW,sIAAA,CAAA,IAAC,CAAC,OAAO;AACtB;AAEO,MAAM,kBAAkB,OAC7B,OACA,UACA;IAEA,MAAM,wBAAwB,MAAM,CAAA,GAAA,sJAAA,CAAA,eAAY,AAAD,EAAE;QAC/C;QACA,aAAa;QACb,QAAQ,iJAAA,CAAA,0BAAuB;QAC/B,QAAQ,CAAC;;AAEb,EAAE,kBAAkB,UAAU;;;AAG9B,EAAE,cAAc,SAAS;CACxB,CAAC;IACA;IAEA,OAAO;QACL,WAAW,WAAW,KAAK,CAAC,KAAK,KAAK,CAAC,sBAAsB,IAAI,GAC9D,SAAS;QACZ,aAAa,sBAAsB,KAAK,CAAC,WAAW,IAAI;IAC1D;AACF","debugId":null}},
    {"offset": {"line": 980, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/agentic/search.ts"],"sourcesContent":["import type { CoreMessage, LanguageModelV1 } from \"ai\";\r\n\r\nimport type { Namespace } from \"@agentset/db\";\r\n\r\nimport type {\r\n  QueryVectorStoreOptions,\r\n  QueryVectorStoreResult,\r\n} from \"../vector-store/parse\";\r\nimport type { Queries } from \"./utils\";\r\nimport { KeywordStore } from \"../keyword-store\";\r\nimport { queryVectorStore } from \"../vector-store/parse\";\r\nimport { evaluateQueries, generateQueries } from \"./utils\";\r\n\r\nexport type AgenticSearchNamespace = Pick<\r\n  Namespace,\r\n  | \"id\"\r\n  | \"vectorStoreConfig\"\r\n  | \"embeddingConfig\"\r\n  | \"createdAt\"\r\n  | \"keywordEnabled\"\r\n>;\r\n\r\nexport async function agenticSearch(\r\n  namespace: AgenticSearchNamespace,\r\n  {\r\n    model,\r\n    messages,\r\n    queryOptions,\r\n    maxEvals = 3,\r\n    tokenBudget = 4096,\r\n    onQueries,\r\n  }: {\r\n    model: LanguageModelV1;\r\n    messages: CoreMessage[];\r\n    queryOptions?: Omit<QueryVectorStoreOptions, \"query\">;\r\n    maxEvals?: number;\r\n    tokenBudget?: number;\r\n    onQueries?: (queries: Queries) => void;\r\n  },\r\n) {\r\n  const queries: Queries = [];\r\n  const chunks: Record<string, QueryVectorStoreResult[\"results\"][number]> = {};\r\n  const queryToResult: Record<string, QueryVectorStoreResult> = {};\r\n  let totalQueries = 0;\r\n  let totalTokens = 0;\r\n\r\n  const lastMessage = messages[messages.length - 1]!.content as string;\r\n\r\n  for (let i = 0; i < maxEvals; i++) {\r\n    console.log(`[EVAL LOOP] ${i + 1} / ${maxEvals}`);\r\n    console.dir(messages, { depth: null });\r\n    console.dir(queries, { depth: null });\r\n\r\n    const { queries: newQueries, totalTokens: queriesTokens } =\r\n      await generateQueries(model, messages, queries);\r\n\r\n    if (i === 0) {\r\n      newQueries.unshift({\r\n        query: lastMessage,\r\n        type: \"semantic\",\r\n      });\r\n    }\r\n\r\n    newQueries.forEach((q) => {\r\n      if (queries.includes(q)) return;\r\n      queries.push(q);\r\n    });\r\n\r\n    totalTokens += queriesTokens;\r\n\r\n    if (onQueries) onQueries(newQueries);\r\n\r\n    const data = (\r\n      await Promise.all(\r\n        newQueries.map(async (query) => {\r\n          if (namespace.keywordEnabled && query.type === \"keyword\") {\r\n            const keywordStore = new KeywordStore(\r\n              namespace.id,\r\n              queryOptions?.tenantId,\r\n            );\r\n\r\n            const keywordResult = await keywordStore.search(query.query, {\r\n              limit: 15,\r\n              includeMetadata: true,\r\n            });\r\n\r\n            totalQueries++;\r\n            return {\r\n              query: query.query,\r\n              unorderedIds: keywordResult.results.map((r) => r.id),\r\n              results: keywordResult.results,\r\n            };\r\n          }\r\n\r\n          const queryResult = await queryVectorStore(namespace, {\r\n            query: query.query,\r\n            topK: 50,\r\n            rerankLimit: 15,\r\n            rerank: true,\r\n            includeMetadata: true,\r\n            ...queryOptions,\r\n          });\r\n          totalQueries++;\r\n          return queryResult;\r\n        }),\r\n      )\r\n    ).filter((d) => d !== null);\r\n\r\n    data.forEach((d) => {\r\n      queryToResult[d.query] = d;\r\n\r\n      d.results.forEach((r) => {\r\n        if (chunks[r.id]) return;\r\n        chunks[r.id] = r;\r\n      });\r\n    });\r\n\r\n    const { canAnswer, totalTokens: evalsTokens } = await evaluateQueries(\r\n      model,\r\n      messages,\r\n      Object.values(chunks),\r\n    );\r\n    totalTokens += evalsTokens;\r\n\r\n    if (canAnswer || totalTokens >= tokenBudget) break;\r\n  }\r\n\r\n  return {\r\n    queries,\r\n    chunks,\r\n    queryToResult,\r\n    totalQueries,\r\n  };\r\n}\r\n"],"names":[],"mappings":";;;AASA;AACA;AACA;;;;AAWO,eAAe,cACpB,SAAiC,EACjC,EACE,KAAK,EACL,QAAQ,EACR,YAAY,EACZ,WAAW,CAAC,EACZ,cAAc,IAAI,EAClB,SAAS,EAQV;IAED,MAAM,UAAmB,EAAE;IAC3B,MAAM,SAAoE,CAAC;IAC3E,MAAM,gBAAwD,CAAC;IAC/D,IAAI,eAAe;IACnB,IAAI,cAAc;IAElB,MAAM,cAAc,QAAQ,CAAC,SAAS,MAAM,GAAG,EAAE,CAAE,OAAO;IAE1D,IAAK,IAAI,IAAI,GAAG,IAAI,UAAU,IAAK;QACjC,QAAQ,GAAG,CAAC,CAAC,YAAY,EAAE,IAAI,EAAE,GAAG,EAAE,UAAU;QAChD,QAAQ,GAAG,CAAC,UAAU;YAAE,OAAO;QAAK;QACpC,QAAQ,GAAG,CAAC,SAAS;YAAE,OAAO;QAAK;QAEnC,MAAM,EAAE,SAAS,UAAU,EAAE,aAAa,aAAa,EAAE,GACvD,MAAM,CAAA,GAAA,+IAAA,CAAA,kBAAe,AAAD,EAAE,OAAO,UAAU;QAEzC,IAAI,MAAM,GAAG;YACX,WAAW,OAAO,CAAC;gBACjB,OAAO;gBACP,MAAM;YACR;QACF;QAEA,WAAW,OAAO,CAAC,CAAC;YAClB,IAAI,QAAQ,QAAQ,CAAC,IAAI;YACzB,QAAQ,IAAI,CAAC;QACf;QAEA,eAAe;QAEf,IAAI,WAAW,UAAU;QAEzB,MAAM,OAAO,CACX,MAAM,QAAQ,GAAG,CACf,WAAW,GAAG,CAAC,OAAO;YACpB,IAAI,UAAU,cAAc,IAAI,MAAM,IAAI,KAAK,WAAW;gBACxD,MAAM,eAAe,IAAI,wJAAA,CAAA,eAAY,CACnC,UAAU,EAAE,EACZ,cAAc;gBAGhB,MAAM,gBAAgB,MAAM,aAAa,MAAM,CAAC,MAAM,KAAK,EAAE;oBAC3D,OAAO;oBACP,iBAAiB;gBACnB;gBAEA;gBACA,OAAO;oBACL,OAAO,MAAM,KAAK;oBAClB,cAAc,cAAc,OAAO,CAAC,GAAG,CAAC,CAAC,IAAM,EAAE,EAAE;oBACnD,SAAS,cAAc,OAAO;gBAChC;YACF;YAEA,MAAM,cAAc,MAAM,CAAA,GAAA,uJAAA,CAAA,mBAAgB,AAAD,EAAE,WAAW;gBACpD,OAAO,MAAM,KAAK;gBAClB,MAAM;gBACN,aAAa;gBACb,QAAQ;gBACR,iBAAiB;gBACjB,GAAG,YAAY;YACjB;YACA;YACA,OAAO;QACT,GAEJ,EAAE,MAAM,CAAC,CAAC,IAAM,MAAM;QAEtB,KAAK,OAAO,CAAC,CAAC;YACZ,aAAa,CAAC,EAAE,KAAK,CAAC,GAAG;YAEzB,EAAE,OAAO,CAAC,OAAO,CAAC,CAAC;gBACjB,IAAI,MAAM,CAAC,EAAE,EAAE,CAAC,EAAE;gBAClB,MAAM,CAAC,EAAE,EAAE,CAAC,GAAG;YACjB;QACF;QAEA,MAAM,EAAE,SAAS,EAAE,aAAa,WAAW,EAAE,GAAG,MAAM,CAAA,GAAA,+IAAA,CAAA,kBAAe,AAAD,EAClE,OACA,UACA,OAAO,MAAM,CAAC;QAEhB,eAAe;QAEf,IAAI,aAAa,eAAe,aAAa;IAC/C;IAEA,OAAO;QACL;QACA;QACA;QACA;IACF;AACF","debugId":null}},
    {"offset": {"line": 1066, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/agentic/index.ts"],"sourcesContent":["import type { CoreMessage, JSONValue, LanguageModelV1 } from \"ai\";\r\nimport { createDataStreamResponse, generateText, streamText } from \"ai\";\r\n\r\nimport type { QueryVectorStoreOptions } from \"../vector-store/parse\";\r\nimport { NEW_MESSAGE_PROMPT } from \"../prompts\";\r\nimport { agenticSearch, AgenticSearchNamespace } from \"./search\";\r\nimport { formatSources } from \"./utils\";\r\n\r\ntype AgenticPipelineOptions = {\r\n  model: LanguageModelV1;\r\n  queryOptions?: Omit<QueryVectorStoreOptions, \"query\">;\r\n  systemPrompt?: string;\r\n  temperature?: number;\r\n  messagesWithoutQuery: CoreMessage[];\r\n  lastMessage: string;\r\n  afterQueries?: (totalQueries: number) => void;\r\n  maxEvals?: number;\r\n  tokenBudget?: number;\r\n};\r\n\r\nconst agenticPipeline = (\r\n  namespace: AgenticSearchNamespace,\r\n  {\r\n    model,\r\n    queryOptions,\r\n    headers,\r\n    systemPrompt,\r\n    temperature,\r\n    messagesWithoutQuery,\r\n    lastMessage,\r\n    afterQueries,\r\n    maxEvals = 3,\r\n    tokenBudget = 4096,\r\n    includeLogs = true,\r\n  }: AgenticPipelineOptions & {\r\n    headers?: HeadersInit;\r\n    afterQueries?: (totalQueries: number) => void;\r\n    includeLogs?: boolean;\r\n  },\r\n) => {\r\n  const messages: CoreMessage[] = [\r\n    ...messagesWithoutQuery,\r\n    { role: \"user\", content: lastMessage },\r\n  ];\r\n\r\n  return createDataStreamResponse({\r\n    execute: async (dataStream) => {\r\n      dataStream.writeMessageAnnotation({\r\n        type: \"status\",\r\n        value: \"generating-queries\",\r\n      });\r\n\r\n      // step 1. generate queries\r\n      const { chunks, queryToResult, totalQueries } = await agenticSearch(\r\n        namespace,\r\n        {\r\n          model,\r\n          messages,\r\n          queryOptions,\r\n          maxEvals,\r\n          tokenBudget,\r\n          onQueries: (newQueries) => {\r\n            dataStream.writeMessageAnnotation({\r\n              type: \"status\",\r\n              value: \"searching\",\r\n              queries: newQueries,\r\n            });\r\n          },\r\n        },\r\n      );\r\n\r\n      afterQueries?.(totalQueries);\r\n\r\n      dataStream.writeMessageAnnotation({\r\n        type: \"status\",\r\n        value: \"generating-answer\",\r\n      });\r\n\r\n      // TODO: shrink chunks and only select relevant ones to pass to the LLM\r\n      const dedupedData = Object.values(chunks);\r\n      const newMessages: CoreMessage[] = [\r\n        ...messagesWithoutQuery,\r\n        {\r\n          role: \"user\",\r\n          content: NEW_MESSAGE_PROMPT.compile({\r\n            chunks: formatSources(dedupedData),\r\n            // put the original query in the message to help with context\r\n            query: `<query>${lastMessage}</query>`,\r\n          }),\r\n        },\r\n      ];\r\n\r\n      const messageStream = streamText({\r\n        model,\r\n        system: systemPrompt,\r\n        messages: newMessages,\r\n        temperature,\r\n        onError: (error) => {\r\n          console.error(error);\r\n        },\r\n      });\r\n\r\n      dataStream.writeMessageAnnotation({\r\n        type: \"agentset_sources\",\r\n        value: { results: dedupedData } as unknown as JSONValue,\r\n        ...(includeLogs && {\r\n          logs: Object.values(queryToResult) as unknown as JSONValue,\r\n        }),\r\n      });\r\n      messageStream.mergeIntoDataStream(dataStream);\r\n    },\r\n    onError(error) {\r\n      console.error(error);\r\n      return \"An error occurred\";\r\n    },\r\n\r\n    headers,\r\n  });\r\n};\r\n\r\nexport const generateAgenticResponse = async (\r\n  namespace: AgenticSearchNamespace,\r\n  {\r\n    model,\r\n    queryOptions,\r\n    systemPrompt,\r\n    temperature,\r\n    messagesWithoutQuery,\r\n    lastMessage,\r\n    afterQueries,\r\n    maxEvals = 3,\r\n    tokenBudget = 4096,\r\n  }: AgenticPipelineOptions,\r\n) => {\r\n  const messages: CoreMessage[] = [\r\n    ...messagesWithoutQuery,\r\n    { role: \"user\", content: lastMessage },\r\n  ];\r\n\r\n  // step 1. generate queries\r\n  const { chunks, totalQueries } = await agenticSearch(namespace, {\r\n    model,\r\n    messages,\r\n    queryOptions,\r\n    maxEvals,\r\n    tokenBudget,\r\n  });\r\n\r\n  afterQueries?.(totalQueries);\r\n\r\n  // TODO: shrink chunks and only select relevant ones to pass to the LLM\r\n  const dedupedData = Object.values(chunks);\r\n  const newMessages: CoreMessage[] = [\r\n    ...messagesWithoutQuery,\r\n    {\r\n      role: \"user\",\r\n      content: NEW_MESSAGE_PROMPT.compile({\r\n        chunks: formatSources(dedupedData),\r\n        // put the original query in the message to help with context\r\n        query: `<query>${lastMessage}</query>`,\r\n      }),\r\n    },\r\n  ];\r\n\r\n  const answer = await generateText({\r\n    model: model,\r\n    system: systemPrompt,\r\n    messages: newMessages,\r\n    temperature: temperature,\r\n  });\r\n\r\n  return {\r\n    answer: answer.text,\r\n    sources: dedupedData,\r\n  };\r\n};\r\n\r\nexport default agenticPipeline;\r\n"],"names":[],"mappings":";;;;AACA;AAGA;AACA;AACA;;;;;AAcA,MAAM,kBAAkB,CACtB,WACA,EACE,KAAK,EACL,YAAY,EACZ,OAAO,EACP,YAAY,EACZ,WAAW,EACX,oBAAoB,EACpB,WAAW,EACX,YAAY,EACZ,WAAW,CAAC,EACZ,cAAc,IAAI,EAClB,cAAc,IAAI,EAKnB;IAED,MAAM,WAA0B;WAC3B;QACH;YAAE,MAAM;YAAQ,SAAS;QAAY;KACtC;IAED,OAAO,CAAA,GAAA,sJAAA,CAAA,2BAAwB,AAAD,EAAE;QAC9B,SAAS,OAAO;YACd,WAAW,sBAAsB,CAAC;gBAChC,MAAM;gBACN,OAAO;YACT;YAEA,2BAA2B;YAC3B,MAAM,EAAE,MAAM,EAAE,aAAa,EAAE,YAAY,EAAE,GAAG,MAAM,CAAA,GAAA,gJAAA,CAAA,gBAAa,AAAD,EAChE,WACA;gBACE;gBACA;gBACA;gBACA;gBACA;gBACA,WAAW,CAAC;oBACV,WAAW,sBAAsB,CAAC;wBAChC,MAAM;wBACN,OAAO;wBACP,SAAS;oBACX;gBACF;YACF;YAGF,eAAe;YAEf,WAAW,sBAAsB,CAAC;gBAChC,MAAM;gBACN,OAAO;YACT;YAEA,uEAAuE;YACvE,MAAM,cAAc,OAAO,MAAM,CAAC;YAClC,MAAM,cAA6B;mBAC9B;gBACH;oBACE,MAAM;oBACN,SAAS,sIAAA,CAAA,qBAAkB,CAAC,OAAO,CAAC;wBAClC,QAAQ,CAAA,GAAA,+IAAA,CAAA,gBAAa,AAAD,EAAE;wBACtB,6DAA6D;wBAC7D,OAAO,CAAC,OAAO,EAAE,YAAY,QAAQ,CAAC;oBACxC;gBACF;aACD;YAED,MAAM,gBAAgB,CAAA,GAAA,sJAAA,CAAA,aAAU,AAAD,EAAE;gBAC/B;gBACA,QAAQ;gBACR,UAAU;gBACV;gBACA,SAAS,CAAC;oBACR,QAAQ,KAAK,CAAC;gBAChB;YACF;YAEA,WAAW,sBAAsB,CAAC;gBAChC,MAAM;gBACN,OAAO;oBAAE,SAAS;gBAAY;gBAC9B,GAAI,eAAe;oBACjB,MAAM,OAAO,MAAM,CAAC;gBACtB,CAAC;YACH;YACA,cAAc,mBAAmB,CAAC;QACpC;QACA,SAAQ,KAAK;YACX,QAAQ,KAAK,CAAC;YACd,OAAO;QACT;QAEA;IACF;AACF;AAEO,MAAM,0BAA0B,OACrC,WACA,EACE,KAAK,EACL,YAAY,EACZ,YAAY,EACZ,WAAW,EACX,oBAAoB,EACpB,WAAW,EACX,YAAY,EACZ,WAAW,CAAC,EACZ,cAAc,IAAI,EACK;IAEzB,MAAM,WAA0B;WAC3B;QACH;YAAE,MAAM;YAAQ,SAAS;QAAY;KACtC;IAED,2BAA2B;IAC3B,MAAM,EAAE,MAAM,EAAE,YAAY,EAAE,GAAG,MAAM,CAAA,GAAA,gJAAA,CAAA,gBAAa,AAAD,EAAE,WAAW;QAC9D;QACA;QACA;QACA;QACA;IACF;IAEA,eAAe;IAEf,uEAAuE;IACvE,MAAM,cAAc,OAAO,MAAM,CAAC;IAClC,MAAM,cAA6B;WAC9B;QACH;YACE,MAAM;YACN,SAAS,sIAAA,CAAA,qBAAkB,CAAC,OAAO,CAAC;gBAClC,QAAQ,CAAA,GAAA,+IAAA,CAAA,gBAAa,AAAD,EAAE;gBACtB,6DAA6D;gBAC7D,OAAO,CAAC,OAAO,EAAE,YAAY,QAAQ,CAAC;YACxC;QACF;KACD;IAED,MAAM,SAAS,MAAM,CAAA,GAAA,sJAAA,CAAA,eAAY,AAAD,EAAE;QAChC,OAAO;QACP,QAAQ;QACR,UAAU;QACV,aAAa;IACf;IAEA,OAAO;QACL,QAAQ,OAAO,IAAI;QACnB,SAAS;IACX;AACF;uCAEe","debugId":null}},
    {"offset": {"line": 1208, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/zod/index.ts"],"sourcesContent":["import * as z from \"zod\";\r\nimport { extendZodWithOpenApi } from \"zod-openapi\";\r\n\r\nextendZodWithOpenApi(z);\r\n\r\nexport default z;\r\n"],"names":[],"mappings":";;;AAAA;AACA;;;AAEA,CAAA,GAAA,+JAAA,CAAA,uBAAoB,AAAD,EAAE;uCAEN","debugId":null}},
    {"offset": {"line": 1223, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/slug.ts"],"sourcesContent":["import _slugify from \"slugify\";\r\n\r\nexport const validSlugRegex = new RegExp(/^[a-zA-Z0-9\\-]+$/);\r\n\r\nexport const toSlug = (text: string, suffix?: string) => {\r\n  return (\r\n    _slugify(text, {\r\n      lower: true,\r\n      strict: true,\r\n      trim: true,\r\n    }) + (suffix ? `-${suffix}` : \"\")\r\n  );\r\n};\r\n"],"names":[],"mappings":";;;;AAAA;;AAEO,MAAM,iBAAiB,IAAI,OAAO;AAElC,MAAM,SAAS,CAAC,MAAc;IACnC,OACE,CAAA,GAAA,oIAAA,CAAA,UAAQ,AAAD,EAAE,MAAM;QACb,OAAO;QACP,QAAQ;QACR,MAAM;IACR,KAAK,CAAC,SAAS,CAAC,CAAC,EAAE,QAAQ,GAAG,EAAE;AAEpC","debugId":null}},
    {"offset": {"line": 1243, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/string-utils.ts"],"sourcesContent":["import { toSlug } from \"./slug\";\r\n\r\nconst tokenCharacters =\r\n  \"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789\";\r\nconst tokenCharactersLength = tokenCharacters.length;\r\n\r\nexport function generateToken(length: number) {\r\n  let result = \"\";\r\n  for (let i = 0; i < length; i++)\r\n    result += tokenCharacters.charAt(\r\n      Math.floor(Math.random() * tokenCharactersLength),\r\n    );\r\n  return result;\r\n}\r\n\r\nexport function truncate(str: string, length: number) {\r\n  if (str.length <= length) return str;\r\n  return str.slice(0, length);\r\n}\r\n\r\nexport function filenamize(value: string, length = 20) {\r\n  const token = generateToken(4);\r\n  const slug = toSlug(value, token);\r\n  const hyphenCount = (slug.match(/-/g) || []).length;\r\n  length = length + hyphenCount + token.length;\r\n\r\n  return truncate(slug, length);\r\n}\r\n\r\nexport function capitalize(str?: string | null) {\r\n  if (!str || typeof str !== \"string\") return str;\r\n\r\n  return str\r\n    .split(\" \")\r\n    .map(\r\n      (word) =>\r\n        word.charAt(0).toUpperCase() +\r\n        (word.length > 1 ? word.slice(1).toLowerCase() : \"\"),\r\n    )\r\n    .join(\" \");\r\n}\r\n\r\nexport function camelCaseToWords(str: string) {\r\n  return str\r\n    .split(/(?=[A-Z])/)\r\n    .map(capitalize)\r\n    .join(\" \");\r\n}\r\n\r\nexport function sanitizeText(text: string) {\r\n  return text.replaceAll(\"<has_function_call>\", \"\");\r\n}\r\n"],"names":[],"mappings":";;;;;;;;AAAA;;AAEA,MAAM,kBACJ;AACF,MAAM,wBAAwB,gBAAgB,MAAM;AAE7C,SAAS,cAAc,MAAc;IAC1C,IAAI,SAAS;IACb,IAAK,IAAI,IAAI,GAAG,IAAI,QAAQ,IAC1B,UAAU,gBAAgB,MAAM,CAC9B,KAAK,KAAK,CAAC,KAAK,MAAM,KAAK;IAE/B,OAAO;AACT;AAEO,SAAS,SAAS,GAAW,EAAE,MAAc;IAClD,IAAI,IAAI,MAAM,IAAI,QAAQ,OAAO;IACjC,OAAO,IAAI,KAAK,CAAC,GAAG;AACtB;AAEO,SAAS,WAAW,KAAa,EAAE,SAAS,EAAE;IACnD,MAAM,QAAQ,cAAc;IAC5B,MAAM,OAAO,CAAA,GAAA,mIAAA,CAAA,SAAM,AAAD,EAAE,OAAO;IAC3B,MAAM,cAAc,CAAC,KAAK,KAAK,CAAC,SAAS,EAAE,EAAE,MAAM;IACnD,SAAS,SAAS,cAAc,MAAM,MAAM;IAE5C,OAAO,SAAS,MAAM;AACxB;AAEO,SAAS,WAAW,GAAmB;IAC5C,IAAI,CAAC,OAAO,OAAO,QAAQ,UAAU,OAAO;IAE5C,OAAO,IACJ,KAAK,CAAC,KACN,GAAG,CACF,CAAC,OACC,KAAK,MAAM,CAAC,GAAG,WAAW,KAC1B,CAAC,KAAK,MAAM,GAAG,IAAI,KAAK,KAAK,CAAC,GAAG,WAAW,KAAK,EAAE,GAEtD,IAAI,CAAC;AACV;AAEO,SAAS,iBAAiB,GAAW;IAC1C,OAAO,IACJ,KAAK,CAAC,aACN,GAAG,CAAC,YACJ,IAAI,CAAC;AACV;AAEO,SAAS,aAAa,IAAY;IACvC,OAAO,KAAK,UAAU,CAAC,uBAAuB;AAChD","debugId":null}},
    {"offset": {"line": 1287, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/errors.ts"],"sourcesContent":["import type { ZodOpenApiResponseObject } from \"zod-openapi\";\r\nimport { NextResponse } from \"next/server\";\r\nimport z from \"@/lib/zod\";\r\nimport { ZodError } from \"zod\";\r\nimport { generateErrorMessage } from \"zod-error\";\r\n\r\nimport { capitalize } from \"../string-utils\";\r\n\r\nexport const ErrorCode = z.enum([\r\n  \"bad_request\",\r\n  \"not_found\",\r\n  \"internal_server_error\",\r\n  \"unauthorized\",\r\n  \"forbidden\",\r\n  \"rate_limit_exceeded\",\r\n  \"invite_expired\",\r\n  \"invite_pending\",\r\n  \"exceeded_limit\",\r\n  \"conflict\",\r\n  \"unprocessable_entity\",\r\n]);\r\n\r\nconst docsBase = \"https://docs.agentset.com\";\r\n\r\nconst errorCodeToHttpStatus: Record<z.infer<typeof ErrorCode>, number> = {\r\n  bad_request: 400,\r\n  unauthorized: 401,\r\n  forbidden: 403,\r\n  exceeded_limit: 403,\r\n  not_found: 404,\r\n  conflict: 409,\r\n  invite_pending: 409,\r\n  invite_expired: 410,\r\n  unprocessable_entity: 422,\r\n  rate_limit_exceeded: 429,\r\n  internal_server_error: 500,\r\n};\r\n\r\nexport const httpStatusToErrorCode = Object.fromEntries(\r\n  Object.entries(errorCodeToHttpStatus).map(([code, status]) => [status, code]),\r\n) as Record<number, z.infer<typeof ErrorCode>>;\r\n\r\nconst speakeasyErrorOverrides: Record<z.infer<typeof ErrorCode>, string> = {\r\n  bad_request: \"BadRequest\",\r\n  unauthorized: \"Unauthorized\",\r\n  forbidden: \"Forbidden\",\r\n  exceeded_limit: \"ExceededLimit\",\r\n  not_found: \"NotFound\",\r\n  conflict: \"Conflict\",\r\n  invite_pending: \"InvitePending\",\r\n  invite_expired: \"InviteExpired\",\r\n  unprocessable_entity: \"UnprocessableEntity\",\r\n  rate_limit_exceeded: \"RateLimitExceeded\",\r\n  internal_server_error: \"InternalServerError\",\r\n};\r\n\r\nconst _ErrorSchema = z.object({\r\n  success: z.literal(false),\r\n  error: z.object({\r\n    code: ErrorCode.openapi({\r\n      description: \"A short code indicating the error code returned.\",\r\n      example: \"not_found\",\r\n    }),\r\n    message: z.string().openapi({\r\n      description: \"A human readable error message.\",\r\n      example: \"The requested resource was not found.\",\r\n    }),\r\n    doc_url: z\r\n      .string()\r\n      .optional()\r\n      .openapi({\r\n        description: \"A URL to more information about the error code reported.\",\r\n        example: `${docsBase}/api-reference`,\r\n      }),\r\n  }),\r\n});\r\n\r\nexport type ErrorResponse = z.infer<typeof _ErrorSchema>;\r\nexport type ErrorCodes = z.infer<typeof ErrorCode>;\r\n\r\nexport class AgentsetApiError extends Error {\r\n  public readonly code: z.infer<typeof ErrorCode>;\r\n  public readonly docUrl?: string;\r\n\r\n  constructor({\r\n    code,\r\n    message,\r\n    docUrl,\r\n  }: {\r\n    code: z.infer<typeof ErrorCode>;\r\n    message: string;\r\n    docUrl?: string;\r\n  }) {\r\n    super(message);\r\n    this.code = code;\r\n    this.docUrl = docUrl ?? `${docErrorUrl}#${code.replace(\"_\", \"-\")}`;\r\n  }\r\n}\r\n\r\nconst docErrorUrl = `${docsBase}/api-reference/errors`;\r\n\r\nexport function fromZodError(error: ZodError): Pick<ErrorResponse, \"error\"> {\r\n  return {\r\n    error: {\r\n      code: \"unprocessable_entity\",\r\n      message: generateErrorMessage(error.issues, {\r\n        maxErrors: 1,\r\n        delimiter: {\r\n          component: \": \",\r\n        },\r\n        path: {\r\n          enabled: true,\r\n          type: \"objectNotation\",\r\n          label: \"\",\r\n        },\r\n        code: {\r\n          enabled: true,\r\n          label: \"\",\r\n        },\r\n        message: {\r\n          enabled: true,\r\n          label: \"\",\r\n        },\r\n      }),\r\n      doc_url: `${docErrorUrl}#unprocessable-entity`,\r\n    },\r\n  };\r\n}\r\n\r\nexport function handleApiError(\r\n  error: any,\r\n): Pick<ErrorResponse, \"error\"> & { status: number } {\r\n  console.error(\"API error occurred\", error.message);\r\n\r\n  // Zod errors\r\n  if (error instanceof ZodError) {\r\n    return {\r\n      ...fromZodError(error),\r\n      status: errorCodeToHttpStatus.unprocessable_entity,\r\n    };\r\n  }\r\n\r\n  // AgentsetApiError errors\r\n  if (error instanceof AgentsetApiError) {\r\n    return {\r\n      error: {\r\n        code: error.code,\r\n        message: error.message,\r\n        doc_url: error.docUrl,\r\n      },\r\n      status: errorCodeToHttpStatus[error.code],\r\n    };\r\n  }\r\n\r\n  // Prisma record not found error\r\n  if (error.code === \"P2025\") {\r\n    return {\r\n      error: {\r\n        code: \"not_found\",\r\n        message:\r\n          error?.meta?.cause ||\r\n          error.message ||\r\n          \"The requested resource was not found.\",\r\n        doc_url: `${docErrorUrl}#not-found`,\r\n      },\r\n      status: 404,\r\n    };\r\n  }\r\n\r\n  // Fallback\r\n  // Unhandled errors are not user-facing, so we don't expose the actual error\r\n  return {\r\n    error: {\r\n      code: \"internal_server_error\",\r\n      message:\r\n        \"An internal server error occurred. Please contact our support if the problem persists.\",\r\n      doc_url: `${docErrorUrl}#internal-server-error`,\r\n    },\r\n    status: 500,\r\n  };\r\n}\r\n\r\nexport function handleAndReturnErrorResponse(\r\n  err: unknown,\r\n  headers?: Record<string, string>,\r\n) {\r\n  const { error, status } = handleApiError(err);\r\n  return NextResponse.json<ErrorResponse>(\r\n    { success: false, error },\r\n    { headers, status },\r\n  );\r\n}\r\n\r\nexport const errorSchemaFactory = (\r\n  code: z.infer<typeof ErrorCode>,\r\n  description: string,\r\n): ZodOpenApiResponseObject => {\r\n  return {\r\n    description,\r\n    content: {\r\n      \"application/json\": {\r\n        schema: {\r\n          \"x-speakeasy-name-override\": speakeasyErrorOverrides[code],\r\n          type: \"object\",\r\n          properties: {\r\n            success: {\r\n              type: \"boolean\",\r\n              example: false,\r\n            },\r\n            error: {\r\n              type: \"object\",\r\n              properties: {\r\n                code: {\r\n                  type: \"string\",\r\n                  enum: [code],\r\n                  description:\r\n                    \"A short code indicating the error code returned.\",\r\n                  example: code,\r\n                },\r\n                message: {\r\n                  \"x-speakeasy-error-message\": true,\r\n                  type: \"string\",\r\n                  description:\r\n                    \"A human readable explanation of what went wrong.\",\r\n                  example: \"The requested resource was not found.\",\r\n                },\r\n                doc_url: {\r\n                  type: \"string\",\r\n                  description:\r\n                    \"A link to our documentation with more details about this error code\",\r\n                  example: `${docErrorUrl}#${code.replace(\"_\", \"-\")}`,\r\n                },\r\n              },\r\n              required: [\"code\", \"message\"],\r\n            },\r\n          },\r\n          required: [\"success\", \"error\"],\r\n        },\r\n      },\r\n    },\r\n  };\r\n};\r\n\r\nexport const exceededLimitError = ({\r\n  plan,\r\n  limit,\r\n  type,\r\n}: {\r\n  plan: string;\r\n  limit: number;\r\n  type: \"retrievals\" | \"api\" | \"pages\";\r\n}) => {\r\n  return `You've reached your ${\r\n    type === \"retrievals\" ? \"monthly\" : \"\"\r\n  } limit of ${limit} ${\r\n    limit === 1 ? type.slice(0, -1) : type\r\n  } on the ${capitalize(plan)} plan. Please upgrade to add more ${type}.`;\r\n};\r\n"],"names":[],"mappings":";;;;;;;;;;AACA;AACA;AACA;AACA;AAEA;;;;;;AAEO,MAAM,YAAY,2IAAA,CAAA,UAAC,CAAC,IAAI,CAAC;IAC9B;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;IACA;CACD;AAED,MAAM,WAAW;AAEjB,MAAM,wBAAmE;IACvE,aAAa;IACb,cAAc;IACd,WAAW;IACX,gBAAgB;IAChB,WAAW;IACX,UAAU;IACV,gBAAgB;IAChB,gBAAgB;IAChB,sBAAsB;IACtB,qBAAqB;IACrB,uBAAuB;AACzB;AAEO,MAAM,wBAAwB,OAAO,WAAW,CACrD,OAAO,OAAO,CAAC,uBAAuB,GAAG,CAAC,CAAC,CAAC,MAAM,OAAO,GAAK;QAAC;QAAQ;KAAK;AAG9E,MAAM,0BAAqE;IACzE,aAAa;IACb,cAAc;IACd,WAAW;IACX,gBAAgB;IAChB,WAAW;IACX,UAAU;IACV,gBAAgB;IAChB,gBAAgB;IAChB,sBAAsB;IACtB,qBAAqB;IACrB,uBAAuB;AACzB;AAEA,MAAM,eAAe,2IAAA,CAAA,UAAC,CAAC,MAAM,CAAC;IAC5B,SAAS,2IAAA,CAAA,UAAC,CAAC,OAAO,CAAC;IACnB,OAAO,2IAAA,CAAA,UAAC,CAAC,MAAM,CAAC;QACd,MAAM,UAAU,OAAO,CAAC;YACtB,aAAa;YACb,SAAS;QACX;QACA,SAAS,2IAAA,CAAA,UAAC,CAAC,MAAM,GAAG,OAAO,CAAC;YAC1B,aAAa;YACb,SAAS;QACX;QACA,SAAS,2IAAA,CAAA,UAAC,CACP,MAAM,GACN,QAAQ,GACR,OAAO,CAAC;YACP,aAAa;YACb,SAAS,GAAG,SAAS,cAAc,CAAC;QACtC;IACJ;AACF;AAKO,MAAM,yBAAyB;IACpB,KAAgC;IAChC,OAAgB;IAEhC,YAAY,EACV,IAAI,EACJ,OAAO,EACP,MAAM,EAKP,CAAE;QACD,KAAK,CAAC;QACN,IAAI,CAAC,IAAI,GAAG;QACZ,IAAI,CAAC,MAAM,GAAG,UAAU,GAAG,YAAY,CAAC,EAAE,KAAK,OAAO,CAAC,KAAK,MAAM;IACpE;AACF;AAEA,MAAM,cAAc,GAAG,SAAS,qBAAqB,CAAC;AAE/C,SAAS,aAAa,KAAe;IAC1C,OAAO;QACL,OAAO;YACL,MAAM;YACN,SAAS,CAAA,GAAA,8IAAA,CAAA,uBAAoB,AAAD,EAAE,MAAM,MAAM,EAAE;gBAC1C,WAAW;gBACX,WAAW;oBACT,WAAW;gBACb;gBACA,MAAM;oBACJ,SAAS;oBACT,MAAM;oBACN,OAAO;gBACT;gBACA,MAAM;oBACJ,SAAS;oBACT,OAAO;gBACT;gBACA,SAAS;oBACP,SAAS;oBACT,OAAO;gBACT;YACF;YACA,SAAS,GAAG,YAAY,qBAAqB,CAAC;QAChD;IACF;AACF;AAEO,SAAS,eACd,KAAU;IAEV,QAAQ,KAAK,CAAC,sBAAsB,MAAM,OAAO;IAEjD,aAAa;IACb,IAAI,iBAAiB,sIAAA,CAAA,WAAQ,EAAE;QAC7B,OAAO;YACL,GAAG,aAAa,MAAM;YACtB,QAAQ,sBAAsB,oBAAoB;QACpD;IACF;IAEA,0BAA0B;IAC1B,IAAI,iBAAiB,kBAAkB;QACrC,OAAO;YACL,OAAO;gBACL,MAAM,MAAM,IAAI;gBAChB,SAAS,MAAM,OAAO;gBACtB,SAAS,MAAM,MAAM;YACvB;YACA,QAAQ,qBAAqB,CAAC,MAAM,IAAI,CAAC;QAC3C;IACF;IAEA,gCAAgC;IAChC,IAAI,MAAM,IAAI,KAAK,SAAS;QAC1B,OAAO;YACL,OAAO;gBACL,MAAM;gBACN,SACE,OAAO,MAAM,SACb,MAAM,OAAO,IACb;gBACF,SAAS,GAAG,YAAY,UAAU,CAAC;YACrC;YACA,QAAQ;QACV;IACF;IAEA,WAAW;IACX,4EAA4E;IAC5E,OAAO;QACL,OAAO;YACL,MAAM;YACN,SACE;YACF,SAAS,GAAG,YAAY,sBAAsB,CAAC;QACjD;QACA,QAAQ;IACV;AACF;AAEO,SAAS,6BACd,GAAY,EACZ,OAAgC;IAEhC,MAAM,EAAE,KAAK,EAAE,MAAM,EAAE,GAAG,eAAe;IACzC,OAAO,gIAAA,CAAA,eAAY,CAAC,IAAI,CACtB;QAAE,SAAS;QAAO;IAAM,GACxB;QAAE;QAAS;IAAO;AAEtB;AAEO,MAAM,qBAAqB,CAChC,MACA;IAEA,OAAO;QACL;QACA,SAAS;YACP,oBAAoB;gBAClB,QAAQ;oBACN,6BAA6B,uBAAuB,CAAC,KAAK;oBAC1D,MAAM;oBACN,YAAY;wBACV,SAAS;4BACP,MAAM;4BACN,SAAS;wBACX;wBACA,OAAO;4BACL,MAAM;4BACN,YAAY;gCACV,MAAM;oCACJ,MAAM;oCACN,MAAM;wCAAC;qCAAK;oCACZ,aACE;oCACF,SAAS;gCACX;gCACA,SAAS;oCACP,6BAA6B;oCAC7B,MAAM;oCACN,aACE;oCACF,SAAS;gCACX;gCACA,SAAS;oCACP,MAAM;oCACN,aACE;oCACF,SAAS,GAAG,YAAY,CAAC,EAAE,KAAK,OAAO,CAAC,KAAK,MAAM;gCACrD;4BACF;4BACA,UAAU;gCAAC;gCAAQ;6BAAU;wBAC/B;oBACF;oBACA,UAAU;wBAAC;wBAAW;qBAAQ;gBAChC;YACF;QACF;IACF;AACF;AAEO,MAAM,qBAAqB,CAAC,EACjC,IAAI,EACJ,KAAK,EACL,IAAI,EAKL;IACC,OAAO,CAAC,oBAAoB,EAC1B,SAAS,eAAe,YAAY,GACrC,UAAU,EAAE,MAAM,CAAC,EAClB,UAAU,IAAI,KAAK,KAAK,CAAC,GAAG,CAAC,KAAK,KACnC,QAAQ,EAAE,CAAA,GAAA,8IAAA,CAAA,aAAU,AAAD,EAAE,MAAM,kCAAkC,EAAE,KAAK,CAAC,CAAC;AACzE","debugId":null}},
    {"offset": {"line": 1517, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/redis.ts"],"sourcesContent":["import { env } from \"@/env\";\nimport { Redis } from \"@upstash/redis\";\n\n// O cliente @upstash/redis espera uma URL HTTPS, nÃ£o uma URL redis://\n// Formato correto: https://flowing-fish-11567.upstash.io\nexport const redis = new Redis({\n  url: \"https://flowing-fish-11567.upstash.io\",\n  token: env.REDIS_TOKEN,\n});\n"],"names":[],"mappings":";;;AAAA;AACA;AAAA;;;AAIO,MAAM,QAAQ,IAAI,+JAAA,CAAA,QAAK,CAAC;IAC7B,KAAK;IACL,OAAO,2HAAA,CAAA,MAAG,CAAC,WAAW;AACxB","debugId":null}},
    {"offset": {"line": 1535, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/rate-limit.ts"],"sourcesContent":["import { Ratelimit } from \"@upstash/ratelimit\";\r\n\r\nimport { redis } from \"../redis\";\r\n\r\n// Create a new ratelimiter, that allows 10 requests per 10 seconds by default\r\nexport const ratelimit = (\r\n  requests: number = 10,\r\n  seconds:\r\n    | `${number} ms`\r\n    | `${number} s`\r\n    | `${number} m`\r\n    | `${number} h`\r\n    | `${number} d` = \"10 s\",\r\n) => {\r\n  return new Ratelimit({\r\n    redis: redis,\r\n    limiter: Ratelimit.slidingWindow(requests, seconds),\r\n    analytics: true,\r\n    prefix: \"agentset\",\r\n  });\r\n};\r\n"],"names":[],"mappings":";;;AAAA;AAEA;;;AAGO,MAAM,YAAY,CACvB,WAAmB,EAAE,EACrB,UAKoB,MAAM;IAE1B,OAAO,IAAI,yJAAA,CAAA,YAAS,CAAC;QACnB,OAAO,oIAAA,CAAA,QAAK;QACZ,SAAS,yJAAA,CAAA,YAAS,CAAC,aAAa,CAAC,UAAU;QAC3C,WAAW;QACX,QAAQ;IACV;AACF","debugId":null}},
    {"offset": {"line": 1564, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/packages/db/src/client.ts"],"sourcesContent":["import { neonConfig } from \"@neondatabase/serverless\";\r\nimport { PrismaNeon } from \"@prisma/adapter-neon\";\r\nimport { PrismaClient } from \"@prisma/client\";\r\n\r\nconst createPrismaClient = () => {\r\n  // Supabase pooled connection string (must use Supavisor)\r\n  const connectionString = process.env.DATABASE_URL ?? \"\";\r\n\r\n  if (connectionString.includes(\"@localhost\")) {\r\n    // Disable SSL for local connections\r\n    neonConfig.useSecureWebSocket = false;\r\n    // WebSocket proxy is hosted on `4000` locally, so add port. Does not work in production.\r\n    neonConfig.wsProxy = (host) => `${host}:4000/v2`;\r\n  }\r\n\r\n  // Only Neon hosts support this -- non-deterministic errors otherwise\r\n  neonConfig.pipelineConnect = false;\r\n\r\n  // So it can also work in Node.js\r\n  neonConfig.webSocketConstructor = WebSocket;\r\n\r\n  const adapter = new PrismaNeon({ connectionString });\r\n  return new PrismaClient({\r\n    adapter,\r\n    log:\r\n      process.env.NODE_ENV === \"development\"\r\n        ? [\"query\", \"error\", \"warn\"]\r\n        : [\"error\"],\r\n  });\r\n};\r\n\r\nconst globalForPrisma = globalThis as unknown as {\r\n  prisma: ReturnType<typeof createPrismaClient> | undefined;\r\n};\r\n\r\nexport const db = globalForPrisma.prisma ?? createPrismaClient();\r\n\r\nif (process.env.NODE_ENV !== \"production\") globalForPrisma.prisma = db;\r\n"],"names":[],"mappings":";;;AAAA;AACA;AACA;;;;AAEA,MAAM,qBAAqB;IACzB,yDAAyD;IACzD,MAAM,mBAAmB,QAAQ,GAAG,CAAC,YAAY,IAAI;IAErD,IAAI,iBAAiB,QAAQ,CAAC,eAAe;QAC3C,oCAAoC;QACpC,wJAAA,CAAA,aAAU,CAAC,kBAAkB,GAAG;QAChC,yFAAyF;QACzF,wJAAA,CAAA,aAAU,CAAC,OAAO,GAAG,CAAC,OAAS,GAAG,KAAK,QAAQ,CAAC;IAClD;IAEA,qEAAqE;IACrE,wJAAA,CAAA,aAAU,CAAC,eAAe,GAAG;IAE7B,iCAAiC;IACjC,wJAAA,CAAA,aAAU,CAAC,oBAAoB,GAAG;IAElC,MAAM,UAAU,IAAI,+JAAA,CAAA,aAAU,CAAC;QAAE;IAAiB;IAClD,OAAO,IAAI,6HAAA,CAAA,eAAY,CAAC;QACtB;QACA,KACE,uCACI;YAAC;YAAS;YAAS;SAAO;IAElC;AACF;AAEA,MAAM,kBAAkB;AAIjB,MAAM,KAAK,gBAAgB,MAAM,IAAI;AAE5C,wCAA2C,gBAAgB,MAAM,GAAG","debugId":null}},
    {"offset": {"line": 1607, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/packages/db/src/types/prisma.ts"],"sourcesContent":["/* eslint-disable @typescript-eslint/no-namespace */\r\nimport type {\r\n  DocumentPayload as _DocumentPayload,\r\n  DocumentProperties as _DocumentProperties,\r\n  IngestJobConfig as _IngestJobConfig,\r\n  IngestJobPayload as _IngestJobPayload,\r\n  EmbeddingConfig,\r\n  VectorStoreConfig,\r\n} from \"@agentset/validation\";\r\n\r\ntype OpenAILanguageModel = \"gpt-4o\" | \"gpt-4o-mini\";\r\n\r\nexport type LLMConfig =\r\n  | {\r\n      provider: \"OPENAI\";\r\n      model: OpenAILanguageModel;\r\n      apiKey: string;\r\n    }\r\n  | {\r\n      provider: \"AZURE_OPENAI\";\r\n      model: OpenAILanguageModel;\r\n      baseUrl: string;\r\n      deployment: string;\r\n      apiKey: string;\r\n      apiVersion?: string;\r\n    };\r\n\r\ndeclare global {\r\n  export namespace PrismaJson {\r\n    type ConnectionConfig = {\r\n      authType: \"OAUTH2\";\r\n      credentials: {\r\n        accessToken: string;\r\n        refreshToken: string | null;\r\n      };\r\n    };\r\n\r\n    type IngestJobPayload = _IngestJobPayload;\r\n    type IngestJobConfig = _IngestJobConfig;\r\n    type NamespaceVectorStoreConfig = VectorStoreConfig;\r\n\r\n    type NamespaceFileStoreConfig = {\r\n      provider: \"S3\";\r\n      bucket: string;\r\n      accessKeyId: string;\r\n      secretAccessKey: string;\r\n      endpoint: string;\r\n      region: string;\r\n      prefix?: string;\r\n    };\r\n\r\n    type NamespaceEmbeddingConfig = EmbeddingConfig;\r\n    type NamespaceLLMConfig = LLMConfig;\r\n    type DocumentProperties = _DocumentProperties;\r\n\r\n    type DocumentSource = _DocumentPayload;\r\n    type DocumentMetadata = Record<string, unknown>;\r\n  }\r\n}\r\n"],"names":[],"mappings":"AAAA,kDAAkD","debugId":null}},
    {"offset": {"line": 1615, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/packages/db/src/index.ts"],"sourcesContent":["export { db } from \"./client\";\r\nexport * from \"@prisma/client\";\r\n\r\nexport * from \"./types/prisma\";\r\n"],"names":[],"mappings":";AAAA;AACA;AAEA","debugId":null}},
    {"offset": {"line": 1639, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/middleware/get-session.ts"],"sourcesContent":["import type { NextRequest } from \"next/server\";\r\n\r\nimport type { Session } from \"../auth-types\";\r\n\r\nexport const getMiddlewareSession = async (req: NextRequest) => {\r\n  const url = `${req.nextUrl.origin}/api/auth/get-session`;\r\n\r\n  const response = await fetch(url, {\r\n    headers: {\r\n      cookie: req.headers.get(\"cookie\") ?? \"\",\r\n      \"Content-Type\": \"application/json\",\r\n    },\r\n  });\r\n\r\n  if (!response.ok) {\r\n    return null;\r\n  }\r\n\r\n  try {\r\n    const data = (await response.json()) as Session | null;\r\n    return data;\r\n  } catch {\r\n    return null;\r\n  }\r\n};\r\n"],"names":[],"mappings":";;;AAIO,MAAM,uBAAuB,OAAO;IACzC,MAAM,MAAM,GAAG,IAAI,OAAO,CAAC,MAAM,CAAC,qBAAqB,CAAC;IAExD,MAAM,WAAW,MAAM,MAAM,KAAK;QAChC,SAAS;YACP,QAAQ,IAAI,OAAO,CAAC,GAAG,CAAC,aAAa;YACrC,gBAAgB;QAClB;IACF;IAEA,IAAI,CAAC,SAAS,EAAE,EAAE;QAChB,OAAO;IACT;IAEA,IAAI;QACF,MAAM,OAAQ,MAAM,SAAS,IAAI;QACjC,OAAO;IACT,EAAE,OAAM;QACN,OAAO;IACT;AACF","debugId":null}},
    {"offset": {"line": 1666, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/session.ts"],"sourcesContent":["import type { NextRequest } from \"next/server\";\r\n\r\nimport type { Namespace } from \"@agentset/db\";\r\nimport { db } from \"@agentset/db\";\r\n\r\nimport type { Session } from \"../auth-types\";\r\nimport { AgentsetApiError } from \"../api/errors\";\r\nimport { getMiddlewareSession } from \"../middleware/get-session\";\r\n\r\ntype AuthenticateSessionResult<T extends string | undefined> = T extends string\r\n  ? {\r\n      namespace: Namespace;\r\n      session: Session;\r\n    }\r\n  : { session: Session };\r\n\r\nexport const authenticateRequestSession = async <T extends string | undefined>(\r\n  request: NextRequest,\r\n  namespaceId?: T,\r\n): Promise<AuthenticateSessionResult<T>> => {\r\n  const session = await getMiddlewareSession(request);\r\n\r\n  if (!session) {\r\n    throw new AgentsetApiError({\r\n      code: \"unauthorized\",\r\n      message: \"Unauthorized\",\r\n    });\r\n  }\r\n\r\n  if (!namespaceId) {\r\n    return { session } as T extends string\r\n      ? {\r\n          namespace: NonNullable<\r\n            Awaited<ReturnType<typeof db.namespace.findUnique>>\r\n          >;\r\n          session: Session;\r\n        }\r\n      : { session: Session };\r\n  }\r\n\r\n  const namespace = await db.namespace.findUnique({\r\n    where: {\r\n      id: namespaceId,\r\n      organization: {\r\n        members: {\r\n          some: {\r\n            userId: session.user.id,\r\n          },\r\n        },\r\n      },\r\n    },\r\n  });\r\n\r\n  if (!namespace) {\r\n    throw new AgentsetApiError({\r\n      code: \"not_found\",\r\n      message: \"Namespace not found\",\r\n    });\r\n  }\r\n\r\n  // TODO: check role\r\n  return { namespace, session } as AuthenticateSessionResult<T>;\r\n};\r\n"],"names":[],"mappings":";;;AAGA;AAAA;AAGA;AACA;;;;AASO,MAAM,6BAA6B,OACxC,SACA;IAEA,MAAM,UAAU,MAAM,CAAA,GAAA,2JAAA,CAAA,uBAAoB,AAAD,EAAE;IAE3C,IAAI,CAAC,SAAS;QACZ,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;YACzB,MAAM;YACN,SAAS;QACX;IACF;IAEA,IAAI,CAAC,aAAa;QAChB,OAAO;YAAE;QAAQ;IAQnB;IAEA,MAAM,YAAY,MAAM,iIAAA,CAAA,KAAE,CAAC,SAAS,CAAC,UAAU,CAAC;QAC9C,OAAO;YACL,IAAI;YACJ,cAAc;gBACZ,SAAS;oBACP,MAAM;wBACJ,QAAQ,QAAQ,IAAI,CAAC,EAAE;oBACzB;gBACF;YACF;QACF;IACF;IAEA,IAAI,CAAC,WAAW;QACd,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;YACzB,MAAM;YACN,SAAS;QACX;IACF;IAEA,mBAAmB;IACnB,OAAO;QAAE;QAAW;IAAQ;AAC9B","debugId":null}},
    {"offset": {"line": 1719, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/tenant.ts"],"sourcesContent":["import type { NextRequest } from \"next/server\";\r\n\r\nexport const getTenantFromRequest = (request: NextRequest) => {\r\n  const tenantId = request.headers.get(\"x-tenant-id\");\r\n  return tenantId?.trim() ?? undefined;\r\n};\r\n"],"names":[],"mappings":";;;AAEO,MAAM,uBAAuB,CAAC;IACnC,MAAM,WAAW,QAAQ,OAAO,CAAC,GAAG,CAAC;IACrC,OAAO,UAAU,UAAU;AAC7B","debugId":null}},
    {"offset": {"line": 1732, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/utils.ts"],"sourcesContent":["import type { NextRequest } from \"next/server\";\r\n\r\nimport { AgentsetApiError } from \"./errors\";\r\n\r\nexport const getSearchParams = (req: NextRequest) => {\r\n  // Create a params object\r\n  const params = {} as Record<string, string>;\r\n\r\n  new URL(req.url).searchParams.forEach(function (val, key) {\r\n    params[key] = val;\r\n  });\r\n\r\n  return params;\r\n};\r\n\r\nexport const parseRequestBody = async (req: NextRequest) => {\r\n  try {\r\n    // eslint-disable-next-line @typescript-eslint/no-unsafe-return\r\n    return await req.json();\r\n  } catch (e) {\r\n    console.error(e);\r\n    throw new AgentsetApiError({\r\n      code: \"bad_request\",\r\n      message:\r\n        \"Invalid JSON format in request body. Please ensure the request body is a valid JSON object.\",\r\n    });\r\n  }\r\n};\r\n"],"names":[],"mappings":";;;;AAEA;;AAEO,MAAM,kBAAkB,CAAC;IAC9B,yBAAyB;IACzB,MAAM,SAAS,CAAC;IAEhB,IAAI,IAAI,IAAI,GAAG,EAAE,YAAY,CAAC,OAAO,CAAC,SAAU,GAAG,EAAE,GAAG;QACtD,MAAM,CAAC,IAAI,GAAG;IAChB;IAEA,OAAO;AACT;AAEO,MAAM,mBAAmB,OAAO;IACrC,IAAI;QACF,+DAA+D;QAC/D,OAAO,MAAM,IAAI,IAAI;IACvB,EAAE,OAAO,GAAG;QACV,QAAQ,KAAK,CAAC;QACd,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;YACzB,MAAM;YACN,SACE;QACJ;IACF;AACF","debugId":null}},
    {"offset": {"line": 1764, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/handler/auth.ts"],"sourcesContent":["import type { Session } from \"@/lib/auth-types\";\r\nimport type { NextRequest } from \"next/server\";\r\n\r\nimport type { Namespace } from \"@agentset/db\";\r\n\r\nimport type { HandlerParams } from \"./base\";\r\nimport { AgentsetApiError, handleAndReturnErrorResponse } from \"../errors\";\r\nimport { ratelimit } from \"../rate-limit\";\r\nimport { authenticateRequestSession } from \"../session\";\r\nimport { getTenantFromRequest } from \"../tenant\";\r\nimport { getSearchParams } from \"../utils\";\r\n\r\ninterface AuthHandler {\r\n  (\r\n    params: Omit<HandlerParams, \"organization\" | \"apiScope\"> & {\r\n      session: Session;\r\n      namespace: Namespace;\r\n    },\r\n  ): Promise<Response>;\r\n}\r\n\r\nexport const withAuthApiHandler = (\r\n  handler: AuthHandler,\r\n  { requireNamespace = true }: { requireNamespace?: boolean } = {},\r\n) => {\r\n  return async (\r\n    req: NextRequest,\r\n    { params }: { params: Promise<Record<string, string> | undefined> },\r\n  ) => {\r\n    const routeParams = await params;\r\n    const searchParams = getSearchParams(req);\r\n\r\n    const namespaceId = searchParams.namespaceId;\r\n    let headers = {};\r\n\r\n    try {\r\n      if (requireNamespace && !namespaceId) {\r\n        throw new AgentsetApiError({\r\n          code: \"bad_request\",\r\n          message: \"Namespace ID is required\",\r\n        });\r\n      }\r\n\r\n      const tenantId = getTenantFromRequest(req);\r\n      const { namespace, session } = await authenticateRequestSession(\r\n        req,\r\n        namespaceId,\r\n      );\r\n\r\n      const rateLimit = 600;\r\n      const { success, limit, reset, remaining } = await ratelimit(\r\n        rateLimit,\r\n        \"1 m\",\r\n      ).limit(`user:${session.user.id}`);\r\n\r\n      headers = {\r\n        \"Retry-After\": reset.toString(),\r\n        \"X-RateLimit-Limit\": limit.toString(),\r\n        \"X-RateLimit-Remaining\": remaining.toString(),\r\n        \"X-RateLimit-Reset\": reset.toString(),\r\n      };\r\n\r\n      if (!success) {\r\n        throw new AgentsetApiError({\r\n          code: \"rate_limit_exceeded\",\r\n          message: \"Too many requests.\",\r\n        });\r\n      }\r\n\r\n      return await handler({\r\n        req,\r\n        params: routeParams ?? {},\r\n        searchParams,\r\n        namespace,\r\n        session,\r\n        tenantId,\r\n      });\r\n    } catch (error) {\r\n      console.error(error);\r\n      return handleAndReturnErrorResponse(error, headers);\r\n    }\r\n  };\r\n};\r\n"],"names":[],"mappings":";;;AAMA;AACA;AACA;AACA;AACA;;;;;;AAWO,MAAM,qBAAqB,CAChC,SACA,EAAE,mBAAmB,IAAI,EAAkC,GAAG,CAAC,CAAC;IAEhE,OAAO,OACL,KACA,EAAE,MAAM,EAA2D;QAEnE,MAAM,cAAc,MAAM;QAC1B,MAAM,eAAe,CAAA,GAAA,2IAAA,CAAA,kBAAe,AAAD,EAAE;QAErC,MAAM,cAAc,aAAa,WAAW;QAC5C,IAAI,UAAU,CAAC;QAEf,IAAI;YACF,IAAI,oBAAoB,CAAC,aAAa;gBACpC,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;oBACzB,MAAM;oBACN,SAAS;gBACX;YACF;YAEA,MAAM,WAAW,CAAA,GAAA,4IAAA,CAAA,uBAAoB,AAAD,EAAE;YACtC,MAAM,EAAE,SAAS,EAAE,OAAO,EAAE,GAAG,MAAM,CAAA,GAAA,6IAAA,CAAA,6BAA0B,AAAD,EAC5D,KACA;YAGF,MAAM,YAAY;YAClB,MAAM,EAAE,OAAO,EAAE,KAAK,EAAE,KAAK,EAAE,SAAS,EAAE,GAAG,MAAM,CAAA,GAAA,mJAAA,CAAA,YAAS,AAAD,EACzD,WACA,OACA,KAAK,CAAC,CAAC,KAAK,EAAE,QAAQ,IAAI,CAAC,EAAE,EAAE;YAEjC,UAAU;gBACR,eAAe,MAAM,QAAQ;gBAC7B,qBAAqB,MAAM,QAAQ;gBACnC,yBAAyB,UAAU,QAAQ;gBAC3C,qBAAqB,MAAM,QAAQ;YACrC;YAEA,IAAI,CAAC,SAAS;gBACZ,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;oBACzB,MAAM;oBACN,SAAS;gBACX;YACF;YAEA,OAAO,MAAM,QAAQ;gBACnB;gBACA,QAAQ,eAAe,CAAC;gBACxB;gBACA;gBACA;gBACA;YACF;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC;YACd,OAAO,CAAA,GAAA,4IAAA,CAAA,+BAA4B,AAAD,EAAE,OAAO;QAC7C;IACF;AACF","debugId":null}},
    {"offset": {"line": 1826, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/ids.ts"],"sourcesContent":["const _prefixes = [\"ns_\", \"user_\", \"org_\", \"job_\", \"doc_\"] as const;\r\n\r\nexport const prefixId = (id: string, prefix: (typeof _prefixes)[number]) => {\r\n  return id.startsWith(prefix) ? id : `${prefix}${id}`;\r\n};\r\n\r\nexport const normalizeId = (id: string, prefix: (typeof _prefixes)[number]) => {\r\n  return id.startsWith(prefix) ? id.replace(prefix, \"\") : id;\r\n};\r\n"],"names":[],"mappings":";;;;AAAA,MAAM,YAAY;IAAC;IAAO;IAAS;IAAQ;IAAQ;CAAO;AAEnD,MAAM,WAAW,CAAC,IAAY;IACnC,OAAO,GAAG,UAAU,CAAC,UAAU,KAAK,GAAG,SAAS,IAAI;AACtD;AAEO,MAAM,cAAc,CAAC,IAAY;IACtC,OAAO,GAAG,UAAU,CAAC,UAAU,GAAG,OAAO,CAAC,QAAQ,MAAM;AAC1D","debugId":null}},
    {"offset": {"line": 1849, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/api-key.ts"],"sourcesContent":["import { unstable_cache } from \"next/cache\";\r\n\r\nimport { db } from \"@agentset/db\";\r\n\r\nexport const getApiKeyInfo = (apiKey: string) => {\r\n  return unstable_cache(\r\n    async () => {\r\n      const data = await db.organizationApiKey.findUnique({\r\n        where: {\r\n          key: apiKey,\r\n        },\r\n        select: {\r\n          scope: true,\r\n          organizationId: true,\r\n          organization: {\r\n            select: {\r\n              plan: true,\r\n              apiRatelimit: true,\r\n              searchLimit: true,\r\n              searchUsage: true,\r\n              totalPages: true,\r\n              pagesLimit: true,\r\n            },\r\n          },\r\n        },\r\n      });\r\n\r\n      return data;\r\n    },\r\n    [\"apiKey\", apiKey],\r\n    {\r\n      tags: [`apiKey:${apiKey}`],\r\n      revalidate: 60 * 1, // 1 min\r\n    },\r\n  )();\r\n};\r\n\r\nexport type ApiKeyInfo = NonNullable<Awaited<ReturnType<typeof getApiKeyInfo>>>;\r\n"],"names":[],"mappings":";;;AAAA;AAEA;AAAA;;;AAEO,MAAM,gBAAgB,CAAC;IAC5B,OAAO,CAAA,GAAA,+HAAA,CAAA,iBAAc,AAAD,EAClB;QACE,MAAM,OAAO,MAAM,iIAAA,CAAA,KAAE,CAAC,kBAAkB,CAAC,UAAU,CAAC;YAClD,OAAO;gBACL,KAAK;YACP;YACA,QAAQ;gBACN,OAAO;gBACP,gBAAgB;gBAChB,cAAc;oBACZ,QAAQ;wBACN,MAAM;wBACN,cAAc;wBACd,aAAa;wBACb,aAAa;wBACb,YAAY;wBACZ,YAAY;oBACd;gBACF;YACF;QACF;QAEA,OAAO;IACT,GACA;QAAC;QAAU;KAAO,EAClB;QACE,MAAM;YAAC,CAAC,OAAO,EAAE,QAAQ;SAAC;QAC1B,YAAY,KAAK;IACnB;AAEJ","debugId":null}},
    {"offset": {"line": 1895, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/handler/base.ts"],"sourcesContent":["import type { NextRequest } from \"next/server\";\r\nimport { tryCatch } from \"@/lib/error\";\r\n\r\nimport type { Organization } from \"@agentset/db\";\r\n\r\nimport type { ApiKeyInfo } from \"../api-key\";\r\nimport { getApiKeyInfo } from \"../api-key\";\r\nimport { AgentsetApiError, handleAndReturnErrorResponse } from \"../errors\";\r\nimport { ratelimit } from \"../rate-limit\";\r\nimport { getTenantFromRequest } from \"../tenant\";\r\nimport { getSearchParams } from \"../utils\";\r\n\r\nexport interface HandlerParams {\r\n  req: NextRequest;\r\n  params: Record<string, string>;\r\n  searchParams: Record<string, string>;\r\n  organization: Pick<Organization, \"id\"> & ApiKeyInfo[\"organization\"];\r\n  apiScope: string;\r\n  tenantId?: string;\r\n  headers?: Record<string, string>;\r\n}\r\n\r\ninterface Handler {\r\n  (params: HandlerParams): Promise<Response>;\r\n}\r\n\r\nexport const withApiHandler = (handler: Handler) => {\r\n  return async (\r\n    req: NextRequest,\r\n    { params }: { params: Promise<Record<string, string> | undefined> },\r\n  ) => {\r\n    const routeParams = await params;\r\n    const searchParams = getSearchParams(req);\r\n\r\n    let apiKey: string | undefined = undefined;\r\n    let headers = {};\r\n\r\n    try {\r\n      const authorizationHeader = req.headers.get(\"Authorization\");\r\n      if (authorizationHeader) {\r\n        if (!authorizationHeader.includes(\"Bearer \")) {\r\n          throw new AgentsetApiError({\r\n            code: \"bad_request\",\r\n            message:\r\n              \"Misconfigured authorization header. Did you forget to add 'Bearer '?\",\r\n          });\r\n        }\r\n        apiKey = authorizationHeader.replace(\"Bearer \", \"\");\r\n      }\r\n\r\n      if (!apiKey) {\r\n        throw new AgentsetApiError({\r\n          code: \"unauthorized\",\r\n          message: \"Unauthorized: Invalid API key.\",\r\n        });\r\n      }\r\n\r\n      const orgApiKey = await tryCatch(getApiKeyInfo(apiKey));\r\n      if (!orgApiKey.data) {\r\n        throw new AgentsetApiError({\r\n          code: \"unauthorized\",\r\n          message: \"Unauthorized: Invalid API key.\",\r\n        });\r\n      }\r\n\r\n      const rateLimit = orgApiKey.data.organization.apiRatelimit;\r\n      const { success, limit, reset, remaining } = await ratelimit(\r\n        rateLimit,\r\n        \"1 m\",\r\n      ).limit(orgApiKey.data.organizationId);\r\n\r\n      headers = {\r\n        \"Retry-After\": reset.toString(),\r\n        \"X-RateLimit-Limit\": limit.toString(),\r\n        \"X-RateLimit-Remaining\": remaining.toString(),\r\n        \"X-RateLimit-Reset\": reset.toString(),\r\n      };\r\n\r\n      if (!success) {\r\n        throw new AgentsetApiError({\r\n          code: \"rate_limit_exceeded\",\r\n          message: \"Too many requests.\",\r\n        });\r\n      }\r\n\r\n      const tenantId = getTenantFromRequest(req);\r\n\r\n      return await handler({\r\n        req,\r\n        params: routeParams ?? {},\r\n        searchParams,\r\n        organization: {\r\n          id: orgApiKey.data.organizationId,\r\n          ...orgApiKey.data.organization,\r\n        },\r\n        apiScope: orgApiKey.data.scope,\r\n        headers,\r\n        tenantId,\r\n      });\r\n    } catch (error) {\r\n      console.error(error);\r\n      return handleAndReturnErrorResponse(error, headers);\r\n    }\r\n  };\r\n};\r\n"],"names":[],"mappings":";;;AACA;AAKA;AACA;AACA;AACA;AACA;;;;;;;AAgBO,MAAM,iBAAiB,CAAC;IAC7B,OAAO,OACL,KACA,EAAE,MAAM,EAA2D;QAEnE,MAAM,cAAc,MAAM;QAC1B,MAAM,eAAe,CAAA,GAAA,2IAAA,CAAA,kBAAe,AAAD,EAAE;QAErC,IAAI,SAA6B;QACjC,IAAI,UAAU,CAAC;QAEf,IAAI;YACF,MAAM,sBAAsB,IAAI,OAAO,CAAC,GAAG,CAAC;YAC5C,IAAI,qBAAqB;gBACvB,IAAI,CAAC,oBAAoB,QAAQ,CAAC,YAAY;oBAC5C,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;wBACzB,MAAM;wBACN,SACE;oBACJ;gBACF;gBACA,SAAS,oBAAoB,OAAO,CAAC,WAAW;YAClD;YAEA,IAAI,CAAC,QAAQ;gBACX,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;oBACzB,MAAM;oBACN,SAAS;gBACX;YACF;YAEA,MAAM,YAAY,MAAM,CAAA,GAAA,oIAAA,CAAA,WAAQ,AAAD,EAAE,CAAA,GAAA,gJAAA,CAAA,gBAAa,AAAD,EAAE;YAC/C,IAAI,CAAC,UAAU,IAAI,EAAE;gBACnB,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;oBACzB,MAAM;oBACN,SAAS;gBACX;YACF;YAEA,MAAM,YAAY,UAAU,IAAI,CAAC,YAAY,CAAC,YAAY;YAC1D,MAAM,EAAE,OAAO,EAAE,KAAK,EAAE,KAAK,EAAE,SAAS,EAAE,GAAG,MAAM,CAAA,GAAA,mJAAA,CAAA,YAAS,AAAD,EACzD,WACA,OACA,KAAK,CAAC,UAAU,IAAI,CAAC,cAAc;YAErC,UAAU;gBACR,eAAe,MAAM,QAAQ;gBAC7B,qBAAqB,MAAM,QAAQ;gBACnC,yBAAyB,UAAU,QAAQ;gBAC3C,qBAAqB,MAAM,QAAQ;YACrC;YAEA,IAAI,CAAC,SAAS;gBACZ,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;oBACzB,MAAM;oBACN,SAAS;gBACX;YACF;YAEA,MAAM,WAAW,CAAA,GAAA,4IAAA,CAAA,uBAAoB,AAAD,EAAE;YAEtC,OAAO,MAAM,QAAQ;gBACnB;gBACA,QAAQ,eAAe,CAAC;gBACxB;gBACA,cAAc;oBACZ,IAAI,UAAU,IAAI,CAAC,cAAc;oBACjC,GAAG,UAAU,IAAI,CAAC,YAAY;gBAChC;gBACA,UAAU,UAAU,IAAI,CAAC,KAAK;gBAC9B;gBACA;YACF;QACF,EAAE,OAAO,OAAO;YACd,QAAQ,KAAK,CAAC;YACd,OAAO,CAAA,GAAA,4IAAA,CAAA,+BAA4B,AAAD,EAAE,OAAO;QAC7C;IACF;AACF","debugId":null}},
    {"offset": {"line": 1979, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/handler/namespace.ts"],"sourcesContent":["import type { Namespace } from \"@agentset/db\";\r\nimport { db } from \"@agentset/db\";\r\n\r\nimport type { HandlerParams } from \"./base\";\r\nimport { AgentsetApiError } from \"../errors\";\r\nimport { normalizeId } from \"../ids\";\r\nimport { withApiHandler } from \"./base\";\r\n\r\ninterface NamespaceHandler {\r\n  (\r\n    params: HandlerParams & {\r\n      namespace: Namespace;\r\n    },\r\n  ): Promise<Response>;\r\n}\r\n\r\nexport const withNamespaceApiHandler = (handler: NamespaceHandler) => {\r\n  return withApiHandler(async (params) => {\r\n    const namespaceId = normalizeId(params.params.namespaceId ?? \"\", \"ns_\");\r\n    if (!namespaceId) {\r\n      throw new AgentsetApiError({\r\n        code: \"bad_request\",\r\n        message: \"Invalid namespace ID.\",\r\n      });\r\n    }\r\n\r\n    const namespace = await db.namespace.findUnique({\r\n      where: {\r\n        id: namespaceId,\r\n      },\r\n    });\r\n\r\n    if (!namespace || namespace.organizationId !== params.organization.id) {\r\n      throw new AgentsetApiError({\r\n        code: \"unauthorized\",\r\n        message: \"Unauthorized: You don't have access to this namespace.\",\r\n      });\r\n    }\r\n\r\n    return await handler({\r\n      ...params,\r\n      namespace,\r\n    });\r\n  });\r\n};\r\n"],"names":[],"mappings":";;;AACA;AAAA;AAGA;AACA;AACA;;;;;AAUO,MAAM,0BAA0B,CAAC;IACtC,OAAO,CAAA,GAAA,qJAAA,CAAA,iBAAc,AAAD,EAAE,OAAO;QAC3B,MAAM,cAAc,CAAA,GAAA,yIAAA,CAAA,cAAW,AAAD,EAAE,OAAO,MAAM,CAAC,WAAW,IAAI,IAAI;QACjE,IAAI,CAAC,aAAa;YAChB,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;gBACzB,MAAM;gBACN,SAAS;YACX;QACF;QAEA,MAAM,YAAY,MAAM,iIAAA,CAAA,KAAE,CAAC,SAAS,CAAC,UAAU,CAAC;YAC9C,OAAO;gBACL,IAAI;YACN;QACF;QAEA,IAAI,CAAC,aAAa,UAAU,cAAc,KAAK,OAAO,YAAY,CAAC,EAAE,EAAE;YACrE,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;gBACzB,MAAM;gBACN,SAAS;YACX;QACF;QAEA,OAAO,MAAM,QAAQ;YACnB,GAAG,MAAM;YACT;QACF;IACF;AACF","debugId":null}},
    {"offset": {"line": 2023, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/api/handler/index.ts"],"sourcesContent":["export * from \"./auth\";\r\nexport * from \"./namespace\";\r\nexport * from \"./base\";\r\n"],"names":[],"mappings":";AAAA;AACA;AACA","debugId":null}},
    {"offset": {"line": 2047, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/deep-research/classes.ts"],"sourcesContent":["/**\r\n * Data models for the Deep Research Cookbook\r\n */\r\n\r\n/**\r\n * Structured representation of a research plan with search queries.\r\n * Used to parse the LLM's planning output into a structured format\r\n * that can be easily processed by the research pipeline.\r\n */\r\nexport interface ResearchPlan {\r\n  queries: string[];\r\n}\r\n\r\n/**\r\n * Structured representation of filtered source indices.\r\n * Used to parse the LLM's source evaluation output into a structured\r\n * format that identifies which search results should be retained.\r\n */\r\nexport interface SourceList {\r\n  sources: number[];\r\n}\r\n\r\n/**\r\n * Container for an individual search result with its metadata and content.\r\n * Holds both the original content and the filtered/processed content\r\n * that's relevant to the research topic.\r\n */\r\nexport class SearchResult {\r\n  id: string;\r\n  metadata?: Record<string, any>;\r\n  content: string;\r\n\r\n  constructor(params: {\r\n    id: string;\r\n    metadata?: Record<string, any>;\r\n    content: string;\r\n  }) {\r\n    this.id = params.id;\r\n    this.metadata = params.metadata;\r\n    this.content = params.content;\r\n  }\r\n\r\n  /**\r\n   * (For Report Generation and Completeness Evaluation) String representation with title, link and refined content.\r\n   */\r\n  toString(): string {\r\n    return `ID: ${this.id}\\nMetadata: ${JSON.stringify(this.metadata)}\\nContent: ${this.content.substring(0, 1000)}`;\r\n  }\r\n\r\n  /**\r\n   * (For Filtering ONLY) Abbreviated string representation with truncated raw content.\r\n   */\r\n  shortStr(): string {\r\n    return `ID: ${this.id}\\nMetadata: ${JSON.stringify(this.metadata)}\\nContent: ${this.content.substring(0, 1000)}`;\r\n  }\r\n}\r\n\r\n/**\r\n * Collection of search results with utilities for manipulation and display.\r\n * Provides methods for combining result sets, deduplication, and\r\n * different string representations for processing and display.\r\n */\r\nexport class SearchResults {\r\n  results: SearchResult[];\r\n\r\n  constructor(results: SearchResult[]) {\r\n    this.results = results;\r\n  }\r\n\r\n  /**\r\n   * Detailed string representation of all search results with indices.\r\n   */\r\n  toString(): string {\r\n    return this.results\r\n      .map((result, i) => `[${i + 1}] ${result.toString()}`)\r\n      .join(\"\\n\\n\");\r\n  }\r\n\r\n  /**\r\n   * Combine two SearchResults objects by concatenating their result lists.\r\n   */\r\n  add(other: SearchResults): SearchResults {\r\n    return new SearchResults([...this.results, ...other.results]);\r\n  }\r\n\r\n  /**\r\n   * Abbreviated string representation of all search results with indices.\r\n   */\r\n  shortStr(): string {\r\n    return this.results\r\n      .map((result, i) => `[${i + 1}] ${result.shortStr()}`)\r\n      .join(\"\\n\\n\");\r\n  }\r\n\r\n  /**\r\n   * Remove duplicate search results based on ID.\r\n   * Returns a new SearchResults object with unique entries.\r\n   */\r\n  dedup(): SearchResults {\r\n    const seenIds = new Set<string>();\r\n    const uniqueResults: SearchResult[] = [];\r\n\r\n    for (const result of this.results) {\r\n      if (!seenIds.has(result.id)) {\r\n        seenIds.add(result.id);\r\n        uniqueResults.push(result);\r\n      }\r\n    }\r\n\r\n    return new SearchResults(uniqueResults);\r\n  }\r\n}\r\n\r\n/**\r\n * Return type for iterative research results containing final search results and used queries.\r\n */\r\nexport interface IterativeResearchResult {\r\n  finalSearchResults: SearchResults;\r\n  queriesUsed: string[];\r\n}\r\n\r\n/**\r\n * Return type for filtered results containing filtered search results and source indices.\r\n */\r\nexport interface FilteredResultsData {\r\n  filteredResults: SearchResults;\r\n  sourceIndices: number[];\r\n}\r\n"],"names":[],"mappings":"AAAA;;CAEC,GAED;;;;CAIC;;;;AAmBM,MAAM;IACX,GAAW;IACX,SAA+B;IAC/B,QAAgB;IAEhB,YAAY,MAIX,CAAE;QACD,IAAI,CAAC,EAAE,GAAG,OAAO,EAAE;QACnB,IAAI,CAAC,QAAQ,GAAG,OAAO,QAAQ;QAC/B,IAAI,CAAC,OAAO,GAAG,OAAO,OAAO;IAC/B;IAEA;;GAEC,GACD,WAAmB;QACjB,OAAO,CAAC,IAAI,EAAE,IAAI,CAAC,EAAE,CAAC,YAAY,EAAE,KAAK,SAAS,CAAC,IAAI,CAAC,QAAQ,EAAE,WAAW,EAAE,IAAI,CAAC,OAAO,CAAC,SAAS,CAAC,GAAG,OAAO;IAClH;IAEA;;GAEC,GACD,WAAmB;QACjB,OAAO,CAAC,IAAI,EAAE,IAAI,CAAC,EAAE,CAAC,YAAY,EAAE,KAAK,SAAS,CAAC,IAAI,CAAC,QAAQ,EAAE,WAAW,EAAE,IAAI,CAAC,OAAO,CAAC,SAAS,CAAC,GAAG,OAAO;IAClH;AACF;AAOO,MAAM;IACX,QAAwB;IAExB,YAAY,OAAuB,CAAE;QACnC,IAAI,CAAC,OAAO,GAAG;IACjB;IAEA;;GAEC,GACD,WAAmB;QACjB,OAAO,IAAI,CAAC,OAAO,CAChB,GAAG,CAAC,CAAC,QAAQ,IAAM,CAAC,CAAC,EAAE,IAAI,EAAE,EAAE,EAAE,OAAO,QAAQ,IAAI,EACpD,IAAI,CAAC;IACV;IAEA;;GAEC,GACD,IAAI,KAAoB,EAAiB;QACvC,OAAO,IAAI,cAAc;eAAI,IAAI,CAAC,OAAO;eAAK,MAAM,OAAO;SAAC;IAC9D;IAEA;;GAEC,GACD,WAAmB;QACjB,OAAO,IAAI,CAAC,OAAO,CAChB,GAAG,CAAC,CAAC,QAAQ,IAAM,CAAC,CAAC,EAAE,IAAI,EAAE,EAAE,EAAE,OAAO,QAAQ,IAAI,EACpD,IAAI,CAAC;IACV;IAEA;;;GAGC,GACD,QAAuB;QACrB,MAAM,UAAU,IAAI;QACpB,MAAM,gBAAgC,EAAE;QAExC,KAAK,MAAM,UAAU,IAAI,CAAC,OAAO,CAAE;YACjC,IAAI,CAAC,QAAQ,GAAG,CAAC,OAAO,EAAE,GAAG;gBAC3B,QAAQ,GAAG,CAAC,OAAO,EAAE;gBACrB,cAAc,IAAI,CAAC;YACrB;QACF;QAEA,OAAO,IAAI,cAAc;IAC3B;AACF","debugId":null}},
    {"offset": {"line": 2121, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/deep-research/config.ts"],"sourcesContent":["/**\r\n * Configuration for the Deep Research Cookbook\r\n */\r\n\r\n/**\r\n * Research Configuration Parameters\r\n *\r\n * These parameters control the Deep Research process, allowing customization\r\n * of research behavior, model selection, and output format.\r\n */\r\n\r\n// Resource Allocation\r\n// Parameters controlling research depth and breadth\r\nexport const RESEARCH_CONFIG = {\r\n  budget: 2, // Number of research refinement cycles to perform (in addition to the initial search operation)\r\n  maxQueries: 2, // Maximum number of search queries per research cycle\r\n  maxSources: 5, // Maximum number of sources to include in final synthesis\r\n  maxTokens: 8192, // Maximum number of tokens in the generated report\r\n};\r\n\r\n/**\r\n * Core prompt function that adds current date information to all prompts\r\n * This ensures all models have the correct temporal context for research\r\n */\r\nexport const getCurrentDateContext = () => {\r\n  const now = new Date();\r\n  const year = now.getFullYear();\r\n  const month = now.getMonth() + 1; // JavaScript months are 0-indexed\r\n  const day = now.getDate();\r\n  const monthName = now.toLocaleString(\"default\", { month: \"long\" });\r\n\r\n  return `Current date is ${year}-${month.toString().padStart(2, \"0\")}-${day\r\n    .toString()\r\n    .padStart(2, \"0\")} (${monthName} ${day}, ${year}).\r\nWhen searching for recent information, prioritize results from the current year (${year}) and month (${monthName} ${year}).\r\nFor queries about recent developments, include the current year (${year}) in your search terms.\r\nWhen ranking search results, consider recency as a factor - newer information is generally more relevant for current topics.`;\r\n};\r\n\r\n// System Prompts\r\n// Instructions for each stage of the research process\r\nexport const PROMPTS = {\r\n  // Planning: Generates initial research queries\r\n  planningPrompt: `${getCurrentDateContext()}\r\nYou are a strategic research planner with expertise in breaking down complex questions into logical search steps. When given a research topic or question, you'll analyze what specific information is needed and develop a sequential research plan.\r\n\r\n    First, identify the core components of the question and any implicit information needs.\r\n\r\n    Then provide a numbered list of 3-5 sequential search queries\r\n\r\n    Your queries should be:\r\n    - Specific and focused (avoid broad queries that return general information)\r\n    - Written in natural language without Boolean operators (no AND/OR)\r\n    - Designed to progress logically from foundational to specific information\r\n\r\n    It's perfectly acceptable to start with exploratory queries to \"test the waters\" before diving deeper. Initial queries can help establish baseline information or verify assumptions before proceeding to more targeted searches.`,\r\n\r\n  // Plan Parsing: Extracts structured data from planning output\r\n  planParsingPrompt: `${getCurrentDateContext()}\r\n    You are a research assistant, you will be provided with a plan of action to research a topic, identify the queries that we should run to search for the topic. Look carefully\r\n    at the general plan provided and identify the key queries that we should run. For dependent queries (those requiring results from earlier searches), leave them for later execution and focus only on the self-contained queries that can be run immediately.`,\r\n\r\n  // Content Processing: Identifies relevant information from search results\r\n  rawContentSummarizerPrompt: `${getCurrentDateContext()}\r\n    You are a research extraction specialist. Given a research topic and raw web content, create a thoroughly detailed synthesis as a cohesive narrative that flows naturally between key concepts.\r\n\r\n    Extract the most valuable information related to the research topic, including relevant facts, statistics, methodologies, claims, and contextual information. Preserve technical terminology and domain-specific language from the source material.\r\n\r\n    Structure your synthesis as a coherent document with natural transitions between ideas. Begin with an introduction that captures the core thesis and purpose of the source material. Develop the narrative by weaving together key findings and their supporting details, ensuring each concept flows logically to the next.\r\n\r\n    Integrate specific metrics, dates, and quantitative information within their proper context. Explore how concepts interconnect within the source material, highlighting meaningful relationships between ideas. Acknowledge limitations by noting where information related to aspects of the research topic may be missing or incomplete.\r\n\r\n    Important guidelines:\r\n    - Maintain original data context (e.g., \"2024 study of 150 patients\" rather than generic \"recent study\")\r\n    - Preserve the integrity of information by keeping details anchored to their original context\r\n    - Create a cohesive narrative rather than disconnected bullet points or lists\r\n    - Use paragraph breaks only when transitioning between major themes\r\n\r\n    Critical Reminder: If content lacks a specific aspect of the research topic, clearly state that in the synthesis, and you should NEVER make up information and NEVER rely on external knowledge.`,\r\n\r\n  // Completeness Evaluation: Determines if more research is needed\r\n  evaluationPrompt: `${getCurrentDateContext()}\r\nYou are a research query optimizer. Your task is to analyze search results against the original research goal and generate follow-up queries to fill in missing information.\r\n\r\n    PROCESS:\r\n    1. Identify ALL information explicitly requested in the original research goal\r\n    2. Analyze what specific information has been successfully retrieved in the search results\r\n    3. Identify ALL information gaps between what was requested and what was found\r\n    4. For entity-specific gaps: Create targeted queries for each missing attribute of identified entities\r\n    5. For general knowledge gaps: Create focused queries to find the missing conceptual information\r\n\r\n    QUERY GENERATION RULES:\r\n    - IF specific entities were identified AND specific attributes are missing:\r\n    * Create direct queries for each entity-attribute pair (e.g., \"LeBron James height\")\r\n    - IF general knowledge gaps exist:\r\n    * Create focused queries to address each conceptual gap (e.g., \"criteria for ranking basketball players\")\r\n    - Queries must be constructed to directly retrieve EXACTLY the missing information\r\n    - Avoid tangential or merely interesting information not required by the original goal\r\n    - Prioritize queries that will yield the most critical missing information first\r\n\r\n    OUTPUT FORMAT:\r\n    First, briefly state:\r\n    1. What specific information was found\r\n    2. What specific information is still missing\r\n    3. What type of knowledge gaps exist (entity-specific or general knowledge)\r\n\r\n    Then provide up to 5 targeted queries that directly address the identified gaps, ordered by importance. Please consider that you\r\n    need to generate queries that tackle a single goal at a time (searching for A AND B will return bad results). Be specific!`,\r\n\r\n  // Evaluation Parsing: Extracts structured data from evaluation output\r\n  evaluationParsingPrompt: `${getCurrentDateContext()}\r\n        You are a research assistant, you will be provided with a some reasoning and a list of queries, and you will need to parse the list into a list of queries.\r\n`,\r\n\r\n  // Source Filtering: Selects most relevant sources\r\n  filterPrompt: `${getCurrentDateContext()}\r\nYou are a web-search filter assistant. Your task is to filter and rank search results based on the research topic, to help your colleague create a comprehensive, in-depth, and detailed research report.\r\n\r\n    You will be given the research topic, and the current search results: their titles, links, and contents. Your goal is to:\r\n    1. Rank ALL results that have ANY relevance to the topic, even if the connection is indirect\r\n    2. Use the following relevance categories:\r\n        - High relevance: Directly addresses the main topic\r\n        - Medium relevance: Contains useful supporting information or related concepts\r\n        - Low relevance: Has tangential or contextual information that might be valuable for background or broader perspective\r\n        - No relevance: Completely unrelated or irrelevant (only these should be excluded)\r\n\r\n    Remember:\r\n    - Keep sources that might provide valuable context or supporting information, even if not directly focused on the main topic\r\n    - Sources with partial relevance should be ranked lower rather than excluded\r\n    - Consider how each source might contribute to different aspects of the research report (background, context, examples, etc.)\r\n\r\n    At the end of your response, return a LIST of source numbers in order of relevance, including ALL sources that have any potential value (high, medium, or low relevance). Only exclude sources that are completely irrelevant to the topic.`,\r\n\r\n  // Source Filtering: Selects most relevant sources\r\n  sourceParsingPrompt: `${getCurrentDateContext()}\r\n    You are a research assistant, you will be provided with a relevance analysis of the search results.\r\n\r\n    You need to return a list of source numbers corresponding to the search results, in the order of relevance to the research topic.`,\r\n\r\n  // Answer Generation: Creates final research report\r\n  answerPrompt: `${getCurrentDateContext()} \r\n  You are a senior research analyst tasked with creating a professional, publication-ready report.\r\n    Using ONLY the provided sources, produce a markdown document (at least 5 pages) following these exact requirements:\r\n\r\n    # Structure Guidelines\r\n\r\n    1. **Abstract**\r\n    - Provide a concise (250-300 words) summary of the entire research\r\n    - State the main research question/objective\r\n    - Highlight key findings and their significance\r\n    - Summarize major conclusions and implications\r\n    - Write in a self-contained manner that can stand alone\r\n    2. **Introduction**\r\n    - Contextualize the research topic\r\n    - State the report's scope and objectives\r\n    - Preview key themes\r\n    3. **Analysis**\r\n    - Group findings into thematic categories\r\n    - Compare/contrast different sources' perspectives\r\n    - Highlight patterns, contradictions, and evidence quality\r\n    - MUST include numbered citations [1][2]... to support all key claims and analysis. Never make factual statements without providing the corresponding citation. Format citations as [n] directly after the relevant text.\r\n    4. **Conclusion**\r\n    - Synthesize overarching insights\r\n    - Discuss practical implications\r\n    - Identify knowledge gaps and research limitations\r\n    - Suggest areas for further investigation\r\n    5. **References**\r\n    - MUST be included in the report to improve the readability and credibility.\r\n    - Include ALL sources in the references section, even those not directly cited in the report\r\n    - Number references consecutively (1, 2, 3...) without gaps\r\n\r\n    # Composition Rules\r\n        * Strict source adherence: Every claim must cite sources using [n] notation\r\n        * Analytical depth: Prioritize insight generation over mere information listing\r\n        * Objective framing: Present conflicting evidence without bias\r\n        * Information hierarchy: Use H2 headers for main sections, H3 for subsections\r\n        * Visual clarity: Format tables with | delimiters and alignment markers\r\n        * Citation integrity: Include numbered references with full source metadata\r\n\r\n    # Prohibitions\r\n        * Bullet points/listicles\r\n        * Unsupported assertions\r\n        * Informal language\r\n        * Repetitive content\r\n        * Source aggregation without analysis\r\n        * External knowledge beyond provided sources\r\n\r\n    # Formatting Requirements\r\n\r\n    [Research Topic]\r\n\r\n    ## Abstract\r\n    [Abstract content...]\r\n\r\n    ## Introduction\r\n    [Cohesive opening paragraph...]\r\n    [More details about the research topic...]\r\n    [General overview of the report...]\r\n\r\n    ## [Primary Theme]\r\n    [Detailed analysis with integrated citations [1][3]. Compare multiple sources...]\r\n    [Additional details)]\r\n\r\n    ### [Subtheme]\r\n    [Specific insights...]\r\n\r\n    ### [Subtheme Where Table or Chart is Helpful]\r\n\r\n    [Table Analysis in full paragraphs, avoid bullet points...]\r\n\r\n    *Table X: Caption...[citation] (MUST be put above the table and seperated by a blank line)*\r\n\r\n    | Comparison Aspect | Source A [2] | Source B [4] |\r\n    |--------------------|--------------|--------------|\r\n    | Key metric         | xx%          | xx%          |\r\n    \r\n\r\n    [Chart Analysis in full paragraphs, avoid bullet points...]\r\n    \\`\\`\\`mermaid\r\n    %% Choose one: flowchart, sequenceDiagram, classDiagram, stateDiagram, gantt, pie, xychart-beta\r\n    %% DO NOT PUT TITLE in MERMAID CODE! titles should be put in THE FIGURE CAPTION\r\n    %% To reduce the rendering difficulty, avoid multiple series, stacked charts, or complex features. \r\n    %% DATA ARRAYS and AXIS RANGES MUST CONTAIN NUMBERS ONLY [10, 20, 30], e.g. for units like heights, use inches (74) instead of feet inches (6'2\")\r\n    %% NEVER include values that are null, n/a, or undefined in the data series.\r\n    [CHART_TYPE]\r\n        %% For xy/bar charts:\r\n        xlabel \"[X_AXIS_LABEL]\"\r\n        ylabel \"[Y_AXIS_LABEL]\"\r\n\r\n        %% For data series, use one of these formats:\r\n        %% Format 1 - Simple bar/line:\r\n        \"[LABEL1]\" [VALUE1]\r\n        \"[LABEL2]\" [VALUE2]\r\n\r\n        %% Format 2 - Array style (xychart-beta):\r\n        %% For measurements with special units (feet/inches, degreesÂ°, minutes', arc-seconds''), you MUST use double single-quotes ('') to escape, e.g., [\"6'2''\", \"45Â°2''\", \"23'45''\"] NOT [\"6'2\\\"\", \"45Â°2\\\"\"]\r\n        xychart-beta\r\n        x-axis \"[X_AXIS_LABEL]\" [\"Label1\", \"Label2\", \"Label3\"]\r\n        y-axis \"[Y_AXIS_LABEL]\" MIN_VALUE --> MAX_VALUE\r\n        bar [value1, value2, value3]\r\n    \\`\\`\\`\r\n    *Figure X: Caption...[citation] (MUST be put below the figure and seperated by a blank line)*\r\n    \r\n    ## Conclusion\r\n    [Synthesized takeaways...] [5][6]\r\n    [Explicit limitations discussion...]\r\n    [Overall summary with 5/6 paragraphs]\r\n\r\n    ### References\r\n    1. [Title of Source](https://url-of-source)\r\n    2. [Complete Source Title](https://example.com/full-url)\r\n\r\n    # Reference Rules\r\n    * Number all citations consecutively: [1], [2], [3], etc.\r\n    * Include ALL sources in the reference list, whether cited in the report or not\r\n    * No gaps allowed in the reference numbering\r\n    * Format each reference as: [Title](URL)\r\n    * For consecutive citations in text, use ranges: [1-3] instead of [1][2][3]\r\n    \r\n    # Example\r\n    If your research report mentioned sources 1, 3, list ALL of them in references including 2 to avoid gaps:\r\n    1. [First Source](https://example.com/first)\r\n    2. [Second Source](https://example.com/second)\r\n    3. [Third Source](https://example.com/third)\r\n    \r\n    Begin by analyzing source relationships before writing. Verify all citations match reference numbers. Maintain academic tone throughout.\r\n    While you think, consider that the sections you need to write should be 3/4 paragraphs each. We do not want to end up with a list of bullet points. Or very short sections.\r\n    Think like a writer, you are optimizing coherence and readability.\r\n    In terms of content is like you are writing the chapter of a book, with a few headings and lots of paragraphs. Plan to write at least 3 paragraphs for each heading you want to\r\n    include in the report.`,\r\n};\r\n"],"names":[],"mappings":"AAAA;;CAEC,GAED;;;;;CAKC,GAED,sBAAsB;AACtB,oDAAoD;;;;;;AAC7C,MAAM,kBAAkB;IAC7B,QAAQ;IACR,YAAY;IACZ,YAAY;IACZ,WAAW;AACb;AAMO,MAAM,wBAAwB;IACnC,MAAM,MAAM,IAAI;IAChB,MAAM,OAAO,IAAI,WAAW;IAC5B,MAAM,QAAQ,IAAI,QAAQ,KAAK,GAAG,kCAAkC;IACpE,MAAM,MAAM,IAAI,OAAO;IACvB,MAAM,YAAY,IAAI,cAAc,CAAC,WAAW;QAAE,OAAO;IAAO;IAEhE,OAAO,CAAC,gBAAgB,EAAE,KAAK,CAAC,EAAE,MAAM,QAAQ,GAAG,QAAQ,CAAC,GAAG,KAAK,CAAC,EAAE,IACpE,QAAQ,GACR,QAAQ,CAAC,GAAG,KAAK,EAAE,EAAE,UAAU,CAAC,EAAE,IAAI,EAAE,EAAE,KAAK;iFAC6B,EAAE,KAAK,aAAa,EAAE,UAAU,CAAC,EAAE,KAAK;iEACxD,EAAE,KAAK;4HACoD,CAAC;AAC7H;AAIO,MAAM,UAAU;IACrB,+CAA+C;IAC/C,gBAAgB,GAAG,wBAAwB;;;;;;;;;;;;qOAYwL,CAAC;IAEpO,8DAA8D;IAC9D,mBAAmB,GAAG,wBAAwB;;iQAEiN,CAAC;IAEhQ,0EAA0E;IAC1E,4BAA4B,GAAG,wBAAwB;;;;;;;;;;;;;;;oMAe2I,CAAC;IAEnM,iEAAiE;IACjE,kBAAkB,GAAG,wBAAwB;;;;;;;;;;;;;;;;;;;;;;;;;;8HA0B+E,CAAC;IAE7H,sEAAsE;IACtE,yBAAyB,GAAG,wBAAwB;;AAEtD,CAAC;IAEC,kDAAkD;IAClD,cAAc,GAAG,wBAAwB;;;;;;;;;;;;;;;;+OAgBoM,CAAC;IAE9O,kDAAkD;IAClD,qBAAqB,GAAG,wBAAwB;;;qIAGmF,CAAC;IAEpI,mDAAmD;IACnD,cAAc,GAAG,wBAAwB;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;0BAiIjB,CAAC;AAC3B","debugId":null}},
    {"offset": {"line": 2383, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/deep-research/index.ts"],"sourcesContent":["/**\r\n * Deep Research Pipeline Implementation\r\n */\r\n\r\nimport type { LanguageModelV1 } from \"ai\";\r\nimport {\r\n  extractReasoningMiddleware,\r\n  generateObject,\r\n  generateText,\r\n  streamText,\r\n  wrapLanguageModel,\r\n} from \"ai\";\r\nimport { z } from \"zod\";\r\n\r\nimport type { Namespace } from \"@agentset/db\";\r\n\r\nimport type { QueryVectorStoreOptions } from \"../vector-store/parse\";\r\nimport type { FilteredResultsData, IterativeResearchResult } from \"./classes\";\r\nimport { queryVectorStore } from \"../vector-store\";\r\nimport { SearchResult, SearchResults } from \"./classes\";\r\nimport { PROMPTS, RESEARCH_CONFIG } from \"./config\";\r\n\r\ntype ModelConfig = {\r\n  planning: LanguageModelV1;\r\n  json: LanguageModelV1;\r\n  summary: LanguageModelV1;\r\n  answer: LanguageModelV1;\r\n};\r\n\r\n/**\r\n * Deep Research Pipeline\r\n *\r\n * This class implements the complete research pipeline, from query generation\r\n * to final report synthesis.\r\n */\r\nexport class DeepResearchPipeline {\r\n  private namespace: Namespace;\r\n  private queryOptions?: Omit<QueryVectorStoreOptions, \"query\">;\r\n\r\n  private modelConfig: ModelConfig;\r\n  private researchConfig: typeof RESEARCH_CONFIG;\r\n  private prompts: typeof PROMPTS;\r\n  private currentSpending: number = 0;\r\n\r\n  private researchPlanSchema = z.object({\r\n    queries: z\r\n      .string()\r\n      .array()\r\n      .describe(\"A list of search queries to thoroughly research the topic\"),\r\n  });\r\n\r\n  private sourceListSchema = z.object({\r\n    sources: z.array(z.number()).describe(\"List of source indices to keep\"),\r\n  });\r\n\r\n  constructor(\r\n    namespace: Namespace,\r\n    {\r\n      researchConfig = RESEARCH_CONFIG,\r\n      prompts = PROMPTS,\r\n      modelConfig,\r\n      queryOptions,\r\n      ...options\r\n    }: {\r\n      modelConfig: ModelConfig;\r\n      queryOptions?: Omit<QueryVectorStoreOptions, \"query\">;\r\n      researchConfig?: typeof RESEARCH_CONFIG;\r\n      prompts?: typeof PROMPTS;\r\n      maxQueries?: number;\r\n      maxSources?: number;\r\n      maxCompletionTokens?: number;\r\n    },\r\n  ) {\r\n    this.namespace = namespace;\r\n    this.modelConfig = modelConfig;\r\n    this.queryOptions = queryOptions;\r\n    this.researchConfig = researchConfig;\r\n    this.prompts = prompts;\r\n\r\n    // Override config with options\r\n    if (options.maxQueries !== undefined) {\r\n      this.researchConfig.maxQueries = options.maxQueries;\r\n    }\r\n    if (options.maxSources !== undefined) {\r\n      this.researchConfig.maxSources = options.maxSources;\r\n    }\r\n    if (options.maxCompletionTokens !== undefined) {\r\n      this.researchConfig.maxTokens = options.maxCompletionTokens;\r\n    }\r\n  }\r\n\r\n  /**\r\n   * Generate initial research queries based on the topic\r\n   *\r\n   * @param topic The research topic\r\n   * @returns List of search queries\r\n   */\r\n  private async generateInitialQueries({\r\n    topic,\r\n  }: {\r\n    topic: string;\r\n  }): Promise<string[]> {\r\n    let allQueries = await this.generateResearchQueries(topic);\r\n\r\n    if (this.researchConfig.maxQueries > 0) {\r\n      allQueries = allQueries.slice(0, this.researchConfig.maxQueries);\r\n    }\r\n\r\n    console.log(`\\n\\n\\x1b[36mðŸ” Initial queries: ${allQueries}\\x1b[0m`);\r\n\r\n    if (allQueries.length === 0) {\r\n      console.error(\"ERROR: No initial queries generated\");\r\n      return [];\r\n    }\r\n\r\n    return allQueries;\r\n  }\r\n\r\n  /**\r\n   * Generate research queries for a given topic using LLM\r\n   *\r\n   * @param topic The research topic\r\n   * @returns List of search queries\r\n   */\r\n  private async generateResearchQueries(topic: string): Promise<string[]> {\r\n    const parsedPlan = await generateObject({\r\n      model: this.modelConfig.json,\r\n      system: this.prompts.planningPrompt,\r\n      prompt: `Research Topic: ${topic}`,\r\n      schema: this.researchPlanSchema,\r\n    });\r\n\r\n    console.log(\r\n      `\\x1b[35mðŸ“‹ Research queries generated: \\n - ${parsedPlan.object.queries.join(\r\n        \"\\n - \",\r\n      )}\\x1b[0m`,\r\n    );\r\n\r\n    return parsedPlan.object.queries;\r\n  }\r\n\r\n  /**\r\n   * Perform a single web search\r\n   */\r\n  private async webSearch(query: string): Promise<SearchResults> {\r\n    console.log(`\\x1b[34mðŸ”Ž Perform web search with query: ${query}\\x1b[0m`);\r\n\r\n    // Truncate long queries to avoid issues (like in the Python version)\r\n    if (query.length > 400) {\r\n      query = query.substring(0, 400);\r\n      console.log(\r\n        `\\x1b[33mâš ï¸ Truncated query to 400 characters: ${query}\\x1b[0m`,\r\n      );\r\n    }\r\n\r\n    const searchResults = await queryVectorStore(this.namespace, {\r\n      ...(this.queryOptions ?? { topK: 10 }),\r\n      query,\r\n    });\r\n    const results = (searchResults?.results ?? []).map((result) => {\r\n      return new SearchResult({\r\n        id: result.id,\r\n        metadata: result.metadata,\r\n        content: result.text,\r\n      });\r\n    });\r\n\r\n    console.log(\r\n      `\\x1b[32mðŸ“Š Web Search Responded with ${results.length} results\\x1b[0m`,\r\n    );\r\n\r\n    // Process and summarize raw content if available\r\n    const processedResults = await this.processSearchResultsWithSummarization(\r\n      query,\r\n      results,\r\n    );\r\n\r\n    return new SearchResults(processedResults);\r\n  }\r\n\r\n  /**\r\n   * Process search results with content summarization\r\n   *\r\n   * @param query The search query\r\n   * @param results The search results to process\r\n   * @returns Processed search results with summarized content\r\n   */\r\n  private async processSearchResultsWithSummarization(\r\n    query: string,\r\n    results: SearchResult[],\r\n  ): Promise<SearchResult[]> {\r\n    // Create tasks for summarization\r\n    const summarizationTasks = [];\r\n    const resultInfo = [];\r\n\r\n    for (const result of results) {\r\n      if (!result.content) {\r\n        continue;\r\n      }\r\n\r\n      // Create a task for summarization\r\n      const task = this._summarize_content_async({\r\n        result,\r\n        query,\r\n      });\r\n\r\n      summarizationTasks.push(task);\r\n      resultInfo.push(result);\r\n    }\r\n\r\n    // Wait for all summarization tasks to complete\r\n    const summarizedContents = await Promise.all(summarizationTasks);\r\n\r\n    // Combine results with summarized content\r\n    const formattedResults: SearchResult[] = [];\r\n    for (let i = 0; i < resultInfo.length; i++) {\r\n      const result = resultInfo[i]!;\r\n      const summarizedContent = summarizedContents[i]!;\r\n\r\n      formattedResults.push(\r\n        new SearchResult({\r\n          id: result.id,\r\n          metadata: result.metadata,\r\n          content: summarizedContent,\r\n        }),\r\n      );\r\n    }\r\n\r\n    return formattedResults;\r\n  }\r\n\r\n  /**\r\n   * Summarize content asynchronously using the LLM\r\n   *\r\n   * @param props The props object containing searchResult and query\r\n   * @returns The summarized content\r\n   */\r\n  private async _summarize_content_async(props: {\r\n    result: SearchResult;\r\n    query: string;\r\n  }): Promise<string> {\r\n    console.log(\r\n      `\\x1b[36mðŸ“ Summarizing content from ID: ${props.result.id}\\x1b[0m`,\r\n    );\r\n\r\n    const result = await generateText({\r\n      model: this.modelConfig.summary,\r\n      system: this.prompts.rawContentSummarizerPrompt,\r\n      prompt: `<Raw Content>${props.result.content}</Raw Content>\\n\\n<Research Topic>${props.query}</Research Topic>`,\r\n    });\r\n\r\n    return result.text;\r\n  }\r\n\r\n  /**\r\n   * Execute searches for all queries in parallel\r\n   *\r\n   * @param queries List of search queries\r\n   * @returns Combined search results\r\n   */\r\n  private async performSearch({\r\n    queries,\r\n  }: {\r\n    queries: string[];\r\n  }): Promise<SearchResults> {\r\n    const tasks = queries.map(async (query) => {\r\n      // Perform search\r\n      const results = await this.webSearch(query);\r\n      return results;\r\n    });\r\n\r\n    const resultsList = await Promise.all(tasks);\r\n\r\n    let combinedResults = new SearchResults([]);\r\n    for (const results of resultsList) {\r\n      combinedResults = combinedResults.add(results);\r\n    }\r\n\r\n    const combinedResultsDedup = combinedResults.dedup();\r\n    console.log(\r\n      `Search complete, found ${combinedResultsDedup.results.length} results after deduplication`,\r\n    );\r\n\r\n    return combinedResultsDedup;\r\n  }\r\n\r\n  /**\r\n   * Evaluate if the current search results are sufficient or if more research is needed\r\n   *\r\n   * @param topic The research topic\r\n   * @param results Current search results\r\n   * @param queries List of queries already used\r\n   * @returns List of additional queries needed or empty list if research is complete\r\n   */\r\n  private async evaluateResearchCompleteness(\r\n    topic: string,\r\n    results: SearchResults,\r\n    queries: string[],\r\n  ): Promise<string[]> {\r\n    const formattedResults = results.toString();\r\n\r\n    // context length issue here!\r\n\r\n    const evaluation = await generateText({\r\n      model: this.modelConfig.planning,\r\n      system: this.prompts.evaluationPrompt,\r\n      prompt:\r\n        `<Research Topic>${topic}</Research Topic>\\n\\n` +\r\n        `<Search Queries Used>${queries}</Search Queries Used>\\n\\n` +\r\n        `<Current Search Results>${formattedResults}</Current Search Results>`,\r\n    });\r\n\r\n    // console.log(\r\n    //   \"\\x1b[43mðŸ”„ ================================================\\x1b[0m\\n\\n\"\r\n    // );\r\n    // console.log(`\\x1b[36mðŸ“ Evaluation:\\n\\n ${evaluation.text}\\x1b[0m`);\r\n\r\n    const parsedEvaluation = await generateObject({\r\n      model: this.modelConfig.json,\r\n      system: this.prompts.evaluationParsingPrompt,\r\n      prompt: `Evaluation to be parsed: ${evaluation.text}`,\r\n      schema: this.researchPlanSchema,\r\n    });\r\n\r\n    return parsedEvaluation.object.queries;\r\n  }\r\n\r\n  /**\r\n   * Process search results by deduplicating and filtering\r\n   *\r\n   * @param topic The research topic\r\n   * @param results Search results to process\r\n   * @returns Filtered search results\r\n   */\r\n  private async processSearchResults({\r\n    topic,\r\n    results,\r\n  }: {\r\n    topic: string;\r\n    results: SearchResults;\r\n  }): Promise<SearchResults> {\r\n    // Deduplicate results\r\n    results = results.dedup();\r\n    console.log(\r\n      `Search complete, found ${results.results.length} results after deduplication`,\r\n    );\r\n\r\n    return results;\r\n  }\r\n\r\n  /**\r\n   * Filter search results based on relevance to the topic\r\n   *\r\n   * @param topic The research topic\r\n   * @param results Search results to filter\r\n   * @returns Tuple of (filtered results, source list)\r\n   */\r\n  private async filterResults({\r\n    topic,\r\n    results,\r\n  }: {\r\n    topic: string;\r\n    results: SearchResults;\r\n  }): Promise<FilteredResultsData> {\r\n    const formattedResults = results.toString();\r\n\r\n    const filterResponse = await generateText({\r\n      model: this.modelConfig.planning,\r\n      system: this.prompts.filterPrompt,\r\n      prompt: `<Research Topic>${topic}</Research Topic>\\n\\n<Current Search Results>${formattedResults}</Current Search Results>`,\r\n    });\r\n\r\n    // console.log(`\\x1b[36mðŸ“ Filter response: ${filterResponse.text}\\x1b[0m`);\r\n\r\n    const parsedFilter = await generateObject({\r\n      model: this.modelConfig.json,\r\n      system: this.prompts.sourceParsingPrompt,\r\n      prompt: `Filter response to be parsed: ${filterResponse.text}`,\r\n      schema: this.sourceListSchema,\r\n    });\r\n\r\n    const sources = parsedFilter.object.sources;\r\n    console.log(`\\x1b[36mðŸ“Š Filtered sources: ${sources}\\x1b[0m`);\r\n\r\n    // Limit sources if needed\r\n    let limitedSources = sources;\r\n    if (this.researchConfig.maxSources > 0) {\r\n      limitedSources = sources.slice(0, this.researchConfig.maxSources);\r\n    }\r\n\r\n    // Filter the results based on the source list\r\n    const filteredResults = new SearchResults(\r\n      limitedSources\r\n        .filter((i) => i > 0 && i <= results.results.length)\r\n        .map((i) => results.results[i - 1]) as SearchResult[],\r\n    );\r\n\r\n    return {\r\n      filteredResults,\r\n      sourceIndices: limitedSources,\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Conduct iterative research within budget to refine results\r\n   *\r\n   * @param topic The research topic\r\n   * @param initialResults Results from initial search\r\n   * @param allQueries List of all queries used so far\r\n   * @returns Tuple of (final results, all queries used)\r\n   */\r\n  private async conductIterativeResearch({\r\n    topic,\r\n    initialResults,\r\n    allQueries,\r\n  }: {\r\n    topic: string;\r\n    initialResults: SearchResults;\r\n    allQueries: string[];\r\n  }): Promise<IterativeResearchResult> {\r\n    let results = initialResults;\r\n\r\n    for (let i = 0; i < this.researchConfig.budget; i++) {\r\n      // Evaluate if more research is needed\r\n      const additionalQueries = await this.evaluateResearchCompleteness(\r\n        topic,\r\n        results,\r\n        allQueries,\r\n      );\r\n\r\n      // Exit if research is complete\r\n      if (additionalQueries.length === 0) {\r\n        console.log(\"\\x1b[33mâœ… No need for additional research\\x1b[0m\");\r\n        break;\r\n      }\r\n\r\n      // Limit the number of queries if needed\r\n      let queriesToUse = additionalQueries;\r\n      if (this.researchConfig.maxQueries > 0) {\r\n        queriesToUse = additionalQueries.slice(\r\n          0,\r\n          this.researchConfig.maxQueries,\r\n        );\r\n      }\r\n\r\n      // console.log(\r\n      //   \"\\x1b[43mðŸ”„ ================================================\\x1b[0m\\n\\n\"\r\n      // );\r\n      console.log(\r\n        `\\x1b[36mðŸ“‹ Additional queries from evaluation parser: ${queriesToUse}\\n\\n\\x1b[0m`,\r\n      );\r\n      // console.log(\r\n      //   \"\\x1b[43mðŸ”„ ================================================\\x1b[0m\\n\\n\"\r\n      // );\r\n\r\n      // Expand research with new queries\r\n      const newResults = await this.performSearch({ queries: queriesToUse });\r\n      results = results.add(newResults);\r\n      allQueries.push(...queriesToUse);\r\n    }\r\n\r\n    return {\r\n      finalSearchResults: results,\r\n      queriesUsed: allQueries,\r\n    };\r\n  }\r\n\r\n  /**\r\n   * Run the complete research pipeline\r\n   *\r\n   * @param topic The research topic\r\n   * @returns The research answer\r\n   */\r\n  async runResearch(topic: string) {\r\n    console.log(`\\x1b[36mðŸ” Researching topic: ${topic}\\x1b[0m`);\r\n\r\n    // Step 1: Generate initial queries\r\n    const initialQueries = await this.generateInitialQueries({ topic });\r\n\r\n    // Step 2: Perform initial search\r\n    const initialResults = await this.performSearch({\r\n      queries: initialQueries,\r\n    });\r\n\r\n    // Step 3: Conduct iterative research\r\n    const { finalSearchResults, queriesUsed } =\r\n      await this.conductIterativeResearch({\r\n        topic,\r\n        initialResults,\r\n        allQueries: initialQueries,\r\n      });\r\n\r\n    // Step 4: Process search results\r\n    const processedResults = await this.processSearchResults({\r\n      topic,\r\n      results: finalSearchResults,\r\n    });\r\n\r\n    // Step 4.5: Filter results based on relevance\r\n    const { filteredResults, sourceIndices } = await this.filterResults({\r\n      topic,\r\n      results: processedResults,\r\n    });\r\n\r\n    console.log(\r\n      `\\x1b[32mðŸ“Š Filtered results: ${filteredResults.results.length} sources kept\\x1b[0m`,\r\n    );\r\n\r\n    // Step 5: Generate research answer with feedback loop\r\n    const answer = this.generateResearchAnswer({\r\n      topic,\r\n      results: filteredResults,\r\n    });\r\n\r\n    return answer;\r\n  }\r\n\r\n  /**\r\n   * Generate a comprehensive answer to the research topic based on the search results\r\n   *\r\n   * @param topic The research topic\r\n   * @param results Filtered search results to use for answer generation\r\n   * @returns Detailed research answer as a string\r\n   */\r\n  private generateResearchAnswer({\r\n    topic,\r\n    results,\r\n  }: {\r\n    topic: string;\r\n    results: SearchResults;\r\n  }) {\r\n    const formattedResults = results.toString();\r\n\r\n    const enhancedModel = wrapLanguageModel({\r\n      model: this.modelConfig.answer,\r\n      middleware: extractReasoningMiddleware({ tagName: \"think\" }),\r\n    });\r\n\r\n    const answer = streamText({\r\n      model: enhancedModel,\r\n      system: this.prompts.answerPrompt,\r\n      prompt: `Research Topic: ${topic}\\n\\nSearch Results:\\n${formattedResults}`,\r\n      maxTokens: this.researchConfig.maxTokens,\r\n    });\r\n\r\n    return answer;\r\n  }\r\n}\r\n"],"names":[],"mappings":"AAAA;;CAEC;;;AAGD;AAOA;AAMA;AAAA;AACA;AACA;;;;;;AAeO,MAAM;IACH,UAAqB;IACrB,aAAsD;IAEtD,YAAyB;IACzB,eAAuC;IACvC,QAAwB;IACxB,kBAA0B,EAAE;IAE5B,qBAAqB,sIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;QACpC,SAAS,sIAAA,CAAA,IAAC,CACP,MAAM,GACN,KAAK,GACL,QAAQ,CAAC;IACd,GAAG;IAEK,mBAAmB,sIAAA,CAAA,IAAC,CAAC,MAAM,CAAC;QAClC,SAAS,sIAAA,CAAA,IAAC,CAAC,KAAK,CAAC,sIAAA,CAAA,IAAC,CAAC,MAAM,IAAI,QAAQ,CAAC;IACxC,GAAG;IAEH,YACE,SAAoB,EACpB,EACE,iBAAiB,yJAAA,CAAA,kBAAe,EAChC,UAAU,yJAAA,CAAA,UAAO,EACjB,WAAW,EACX,YAAY,EACZ,GAAG,SASJ,CACD;QACA,IAAI,CAAC,SAAS,GAAG;QACjB,IAAI,CAAC,WAAW,GAAG;QACnB,IAAI,CAAC,YAAY,GAAG;QACpB,IAAI,CAAC,cAAc,GAAG;QACtB,IAAI,CAAC,OAAO,GAAG;QAEf,+BAA+B;QAC/B,IAAI,QAAQ,UAAU,KAAK,WAAW;YACpC,IAAI,CAAC,cAAc,CAAC,UAAU,GAAG,QAAQ,UAAU;QACrD;QACA,IAAI,QAAQ,UAAU,KAAK,WAAW;YACpC,IAAI,CAAC,cAAc,CAAC,UAAU,GAAG,QAAQ,UAAU;QACrD;QACA,IAAI,QAAQ,mBAAmB,KAAK,WAAW;YAC7C,IAAI,CAAC,cAAc,CAAC,SAAS,GAAG,QAAQ,mBAAmB;QAC7D;IACF;IAEA;;;;;GAKC,GACD,MAAc,uBAAuB,EACnC,KAAK,EAGN,EAAqB;QACpB,IAAI,aAAa,MAAM,IAAI,CAAC,uBAAuB,CAAC;QAEpD,IAAI,IAAI,CAAC,cAAc,CAAC,UAAU,GAAG,GAAG;YACtC,aAAa,WAAW,KAAK,CAAC,GAAG,IAAI,CAAC,cAAc,CAAC,UAAU;QACjE;QAEA,QAAQ,GAAG,CAAC,CAAC,gCAAgC,EAAE,WAAW,OAAO,CAAC;QAElE,IAAI,WAAW,MAAM,KAAK,GAAG;YAC3B,QAAQ,KAAK,CAAC;YACd,OAAO,EAAE;QACX;QAEA,OAAO;IACT;IAEA;;;;;GAKC,GACD,MAAc,wBAAwB,KAAa,EAAqB;QACtE,MAAM,aAAa,MAAM,CAAA,GAAA,sJAAA,CAAA,iBAAc,AAAD,EAAE;YACtC,OAAO,IAAI,CAAC,WAAW,CAAC,IAAI;YAC5B,QAAQ,IAAI,CAAC,OAAO,CAAC,cAAc;YACnC,QAAQ,CAAC,gBAAgB,EAAE,OAAO;YAClC,QAAQ,IAAI,CAAC,kBAAkB;QACjC;QAEA,QAAQ,GAAG,CACT,CAAC,4CAA4C,EAAE,WAAW,MAAM,CAAC,OAAO,CAAC,IAAI,CAC3E,SACA,OAAO,CAAC;QAGZ,OAAO,WAAW,MAAM,CAAC,OAAO;IAClC;IAEA;;GAEC,GACD,MAAc,UAAU,KAAa,EAA0B;QAC7D,QAAQ,GAAG,CAAC,CAAC,0CAA0C,EAAE,MAAM,OAAO,CAAC;QAEvE,qEAAqE;QACrE,IAAI,MAAM,MAAM,GAAG,KAAK;YACtB,QAAQ,MAAM,SAAS,CAAC,GAAG;YAC3B,QAAQ,GAAG,CACT,CAAC,8CAA8C,EAAE,MAAM,OAAO,CAAC;QAEnE;QAEA,MAAM,gBAAgB,MAAM,CAAA,GAAA,uJAAA,CAAA,mBAAgB,AAAD,EAAE,IAAI,CAAC,SAAS,EAAE;YAC3D,GAAI,IAAI,CAAC,YAAY,IAAI;gBAAE,MAAM;YAAG,CAAC;YACrC;QACF;QACA,MAAM,UAAU,CAAC,eAAe,WAAW,EAAE,EAAE,GAAG,CAAC,CAAC;YAClD,OAAO,IAAI,0JAAA,CAAA,eAAY,CAAC;gBACtB,IAAI,OAAO,EAAE;gBACb,UAAU,OAAO,QAAQ;gBACzB,SAAS,OAAO,IAAI;YACtB;QACF;QAEA,QAAQ,GAAG,CACT,CAAC,qCAAqC,EAAE,QAAQ,MAAM,CAAC,eAAe,CAAC;QAGzE,iDAAiD;QACjD,MAAM,mBAAmB,MAAM,IAAI,CAAC,qCAAqC,CACvE,OACA;QAGF,OAAO,IAAI,0JAAA,CAAA,gBAAa,CAAC;IAC3B;IAEA;;;;;;GAMC,GACD,MAAc,sCACZ,KAAa,EACb,OAAuB,EACE;QACzB,iCAAiC;QACjC,MAAM,qBAAqB,EAAE;QAC7B,MAAM,aAAa,EAAE;QAErB,KAAK,MAAM,UAAU,QAAS;YAC5B,IAAI,CAAC,OAAO,OAAO,EAAE;gBACnB;YACF;YAEA,kCAAkC;YAClC,MAAM,OAAO,IAAI,CAAC,wBAAwB,CAAC;gBACzC;gBACA;YACF;YAEA,mBAAmB,IAAI,CAAC;YACxB,WAAW,IAAI,CAAC;QAClB;QAEA,+CAA+C;QAC/C,MAAM,qBAAqB,MAAM,QAAQ,GAAG,CAAC;QAE7C,0CAA0C;QAC1C,MAAM,mBAAmC,EAAE;QAC3C,IAAK,IAAI,IAAI,GAAG,IAAI,WAAW,MAAM,EAAE,IAAK;YAC1C,MAAM,SAAS,UAAU,CAAC,EAAE;YAC5B,MAAM,oBAAoB,kBAAkB,CAAC,EAAE;YAE/C,iBAAiB,IAAI,CACnB,IAAI,0JAAA,CAAA,eAAY,CAAC;gBACf,IAAI,OAAO,EAAE;gBACb,UAAU,OAAO,QAAQ;gBACzB,SAAS;YACX;QAEJ;QAEA,OAAO;IACT;IAEA;;;;;GAKC,GACD,MAAc,yBAAyB,KAGtC,EAAmB;QAClB,QAAQ,GAAG,CACT,CAAC,wCAAwC,EAAE,MAAM,MAAM,CAAC,EAAE,CAAC,OAAO,CAAC;QAGrE,MAAM,SAAS,MAAM,CAAA,GAAA,sJAAA,CAAA,eAAY,AAAD,EAAE;YAChC,OAAO,IAAI,CAAC,WAAW,CAAC,OAAO;YAC/B,QAAQ,IAAI,CAAC,OAAO,CAAC,0BAA0B;YAC/C,QAAQ,CAAC,aAAa,EAAE,MAAM,MAAM,CAAC,OAAO,CAAC,kCAAkC,EAAE,MAAM,KAAK,CAAC,iBAAiB,CAAC;QACjH;QAEA,OAAO,OAAO,IAAI;IACpB;IAEA;;;;;GAKC,GACD,MAAc,cAAc,EAC1B,OAAO,EAGR,EAA0B;QACzB,MAAM,QAAQ,QAAQ,GAAG,CAAC,OAAO;YAC/B,iBAAiB;YACjB,MAAM,UAAU,MAAM,IAAI,CAAC,SAAS,CAAC;YACrC,OAAO;QACT;QAEA,MAAM,cAAc,MAAM,QAAQ,GAAG,CAAC;QAEtC,IAAI,kBAAkB,IAAI,0JAAA,CAAA,gBAAa,CAAC,EAAE;QAC1C,KAAK,MAAM,WAAW,YAAa;YACjC,kBAAkB,gBAAgB,GAAG,CAAC;QACxC;QAEA,MAAM,uBAAuB,gBAAgB,KAAK;QAClD,QAAQ,GAAG,CACT,CAAC,uBAAuB,EAAE,qBAAqB,OAAO,CAAC,MAAM,CAAC,4BAA4B,CAAC;QAG7F,OAAO;IACT;IAEA;;;;;;;GAOC,GACD,MAAc,6BACZ,KAAa,EACb,OAAsB,EACtB,OAAiB,EACE;QACnB,MAAM,mBAAmB,QAAQ,QAAQ;QAEzC,6BAA6B;QAE7B,MAAM,aAAa,MAAM,CAAA,GAAA,sJAAA,CAAA,eAAY,AAAD,EAAE;YACpC,OAAO,IAAI,CAAC,WAAW,CAAC,QAAQ;YAChC,QAAQ,IAAI,CAAC,OAAO,CAAC,gBAAgB;YACrC,QACE,CAAC,gBAAgB,EAAE,MAAM,qBAAqB,CAAC,GAC/C,CAAC,qBAAqB,EAAE,QAAQ,0BAA0B,CAAC,GAC3D,CAAC,wBAAwB,EAAE,iBAAiB,yBAAyB,CAAC;QAC1E;QAEA,eAAe;QACf,6EAA6E;QAC7E,KAAK;QACL,uEAAuE;QAEvE,MAAM,mBAAmB,MAAM,CAAA,GAAA,sJAAA,CAAA,iBAAc,AAAD,EAAE;YAC5C,OAAO,IAAI,CAAC,WAAW,CAAC,IAAI;YAC5B,QAAQ,IAAI,CAAC,OAAO,CAAC,uBAAuB;YAC5C,QAAQ,CAAC,yBAAyB,EAAE,WAAW,IAAI,EAAE;YACrD,QAAQ,IAAI,CAAC,kBAAkB;QACjC;QAEA,OAAO,iBAAiB,MAAM,CAAC,OAAO;IACxC;IAEA;;;;;;GAMC,GACD,MAAc,qBAAqB,EACjC,KAAK,EACL,OAAO,EAIR,EAA0B;QACzB,sBAAsB;QACtB,UAAU,QAAQ,KAAK;QACvB,QAAQ,GAAG,CACT,CAAC,uBAAuB,EAAE,QAAQ,OAAO,CAAC,MAAM,CAAC,4BAA4B,CAAC;QAGhF,OAAO;IACT;IAEA;;;;;;GAMC,GACD,MAAc,cAAc,EAC1B,KAAK,EACL,OAAO,EAIR,EAAgC;QAC/B,MAAM,mBAAmB,QAAQ,QAAQ;QAEzC,MAAM,iBAAiB,MAAM,CAAA,GAAA,sJAAA,CAAA,eAAY,AAAD,EAAE;YACxC,OAAO,IAAI,CAAC,WAAW,CAAC,QAAQ;YAChC,QAAQ,IAAI,CAAC,OAAO,CAAC,YAAY;YACjC,QAAQ,CAAC,gBAAgB,EAAE,MAAM,6CAA6C,EAAE,iBAAiB,yBAAyB,CAAC;QAC7H;QAEA,4EAA4E;QAE5E,MAAM,eAAe,MAAM,CAAA,GAAA,sJAAA,CAAA,iBAAc,AAAD,EAAE;YACxC,OAAO,IAAI,CAAC,WAAW,CAAC,IAAI;YAC5B,QAAQ,IAAI,CAAC,OAAO,CAAC,mBAAmB;YACxC,QAAQ,CAAC,8BAA8B,EAAE,eAAe,IAAI,EAAE;YAC9D,QAAQ,IAAI,CAAC,gBAAgB;QAC/B;QAEA,MAAM,UAAU,aAAa,MAAM,CAAC,OAAO;QAC3C,QAAQ,GAAG,CAAC,CAAC,6BAA6B,EAAE,QAAQ,OAAO,CAAC;QAE5D,0BAA0B;QAC1B,IAAI,iBAAiB;QACrB,IAAI,IAAI,CAAC,cAAc,CAAC,UAAU,GAAG,GAAG;YACtC,iBAAiB,QAAQ,KAAK,CAAC,GAAG,IAAI,CAAC,cAAc,CAAC,UAAU;QAClE;QAEA,8CAA8C;QAC9C,MAAM,kBAAkB,IAAI,0JAAA,CAAA,gBAAa,CACvC,eACG,MAAM,CAAC,CAAC,IAAM,IAAI,KAAK,KAAK,QAAQ,OAAO,CAAC,MAAM,EAClD,GAAG,CAAC,CAAC,IAAM,QAAQ,OAAO,CAAC,IAAI,EAAE;QAGtC,OAAO;YACL;YACA,eAAe;QACjB;IACF;IAEA;;;;;;;GAOC,GACD,MAAc,yBAAyB,EACrC,KAAK,EACL,cAAc,EACd,UAAU,EAKX,EAAoC;QACnC,IAAI,UAAU;QAEd,IAAK,IAAI,IAAI,GAAG,IAAI,IAAI,CAAC,cAAc,CAAC,MAAM,EAAE,IAAK;YACnD,sCAAsC;YACtC,MAAM,oBAAoB,MAAM,IAAI,CAAC,4BAA4B,CAC/D,OACA,SACA;YAGF,+BAA+B;YAC/B,IAAI,kBAAkB,MAAM,KAAK,GAAG;gBAClC,QAAQ,GAAG,CAAC;gBACZ;YACF;YAEA,wCAAwC;YACxC,IAAI,eAAe;YACnB,IAAI,IAAI,CAAC,cAAc,CAAC,UAAU,GAAG,GAAG;gBACtC,eAAe,kBAAkB,KAAK,CACpC,GACA,IAAI,CAAC,cAAc,CAAC,UAAU;YAElC;YAEA,eAAe;YACf,6EAA6E;YAC7E,KAAK;YACL,QAAQ,GAAG,CACT,CAAC,sDAAsD,EAAE,aAAa,WAAW,CAAC;YAEpF,eAAe;YACf,6EAA6E;YAC7E,KAAK;YAEL,mCAAmC;YACnC,MAAM,aAAa,MAAM,IAAI,CAAC,aAAa,CAAC;gBAAE,SAAS;YAAa;YACpE,UAAU,QAAQ,GAAG,CAAC;YACtB,WAAW,IAAI,IAAI;QACrB;QAEA,OAAO;YACL,oBAAoB;YACpB,aAAa;QACf;IACF;IAEA;;;;;GAKC,GACD,MAAM,YAAY,KAAa,EAAE;QAC/B,QAAQ,GAAG,CAAC,CAAC,8BAA8B,EAAE,MAAM,OAAO,CAAC;QAE3D,mCAAmC;QACnC,MAAM,iBAAiB,MAAM,IAAI,CAAC,sBAAsB,CAAC;YAAE;QAAM;QAEjE,iCAAiC;QACjC,MAAM,iBAAiB,MAAM,IAAI,CAAC,aAAa,CAAC;YAC9C,SAAS;QACX;QAEA,qCAAqC;QACrC,MAAM,EAAE,kBAAkB,EAAE,WAAW,EAAE,GACvC,MAAM,IAAI,CAAC,wBAAwB,CAAC;YAClC;YACA;YACA,YAAY;QACd;QAEF,iCAAiC;QACjC,MAAM,mBAAmB,MAAM,IAAI,CAAC,oBAAoB,CAAC;YACvD;YACA,SAAS;QACX;QAEA,8CAA8C;QAC9C,MAAM,EAAE,eAAe,EAAE,aAAa,EAAE,GAAG,MAAM,IAAI,CAAC,aAAa,CAAC;YAClE;YACA,SAAS;QACX;QAEA,QAAQ,GAAG,CACT,CAAC,6BAA6B,EAAE,gBAAgB,OAAO,CAAC,MAAM,CAAC,oBAAoB,CAAC;QAGtF,sDAAsD;QACtD,MAAM,SAAS,IAAI,CAAC,sBAAsB,CAAC;YACzC;YACA,SAAS;QACX;QAEA,OAAO;IACT;IAEA;;;;;;GAMC,GACD,AAAQ,uBAAuB,EAC7B,KAAK,EACL,OAAO,EAIR,EAAE;QACD,MAAM,mBAAmB,QAAQ,QAAQ;QAEzC,MAAM,gBAAgB,CAAA,GAAA,sJAAA,CAAA,oBAAiB,AAAD,EAAE;YACtC,OAAO,IAAI,CAAC,WAAW,CAAC,MAAM;YAC9B,YAAY,CAAA,GAAA,sJAAA,CAAA,6BAA0B,AAAD,EAAE;gBAAE,SAAS;YAAQ;QAC5D;QAEA,MAAM,SAAS,CAAA,GAAA,sJAAA,CAAA,aAAU,AAAD,EAAE;YACxB,OAAO;YACP,QAAQ,IAAI,CAAC,OAAO,CAAC,YAAY;YACjC,QAAQ,CAAC,gBAAgB,EAAE,MAAM,qBAAqB,EAAE,kBAAkB;YAC1E,WAAW,IAAI,CAAC,cAAc,CAAC,SAAS;QAC1C;QAEA,OAAO;IACT;AACF","debugId":null}},
    {"offset": {"line": 2741, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/lib/llm.ts"],"sourcesContent":["import { env } from \"@/env\";\n\nimport type { LLMConfig } from \"@agentset/db\";\n\nexport const getNamespaceLanguageModel = async (config?: LLMConfig) => {\n  if (!config) {\n    const { createOpenAI } = await import(\"@ai-sdk/openai\");\n\n    const defaultOpenAI = createOpenAI({\n      apiKey: env.DEFAULT_OPENAI_API_KEY,\n    });\n\n    return defaultOpenAI.languageModel(env.DEFAULT_OPENAI_MODEL || \"gpt-4\");\n  }\n\n  switch (config.provider) {\n    case \"OPENAI\": {\n      const { createOpenAI } = await import(\"@ai-sdk/openai\");\n\n      const { apiKey, model } = config;\n      const openai = createOpenAI({ apiKey });\n      return openai.languageModel(model);\n    }\n\n    case \"AZURE_OPENAI\": {\n      const { createOpenAI } = await import(\"@ai-sdk/openai\");\n\n      const { apiKey, model } = config;\n      const openai = createOpenAI({ apiKey });\n      return openai.languageModel(model);\n    }\n\n    default: {\n      // This exhaustive check ensures TypeScript will error if a new provider\n      // is added without handling it in the switch statement\n      const _exhaustiveCheck: never = config;\n      throw new Error(`Unknown vector store provider: ${_exhaustiveCheck}`);\n    }\n  }\n};\n"],"names":[],"mappings":";;;AAAA;;AAIO,MAAM,4BAA4B,OAAO;IAC9C,IAAI,CAAC,QAAQ;QACX,MAAM,EAAE,YAAY,EAAE,GAAG;QAEzB,MAAM,gBAAgB,aAAa;YACjC,QAAQ,2HAAA,CAAA,MAAG,CAAC,sBAAsB;QACpC;QAEA,OAAO,cAAc,aAAa,CAAC,2HAAA,CAAA,MAAG,CAAC,oBAAoB,IAAI;IACjE;IAEA,OAAQ,OAAO,QAAQ;QACrB,KAAK;YAAU;gBACb,MAAM,EAAE,YAAY,EAAE,GAAG;gBAEzB,MAAM,EAAE,MAAM,EAAE,KAAK,EAAE,GAAG;gBAC1B,MAAM,SAAS,aAAa;oBAAE;gBAAO;gBACrC,OAAO,OAAO,aAAa,CAAC;YAC9B;QAEA,KAAK;YAAgB;gBACnB,MAAM,EAAE,YAAY,EAAE,GAAG;gBAEzB,MAAM,EAAE,MAAM,EAAE,KAAK,EAAE,GAAG;gBAC1B,MAAM,SAAS,aAAa;oBAAE;gBAAO;gBACrC,OAAO,OAAO,aAAa,CAAC;YAC9B;QAEA;YAAS;gBACP,wEAAwE;gBACxE,uDAAuD;gBACvD,MAAM,mBAA0B;gBAChC,MAAM,IAAI,MAAM,CAAC,+BAA+B,EAAE,kBAAkB;YACtE;IACF;AACF","debugId":null}},
    {"offset": {"line": 2788, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/schemas/api/query.ts"],"sourcesContent":["import z from \"@/lib/zod\";\r\n\r\nexport const baseQueryVectorStoreSchema = z.object({\r\n  query: z.string().describe(\"The query to search for.\"),\r\n  topK: z\r\n    .number()\r\n    .min(1)\r\n    .max(100)\r\n    .optional()\r\n    .default(10)\r\n    .describe(\r\n      \"The number of results to fetch from the vector store. Defaults to `10`.\",\r\n    ),\r\n  rerank: z\r\n    .boolean()\r\n    .optional()\r\n    .default(true)\r\n    .describe(\"Whether to rerank the results. Defaults to `true`.\"),\r\n  rerankLimit: z\r\n    .number()\r\n    .min(1)\r\n    .max(100)\r\n    .optional()\r\n    .describe(\r\n      \"The number of results to return after reranking. Defaults to `topK`.\",\r\n    ),\r\n  filter: z\r\n    .record(z.string(), z.any())\r\n    .optional()\r\n    .describe(\"A filter to apply to the results.\"),\r\n  minScore: z\r\n    .number()\r\n    .min(0)\r\n    .max(1)\r\n    .optional()\r\n    .describe(\"The minimum score to return.\"),\r\n  includeRelationships: z\r\n    .boolean()\r\n    .optional()\r\n    .default(false)\r\n    .describe(\r\n      \"Whether to include relationships in the results. Defaults to `false`.\",\r\n    ),\r\n  includeMetadata: z\r\n    .boolean()\r\n    .optional()\r\n    .default(true)\r\n    .describe(\r\n      \"Whether to include metadata in the results. Defaults to `true`.\",\r\n    ),\r\n});\r\n\r\nexport const refineRereankLimit = <\r\n  T extends Partial<(typeof baseQueryVectorStoreSchema)[\"shape\"]> &\r\n    z.ZodRawShape,\r\n  Out extends {\r\n    rerankLimit?: number;\r\n    topK: number;\r\n    [key: string]: any;\r\n  },\r\n>(\r\n  schema: z.ZodObject<T, \"strip\", z.ZodTypeAny, Out>,\r\n) => {\r\n  return schema.superRefine((val, ctx) => {\r\n    if (val.rerankLimit && val.rerankLimit > val.topK) {\r\n      ctx.addIssue({\r\n        path: [\"rerankLimit\"],\r\n        code: z.ZodIssueCode.too_big,\r\n        message: \"rerankLimit cannot be larger than topK\",\r\n        inclusive: true,\r\n        type: \"number\",\r\n        maximum: val.topK,\r\n      });\r\n      return false;\r\n    }\r\n\r\n    return true;\r\n  });\r\n};\r\n\r\nexport const queryVectorStoreSchema = refineRereankLimit(\r\n  baseQueryVectorStoreSchema,\r\n);\r\n"],"names":[],"mappings":";;;;;AAAA;;AAEO,MAAM,6BAA6B,2IAAA,CAAA,UAAC,CAAC,MAAM,CAAC;IACjD,OAAO,2IAAA,CAAA,UAAC,CAAC,MAAM,GAAG,QAAQ,CAAC;IAC3B,MAAM,2IAAA,CAAA,UAAC,CACJ,MAAM,GACN,GAAG,CAAC,GACJ,GAAG,CAAC,KACJ,QAAQ,GACR,OAAO,CAAC,IACR,QAAQ,CACP;IAEJ,QAAQ,2IAAA,CAAA,UAAC,CACN,OAAO,GACP,QAAQ,GACR,OAAO,CAAC,MACR,QAAQ,CAAC;IACZ,aAAa,2IAAA,CAAA,UAAC,CACX,MAAM,GACN,GAAG,CAAC,GACJ,GAAG,CAAC,KACJ,QAAQ,GACR,QAAQ,CACP;IAEJ,QAAQ,2IAAA,CAAA,UAAC,CACN,MAAM,CAAC,2IAAA,CAAA,UAAC,CAAC,MAAM,IAAI,2IAAA,CAAA,UAAC,CAAC,GAAG,IACxB,QAAQ,GACR,QAAQ,CAAC;IACZ,UAAU,2IAAA,CAAA,UAAC,CACR,MAAM,GACN,GAAG,CAAC,GACJ,GAAG,CAAC,GACJ,QAAQ,GACR,QAAQ,CAAC;IACZ,sBAAsB,2IAAA,CAAA,UAAC,CACpB,OAAO,GACP,QAAQ,GACR,OAAO,CAAC,OACR,QAAQ,CACP;IAEJ,iBAAiB,2IAAA,CAAA,UAAC,CACf,OAAO,GACP,QAAQ,GACR,OAAO,CAAC,MACR,QAAQ,CACP;AAEN;AAEO,MAAM,qBAAqB,CAShC;IAEA,OAAO,OAAO,WAAW,CAAC,CAAC,KAAK;QAC9B,IAAI,IAAI,WAAW,IAAI,IAAI,WAAW,GAAG,IAAI,IAAI,EAAE;YACjD,IAAI,QAAQ,CAAC;gBACX,MAAM;oBAAC;iBAAc;gBACrB,MAAM,2IAAA,CAAA,UAAC,CAAC,YAAY,CAAC,OAAO;gBAC5B,SAAS;gBACT,WAAW;gBACX,MAAM;gBACN,SAAS,IAAI,IAAI;YACnB;YACA,OAAO;QACT;QAEA,OAAO;IACT;AACF;AAEO,MAAM,yBAAyB,mBACpC","debugId":null}},
    {"offset": {"line": 2830, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/app/api/%28internal-api%29/chat/schema.ts"],"sourcesContent":["import { DEFAULT_SYSTEM_PROMPT } from \"@/lib/prompts\";\r\nimport z from \"@/lib/zod\";\r\nimport {\r\n  baseQueryVectorStoreSchema,\r\n  refineRereankLimit,\r\n} from \"@/schemas/api/query\";\r\nimport { coreMessageSchema } from \"ai\";\r\n\r\nexport const chatSchema = refineRereankLimit(\r\n  baseQueryVectorStoreSchema.omit({ query: true }).extend({\r\n    systemPrompt: z\r\n      .string()\r\n      .optional()\r\n      .default(DEFAULT_SYSTEM_PROMPT.compile())\r\n      .describe(\r\n        \"The system prompt to use for the chat. Defaults to the default system prompt.\",\r\n      ),\r\n    messages: z.array(coreMessageSchema),\r\n    temperature: z.number().optional(),\r\n    mode: z.enum([\"normal\", \"agentic\", \"deepResearch\"]).optional(),\r\n  }),\r\n);\r\n"],"names":[],"mappings":";;;AAAA;AACA;AACA;AAIA;;;;;AAEO,MAAM,aAAa,CAAA,GAAA,+IAAA,CAAA,qBAAkB,AAAD,EACzC,+IAAA,CAAA,6BAA0B,CAAC,IAAI,CAAC;IAAE,OAAO;AAAK,GAAG,MAAM,CAAC;IACtD,cAAc,2IAAA,CAAA,UAAC,CACZ,MAAM,GACN,QAAQ,GACR,OAAO,CAAC,sIAAA,CAAA,wBAAqB,CAAC,OAAO,IACrC,QAAQ,CACP;IAEJ,UAAU,2IAAA,CAAA,UAAC,CAAC,KAAK,CAAC,sJAAA,CAAA,oBAAiB;IACnC,aAAa,2IAAA,CAAA,UAAC,CAAC,MAAM,GAAG,QAAQ;IAChC,MAAM,2IAAA,CAAA,UAAC,CAAC,IAAI,CAAC;QAAC;QAAU;QAAW;KAAe,EAAE,QAAQ;AAC9D","debugId":null}},
    {"offset": {"line": 2859, "column": 0}, "map": {"version":3,"sources":["file:///C:/app/agentset/apps/web/src/app/api/%28internal-api%29/chat/route.ts"],"sourcesContent":["import type { CoreMessage, JSONValue } from \"ai\";\r\nimport agenticPipeline from \"@/lib/agentic\";\r\nimport { AgentsetApiError } from \"@/lib/api/errors\";\r\nimport { withAuthApiHandler } from \"@/lib/api/handler\";\r\nimport { parseRequestBody } from \"@/lib/api/utils\";\r\nimport { DeepResearchPipeline } from \"@/lib/deep-research\";\r\nimport { getNamespaceLanguageModel } from \"@/lib/llm\";\r\nimport {\r\n  CONDENSE_SYSTEM_PROMPT,\r\n  CONDENSE_USER_PROMPT,\r\n  NEW_MESSAGE_PROMPT,\r\n} from \"@/lib/prompts\";\r\nimport { queryVectorStore } from \"@/lib/vector-store\";\r\nimport { waitUntil } from \"@vercel/functions\";\r\nimport { createDataStreamResponse, generateText, streamText } from \"ai\";\r\n\r\nimport { db } from \"@agentset/db\";\r\n\r\nimport { chatSchema } from \"./schema\";\r\n\r\nconst incrementUsage = (namespaceId: string, queries: number) => {\r\n  waitUntil(\r\n    (async () => {\r\n      // track usage\r\n      await db.namespace.update({\r\n        where: {\r\n          id: namespaceId,\r\n        },\r\n        data: {\r\n          totalPlaygroundUsage: { increment: 1 },\r\n          organization: {\r\n            update: {\r\n              searchUsage: { increment: queries },\r\n            },\r\n          },\r\n        },\r\n      });\r\n    })(),\r\n  );\r\n};\r\n\r\n// export const runtime = \"edge\";\r\nexport const preferredRegion = \"iad1\"; // make this closer to the DB\r\nexport const maxDuration = 60;\r\n\r\nexport const POST = withAuthApiHandler(\r\n  async ({ req, namespace, tenantId, headers }) => {\r\n    const body = await chatSchema.parseAsync(await parseRequestBody(req));\r\n\r\n    const messagesWithoutQuery = body.messages.slice(0, -1);\r\n    const lastMessage =\r\n      body.messages.length > 0\r\n        ? (body.messages[body.messages.length - 1]!.content as string)\r\n        : null;\r\n\r\n    if (!lastMessage) {\r\n      throw new AgentsetApiError({\r\n        code: \"bad_request\",\r\n        message: \"Messages must contain at least one message\",\r\n      });\r\n    }\r\n\r\n    // TODO: pass namespace config\r\n    const languageModel = await getNamespaceLanguageModel();\r\n\r\n    let query: string;\r\n    if (messagesWithoutQuery.length === 0 || body.mode === \"agentic\") {\r\n      query = lastMessage;\r\n    } else {\r\n      // limit messagesWithoutQuery to the last 10 messages\r\n      const messagesToCondense = messagesWithoutQuery.slice(-10);\r\n\r\n      // we need to condense the messages + last message into a single query\r\n      query = (\r\n        await generateText({\r\n          model: languageModel,\r\n          prompt: CONDENSE_SYSTEM_PROMPT.compile({\r\n            question: lastMessage,\r\n            chatHistory: CONDENSE_USER_PROMPT.compile({\r\n              query: lastMessage,\r\n              chatHistory: messagesToCondense\r\n                .map(\r\n                  (m) =>\r\n                    `- ${m.role === \"user\" ? \"Human\" : \"Assistant\"}: ${m.content as string}`,\r\n                )\r\n                .join(\"\\n\\n\"),\r\n            }),\r\n          }),\r\n        })\r\n      ).text;\r\n    }\r\n\r\n    if (body.mode === \"deepResearch\") {\r\n      const pipeline = new DeepResearchPipeline(namespace, {\r\n        modelConfig: {\r\n          json: languageModel,\r\n          planning: languageModel,\r\n          summary: languageModel,\r\n          answer: languageModel,\r\n        },\r\n        queryOptions: {\r\n          tenantId,\r\n          topK: body.topK,\r\n          minScore: body.minScore,\r\n          filter: body.filter,\r\n          includeMetadata: body.includeMetadata,\r\n          includeRelationships: body.includeRelationships,\r\n          rerankLimit: body.rerankLimit,\r\n          rerank: body.rerank,\r\n        },\r\n        // maxQueries\r\n      });\r\n\r\n      const answer = await pipeline.runResearch(query);\r\n      incrementUsage(namespace.id, 1);\r\n\r\n      return answer.toDataStreamResponse({ headers });\r\n    }\r\n\r\n    if (body.mode === \"agentic\") {\r\n      const result = agenticPipeline(namespace, {\r\n        model: languageModel,\r\n        queryOptions: {\r\n          tenantId,\r\n          topK: body.topK,\r\n          minScore: body.minScore,\r\n          filter: body.filter,\r\n          includeMetadata: body.includeMetadata,\r\n          includeRelationships: body.includeRelationships,\r\n          rerankLimit: body.rerankLimit,\r\n          rerank: body.rerank,\r\n        },\r\n        systemPrompt: body.systemPrompt,\r\n        temperature: body.temperature,\r\n        messagesWithoutQuery,\r\n        lastMessage,\r\n        afterQueries: (totalQueries) => {\r\n          incrementUsage(namespace.id, totalQueries);\r\n        },\r\n      });\r\n\r\n      return result;\r\n    }\r\n\r\n    // TODO: track the usage\r\n    const data = await queryVectorStore(namespace, {\r\n      query,\r\n      tenantId,\r\n      topK: body.topK,\r\n      minScore: body.minScore,\r\n      filter: body.filter,\r\n      includeMetadata: body.includeMetadata,\r\n      includeRelationships: body.includeRelationships,\r\n      rerankLimit: body.rerankLimit,\r\n      rerank: body.rerank,\r\n    });\r\n\r\n    if (!data) {\r\n      throw new AgentsetApiError({\r\n        code: \"internal_server_error\",\r\n        message: \"Failed to parse chunks\",\r\n      });\r\n    }\r\n\r\n    const newMessages: CoreMessage[] = [\r\n      ...messagesWithoutQuery,\r\n      {\r\n        role: \"user\",\r\n        content: NEW_MESSAGE_PROMPT.compile({\r\n          chunks: data.results\r\n            .map((chunk, idx) => `[${idx + 1}]: ${chunk.text}`)\r\n            .join(\"\\n\\n\"),\r\n          query: lastMessage, // put the original query in the message to help with context\r\n        }),\r\n      },\r\n    ];\r\n\r\n    incrementUsage(namespace.id, 1);\r\n\r\n    // add the sources to the stream\r\n    return createDataStreamResponse({\r\n      execute: (dataStream) => {\r\n        const messageStream = streamText({\r\n          model: languageModel,\r\n          system: body.systemPrompt,\r\n          messages: newMessages,\r\n          temperature: body.temperature,\r\n          onError: (error) => {\r\n            console.error(error);\r\n          },\r\n        });\r\n\r\n        dataStream.writeMessageAnnotation({\r\n          type: \"agentset_sources\",\r\n          value: data as unknown as JSONValue,\r\n        });\r\n        messageStream.mergeIntoDataStream(dataStream);\r\n      },\r\n      headers,\r\n    });\r\n  },\r\n);\r\n"],"names":[],"mappings":";;;;;AACA;AACA;AACA;AAAA;AACA;AACA;AACA;AACA;AAKA;AAAA;AACA;AACA;AAEA;AAAA;AAEA;;;;;;;;;;;;;AAEA,MAAM,iBAAiB,CAAC,aAAqB;IAC3C,CAAA,GAAA,gJAAA,CAAA,YAAS,AAAD,EACN,CAAC;QACC,cAAc;QACd,MAAM,iIAAA,CAAA,KAAE,CAAC,SAAS,CAAC,MAAM,CAAC;YACxB,OAAO;gBACL,IAAI;YACN;YACA,MAAM;gBACJ,sBAAsB;oBAAE,WAAW;gBAAE;gBACrC,cAAc;oBACZ,QAAQ;wBACN,aAAa;4BAAE,WAAW;wBAAQ;oBACpC;gBACF;YACF;QACF;IACF,CAAC;AAEL;AAGO,MAAM,kBAAkB,QAAQ,6BAA6B;AAC7D,MAAM,cAAc;AAEpB,MAAM,OAAO,CAAA,GAAA,qJAAA,CAAA,qBAAkB,AAAD,EACnC,OAAO,EAAE,GAAG,EAAE,SAAS,EAAE,QAAQ,EAAE,OAAO,EAAE;IAC1C,MAAM,OAAO,MAAM,2KAAA,CAAA,aAAU,CAAC,UAAU,CAAC,MAAM,CAAA,GAAA,2IAAA,CAAA,mBAAgB,AAAD,EAAE;IAEhE,MAAM,uBAAuB,KAAK,QAAQ,CAAC,KAAK,CAAC,GAAG,CAAC;IACrD,MAAM,cACJ,KAAK,QAAQ,CAAC,MAAM,GAAG,IAClB,KAAK,QAAQ,CAAC,KAAK,QAAQ,CAAC,MAAM,GAAG,EAAE,CAAE,OAAO,GACjD;IAEN,IAAI,CAAC,aAAa;QAChB,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;YACzB,MAAM;YACN,SAAS;QACX;IACF;IAEA,8BAA8B;IAC9B,MAAM,gBAAgB,MAAM,CAAA,GAAA,kIAAA,CAAA,4BAAyB,AAAD;IAEpD,IAAI;IACJ,IAAI,qBAAqB,MAAM,KAAK,KAAK,KAAK,IAAI,KAAK,WAAW;QAChE,QAAQ;IACV,OAAO;QACL,qDAAqD;QACrD,MAAM,qBAAqB,qBAAqB,KAAK,CAAC,CAAC;QAEvD,sEAAsE;QACtE,QAAQ,CACN,MAAM,CAAA,GAAA,sJAAA,CAAA,eAAY,AAAD,EAAE;YACjB,OAAO;YACP,QAAQ,sIAAA,CAAA,yBAAsB,CAAC,OAAO,CAAC;gBACrC,UAAU;gBACV,aAAa,sIAAA,CAAA,uBAAoB,CAAC,OAAO,CAAC;oBACxC,OAAO;oBACP,aAAa,mBACV,GAAG,CACF,CAAC,IACC,CAAC,EAAE,EAAE,EAAE,IAAI,KAAK,SAAS,UAAU,YAAY,EAAE,EAAE,EAAE,OAAO,EAAY,EAE3E,IAAI,CAAC;gBACV;YACF;QACF,EACF,EAAE,IAAI;IACR;IAEA,IAAI,KAAK,IAAI,KAAK,gBAAgB;QAChC,MAAM,WAAW,IAAI,wJAAA,CAAA,uBAAoB,CAAC,WAAW;YACnD,aAAa;gBACX,MAAM;gBACN,UAAU;gBACV,SAAS;gBACT,QAAQ;YACV;YACA,cAAc;gBACZ;gBACA,MAAM,KAAK,IAAI;gBACf,UAAU,KAAK,QAAQ;gBACvB,QAAQ,KAAK,MAAM;gBACnB,iBAAiB,KAAK,eAAe;gBACrC,sBAAsB,KAAK,oBAAoB;gBAC/C,aAAa,KAAK,WAAW;gBAC7B,QAAQ,KAAK,MAAM;YACrB;QAEF;QAEA,MAAM,SAAS,MAAM,SAAS,WAAW,CAAC;QAC1C,eAAe,UAAU,EAAE,EAAE;QAE7B,OAAO,OAAO,oBAAoB,CAAC;YAAE;QAAQ;IAC/C;IAEA,IAAI,KAAK,IAAI,KAAK,WAAW;QAC3B,MAAM,SAAS,CAAA,GAAA,+IAAA,CAAA,UAAe,AAAD,EAAE,WAAW;YACxC,OAAO;YACP,cAAc;gBACZ;gBACA,MAAM,KAAK,IAAI;gBACf,UAAU,KAAK,QAAQ;gBACvB,QAAQ,KAAK,MAAM;gBACnB,iBAAiB,KAAK,eAAe;gBACrC,sBAAsB,KAAK,oBAAoB;gBAC/C,aAAa,KAAK,WAAW;gBAC7B,QAAQ,KAAK,MAAM;YACrB;YACA,cAAc,KAAK,YAAY;YAC/B,aAAa,KAAK,WAAW;YAC7B;YACA;YACA,cAAc,CAAC;gBACb,eAAe,UAAU,EAAE,EAAE;YAC/B;QACF;QAEA,OAAO;IACT;IAEA,wBAAwB;IACxB,MAAM,OAAO,MAAM,CAAA,GAAA,uJAAA,CAAA,mBAAgB,AAAD,EAAE,WAAW;QAC7C;QACA;QACA,MAAM,KAAK,IAAI;QACf,UAAU,KAAK,QAAQ;QACvB,QAAQ,KAAK,MAAM;QACnB,iBAAiB,KAAK,eAAe;QACrC,sBAAsB,KAAK,oBAAoB;QAC/C,aAAa,KAAK,WAAW;QAC7B,QAAQ,KAAK,MAAM;IACrB;IAEA,IAAI,CAAC,MAAM;QACT,MAAM,IAAI,4IAAA,CAAA,mBAAgB,CAAC;YACzB,MAAM;YACN,SAAS;QACX;IACF;IAEA,MAAM,cAA6B;WAC9B;QACH;YACE,MAAM;YACN,SAAS,sIAAA,CAAA,qBAAkB,CAAC,OAAO,CAAC;gBAClC,QAAQ,KAAK,OAAO,CACjB,GAAG,CAAC,CAAC,OAAO,MAAQ,CAAC,CAAC,EAAE,MAAM,EAAE,GAAG,EAAE,MAAM,IAAI,EAAE,EACjD,IAAI,CAAC;gBACR,OAAO;YACT;QACF;KACD;IAED,eAAe,UAAU,EAAE,EAAE;IAE7B,gCAAgC;IAChC,OAAO,CAAA,GAAA,sJAAA,CAAA,2BAAwB,AAAD,EAAE;QAC9B,SAAS,CAAC;YACR,MAAM,gBAAgB,CAAA,GAAA,sJAAA,CAAA,aAAU,AAAD,EAAE;gBAC/B,OAAO;gBACP,QAAQ,KAAK,YAAY;gBACzB,UAAU;gBACV,aAAa,KAAK,WAAW;gBAC7B,SAAS,CAAC;oBACR,QAAQ,KAAK,CAAC;gBAChB;YACF;YAEA,WAAW,sBAAsB,CAAC;gBAChC,MAAM;gBACN,OAAO;YACT;YACA,cAAc,mBAAmB,CAAC;QACpC;QACA;IACF;AACF","debugId":null}}]
}